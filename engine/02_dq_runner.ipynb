{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "3a8d07e4-7ab9-4458-9ab6-364db7b04ca7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install databricks-labs-dqx==0.9.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16d44982-8ced-4f0e-815a-5cb5f661a9f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "dq framework runner\n",
    "\n",
    "1. 'main()' orquesta el proceso\n",
    "2. 'execute_builtin_rule' ejecuta las reglas BUILT-IN\n",
    "3. 'execute_custom_rule' ejecuta las reglas custom (Python o SQL)\n",
    "4. 'run_validations_for_table' ejecuta todas las reglas de cada tabla, logueando trazabilidad\n",
    "'''\n",
    "\n",
    "# 1. Imports\n",
    "import json, time, uuid\n",
    "from datetime import datetime\n",
    "from typing import Callable, Tuple\n",
    "from pyspark.sql import DataFrame,SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, lag, when, col, lit, expr, sum as sum_\n",
    "import importlib\n",
    "import databricks.labs.dqx.check_funcs as dqx\n",
    "import inspect\n",
    "import utils.custom_rules_library_py as cr\n",
    "import utils.dq_utils as dq\n",
    "importlib.reload(cr)\n",
    "importlib.reload(dq)\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# 2. Widgets\n",
    "\n",
    "dbutils.widgets.text(\"catalog_name\", \"workspace\", \"Catálogo de UC donde residen las tablas\")\n",
    "dbutils.widgets.text(\"schema_name\", \"framework_dq\", \"Esquema de UC donde residen las tablas\")\n",
    "\n",
    "CATALOG = dbutils.widgets.get(\"catalog_name\")\n",
    "SCHEMA = dbutils.widgets.get(\"schema_name\")\n",
    "\n",
    "# 3. Constantes de tablas\n",
    "TABLE_CONFIG = f\"{CATALOG}.{SCHEMA}.dq_tables_config\"\n",
    "RULE_LIB_TABLE = f\"{CATALOG}.{SCHEMA}.dq_rules_library\"\n",
    "CATALOG_TABLE = f\"{CATALOG}.{SCHEMA}.dq_validations_catalog\"\n",
    "EXECUTION_TABLE = f\"{CATALOG}.{SCHEMA}.dq_execution_traceability\"\n",
    "VALIDATIONS_TABLE = f\"{CATALOG}.{SCHEMA}.dq_validations_traceability\"\n",
    "\n",
    "# 4. Leer reglas BUILT-IN del catálogo\n",
    "df_rules_library = spark.table(RULE_LIB_TABLE) \\\n",
    "                .filter(col(\"implementation_type\") == \"BUILT-IN\") \\\n",
    "                .select(\"technical_rule_name\", \"rule_level\") \\\n",
    "                .collect()\n",
    "\n",
    "# 5. Crear diccionarios dinámicos: reglas DQX (set de funciones nivel fila y set de funciones nivel dataset), reglas Custom y reglas con literal conversion\n",
    "\n",
    "BUILT_IN_RULE_FUNCTIONS = {}\n",
    "ROW_LEVEL_FUNCS = set()\n",
    "DATASET_LEVEL_FUNCS = set()\n",
    "CUSTOM_RULE_FUNCTIONS = {}\n",
    "RULES_REQUIRING_LITERAL_CONVERSION = []\n",
    "\n",
    "for row in df_rules_library:\n",
    "    tech_name = row.technical_rule_name\n",
    "\n",
    "    # Built-in DQX\n",
    "    if hasattr(dqx, tech_name):\n",
    "        BUILT_IN_RULE_FUNCTIONS[tech_name] = getattr(dqx, tech_name)\n",
    "        if row.rule_level.upper() == \"ROW\":\n",
    "            ROW_LEVEL_FUNCS.add(tech_name)\n",
    "        elif row.rule_level.upper() == \"DATASET\":\n",
    "            DATASET_LEVEL_FUNCS.add(tech_name)\n",
    "\n",
    "    elif row.implementation_type.upper() == \"CUSTOM\":\n",
    "        if hasattr(cr, tech_name):\n",
    "            # Asocia automáticamente con tu módulo cr\n",
    "            CUSTOM_RULE_FUNCTIONS[tech_name] = getattr(cr, tech_name)\n",
    "        else:\n",
    "            # Función SQL en catálogo\n",
    "            CUSTOM_RULE_FUNCTIONS[tech_name] = f\"{CATALOG}.{SCHEMA}.{row.sql_function}\"\n",
    "    \n",
    "    # Reglas que requieren literal conversion\n",
    "    if hasattr(row, \"requires_literal_conversion\") and row.requires_literal_conversion:\n",
    "        RULES_REQUIRING_LITERAL_CONVERSION.append(tech_name)\n",
    "'''\n",
    "CUSTOM_RULE_FUNCTIONS = {\n",
    "    \"custom_validate_dni\": cr.custom_validate_dni,\n",
    "    \"is_valid_nif_es\": f\"{CATALOG}.{SCHEMA}.is_valid_nif_es\"\n",
    "}\n",
    "\n",
    "RULES_REQUIRING_LITERAL_CONVERSION = [\n",
    "    \"is_equal_to\", \"is_not_equal_to\", \"is_not_less_than\", \"is_not_greater_than\"\n",
    "]\n",
    "'''    \n",
    "\n",
    "# 6. Función auxiliar para envolver consultas SQL del usuario\n",
    "def sql_query_wrapper(user_query: str, merge_columns: list[str]) -> Tuple[col, Callable]:\n",
    "    '''\n",
    "    Ejecuta una consulta SQL del usuario sin que tenga que definir la columna 'condition' en la query.\n",
    "    Devuelve condition_col y apply_func compatibles con DQX\n",
    "    '''\n",
    "    if not merge_columns:\n",
    "        raise ValueError(\"merge_columns no puede estar vacío\")\n",
    "\n",
    "    unique_condition_column = \"condition_tmp\"\n",
    "\n",
    "    def apply_func(df: DataFrame, spark: SparkSession, _: dict) -> DataFrame:\n",
    "        df_result = spark.sql(user_query)\n",
    "        df_result = df_result.withColumn(unique_condition_column, lit(True))\n",
    "        return df_result\n",
    "\n",
    "    condition_col = col(unique_condition_column)\n",
    "    return condition_col, apply_func \n",
    "\n",
    "# 7. Ejecución de las reglas BUILT-IN\n",
    "def execute_builtin_rule(df, rule_name, rule_type, params, primary_key):\n",
    "    '''Ejecuta reglas BUILT-IN y devuelve df_failed y status'''\n",
    "    df_failed = None\n",
    "    status = \"PASSED\"\n",
    "\n",
    "    if rule_name in ROW_LEVEL_FUNCS:\n",
    "        func_to_call = BUILT_IN_RULE_FUNCTIONS[rule_name]\n",
    "        sig = inspect.signature(func_to_call)\n",
    "\n",
    "        if \"df\" in sig.parameters:\n",
    "            condition_col = BUILT_IN_RULE_FUNCTIONS[rule_name](df, **params)\n",
    "        else:\n",
    "            if \"column\" in params:\n",
    "                params[\"column\"] = col(params[\"column\"])\n",
    "            condition_col = BUILT_IN_RULE_FUNCTIONS[rule_name](**params)\n",
    "            df_failed = df.filter(condition_col.isNotNull())\n",
    "\n",
    "    elif rule_name in DATASET_LEVEL_FUNCS:\n",
    "\n",
    "        # 1. Obtener la columna de condición y la función DQX\n",
    "        if rule_name == \"sql_query\":\n",
    "            condition_col, apply_func_dqx = sql_query_wrapper(params[\"query\"], params[\"merge_columns\"])      \n",
    "        else:\n",
    "            # Llamada genérica para obtener la regla\n",
    "            condition_col, apply_func_dqx = BUILT_IN_RULE_FUNCTIONS[rule_name](**params)\n",
    "            \n",
    "        # 2. Wrapper de la función para adaptar a los argumentos de DQX\n",
    "        # Análisis de qué argumentos pide la función apply_func\n",
    "        sig = inspect.signature(apply_func_dqx)\n",
    "        params_count = len(sig.parameters)\n",
    "        \n",
    "        # Definición del wrapper basado en lo que pide la función para ajustar firma: df, spark, context\n",
    "        def apply_func(df, spark, context):\n",
    "            if params_count == 1:\n",
    "                return apply_func_dqx(df)\n",
    "            elif params_count == 2:\n",
    "                return apply_func_dqx(df, spark)\n",
    "            else:\n",
    "                return apply_func_dqx(df, spark, context)\n",
    "        try:\n",
    "            df_failed = apply_func(df, spark, {}).filter(condition_col.isNotNull())\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error ejecutando la lógica interna de {rule_name}: {str(e)}\")\n",
    "\n",
    "    else:\n",
    "        status = \"ERROR\"\n",
    "        print(f\"BUILT-IN desconocida: {rule_name}\")\n",
    "\n",
    "    return df_failed, status\n",
    "\n",
    "# 8. Ejecución de las reglas Custom\n",
    "def execute_custom_rule(df, rule_name, params):\n",
    "    '''Ejecuta reglas Custom (Python o SQL) y devuelve df_failed y status'''\n",
    "\n",
    "    df_failed = None\n",
    "    status = \"PASSED\"\n",
    "    \n",
    "    columns = params.get(\"columns\", [])\n",
    "    if not columns:\n",
    "        print(f\"Regla custom mal configurada: {rule_name}, no se especificaron columnas\")\n",
    "        return df_failed, \"ERROR\"\n",
    "\n",
    "    # Comprobar que la regla existe en la whitelist\n",
    "    udf_func = CUSTOM_RULE_FUNCTIONS.get(rule_name)\n",
    "    if not udf_func:\n",
    "        print(f\"UDF '{rule_name}' no está en la whitelist y no se puede ejecutar\")\n",
    "        return df_failed, \"ERROR\"\n",
    "    \n",
    "    # Ejecutar regla SQL\n",
    "    if isinstance(udf_func, str):\n",
    "        # Construye la expresión SQL\n",
    "        sql_expr = f\"{udf_func}({', '.join([ '`{}`'.format(c) for c in columns ])})\"\n",
    "        df_failed = df.filter(~expr(sql_expr))\n",
    "        return df_failed, status\n",
    "    # Ejecutar regla Python\n",
    "    elif callable(udf_func):\n",
    "        df_failed = udf_func(df, params.get(\"columns\"))\n",
    "        return df_failed, status\n",
    "    # Desconocida\n",
    "    else:\n",
    "        print(f\"Regla custom desconocida: {rule_name}\")\n",
    "        return df_failed, \"ERROR\"\n",
    "\n",
    "# 9. Runner por tabla\n",
    "def run_validations_for_table(table_name, exec_timestamp, execution_id, severity_param=None, validation_id_param=None):\n",
    "    '''Ejecuta todas las validaciones de una tabla y retorna estado y conteos'''\n",
    "\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    exec_date = exec_timestamp.date()\n",
    "\n",
    "    #  Leer configuración de tabla \n",
    "    config_row = spark.table(TABLE_CONFIG).filter(col(\"table_name\") == table_name).first()\n",
    "    if not config_row:\n",
    "        print(f\"No se encontró configuración para la tabla '{table_name}'.\")\n",
    "        return \"SKIPPED\", 0, 0\n",
    "\n",
    "    table_id = config_row.table_id\n",
    "    table_tech_name = config_row.table_name_tech\n",
    "    staging_table = config_row.staging_evidences_table\n",
    "    primary_key = config_row.primary_key\n",
    "   \n",
    "    # Leer validaciones activas\n",
    "    filter_query = f\"table_id='{table_id}' AND is_active=True\"\n",
    "    if severity_param: filter_query += f\" AND severity='{severity_param}'\"\n",
    "    if validation_id_param: filter_query += f\" AND validation_id='{validation_id_param}'\"\n",
    "\n",
    "    df_validations = spark.table(CATALOG_TABLE).filter(filter_query)\n",
    "    if df_validations.limit(1).count() == 0:\n",
    "        print(f\"No hay validaciones activas para {table_name}.\")\n",
    "        dq.log_execution_finish(execution_id, \"SUCCESS\", 0, 0, 0, EXECUTION_TABLE)\n",
    "        return \"SUCCESS\", 0, 0\n",
    "\n",
    "    df_rules_lib = spark.table(RULE_LIB_TABLE).select(\"rule_id\", \"technical_rule_name\", col(\"implementation_type\").alias(\"rule_type\"))\n",
    "    rules_to_run = df_validations.join(df_rules_lib, \"rule_id\", \"left\").collect()\n",
    "\n",
    "    df = spark.table(table_tech_name)\n",
    "    total_records = df.count()\n",
    "    validations_executed = 0\n",
    "    validations_failed = 0\n",
    "\n",
    "    for rule in rules_to_run:\n",
    "        validations_executed += 1\n",
    "        validation_id = rule.validation_id\n",
    "        rule_id = rule.rule_id\n",
    "        rule_type = rule.rule_type\n",
    "        technical_rule_name = rule.technical_rule_name\n",
    "        validation_params = json.loads(rule.validation_definition or \"{}\")\n",
    "        failed_field = validation_params.get(\"column\") or (validation_params.get(\"columns\") or [primary_key])[0]\n",
    "\n",
    "        df_failed = None\n",
    "        status = \"PASSED\"\n",
    "\n",
    "        try:\n",
    "            \n",
    "            # Conversión literal. Algunas reglas necesitan el literal como parámetro y no la columna\n",
    "            if technical_rule_name in RULES_REQUIRING_LITERAL_CONVERSION:\n",
    "                for key in [\"value\", \"limit\"]:\n",
    "                    if key in validation_params and isinstance(validation_params[key], str):\n",
    "                        validation_params[key] = lit(validation_params[key])\n",
    "            \n",
    "            # Aplicar filtro de perímetro si existe\n",
    "            perimeter_sql = getattr(rule, \"perimeter_definition\", None)\n",
    "            df_filtered = df\n",
    "            total_records_perimeter = df.count()\n",
    "\n",
    "            if perimeter_sql:\n",
    "                try:\n",
    "                    df_filtered = df.filter(perimeter_sql)\n",
    "                    total_records_perimeter = df_filtered.count()\n",
    "                    print(f\"Aplicando filtro de perímetro: '{perimeter_sql}' -> {total_records_perimeter} registros\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Filtro de perímetro inválido '{perimeter_sql}': {e}\")\n",
    "                    print(\"Ejecutando validación sobre el dataset completo\")\n",
    "                    df_filtered = df\n",
    "                    total_records_perimeter = df.count()\n",
    "                    \n",
    "            # Ejecutar regla\n",
    "            if rule_type == \"BUILT-IN\":\n",
    "                df_failed, status = execute_builtin_rule(df, technical_rule_name, rule_type, validation_params, primary_key)\n",
    "            elif rule_type == \"CUSTOM\":\n",
    "                df_failed, status = execute_custom_rule(df, technical_rule_name, validation_params)\n",
    "            else:\n",
    "                status = \"ERROR\"\n",
    "                print(f\"Tipo de regla desconocido: {rule_type}\")\n",
    "\n",
    "            failed_count = 0\n",
    "            if df_failed is not None and df_failed.limit(1).count() > 0:\n",
    "                failed_count = df_failed.count()\n",
    "                validations_failed += 1\n",
    "                status = \"FAILED\"\n",
    "                dq.log_evidences_staging(df_failed, staging_table, execution_id, exec_date, validation_id, primary_key, failed_field, CATALOG, SCHEMA)\n",
    "\n",
    "            dq.log_validations_traceability(execution_id, exec_date, validation_id, rule_id, status, total_records_perimeter, failed_count, VALIDATIONS_TABLE)\n",
    "\n",
    "        except Exception as e:\n",
    "            validations_failed += 1\n",
    "            print(f\"Error crítico en regla {technical_rule_name}: {e}\")\n",
    "            dq.log_validations_traceability(execution_id, exec_date, validation_id, rule_id, \"ERROR\", total_records_perimeter, -1, VALIDATIONS_TABLE)\n",
    "\n",
    "    status = \"SUCCESS\" if validations_failed==0 else \"FAILED\"\n",
    "    return status, validations_executed, validations_failed\n",
    "\n",
    "# 10. Runner por múltiples tablas\n",
    "\n",
    "def run_validations_for_tables(tables_param=None, severity_param=None, validation_id_param=None):\n",
    "    '''\n",
    "    Ejecuta validaciones para todas las tablas indicadas o todas si tables_param=None or ALL\n",
    "    '''\n",
    "\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    tables_param_clean = tables_param.strip() if tables_param else None\n",
    "\n",
    "    total_validations_executed = 0\n",
    "    total_validations_failed = 0\n",
    "\n",
    "    if not tables_param_clean or tables_param_clean.lower() == \"all\":\n",
    "        df_tables = spark.table(TABLE_CONFIG).select(\"table_name\").collect()\n",
    "        tables_to_run = [r.table_name for r in df_tables]\n",
    "    else:\n",
    "        tables_to_run = [t.strip() for t in tables_param_clean.split(\",\") if t.strip()]\n",
    " \n",
    "    exec_timestamp = datetime.now()\n",
    "    execution_id = str(uuid.uuid4())\n",
    "    dq.log_execution_start(execution_id, exec_timestamp, tables_to_run, EXECUTION_TABLE)\n",
    "\n",
    "    for table_name in tables_to_run:\n",
    "        status, validations_executed, validations_failed = run_validations_for_table(table_name, exec_timestamp, execution_id, severity_param, validation_id_param)\n",
    "        total_validations_executed += validations_executed\n",
    "        total_validations_failed += validations_failed\n",
    "    \n",
    "    duration_seconds = round((datetime.now() - exec_timestamp).total_seconds(), 2)\n",
    "    dq.log_execution_finish(execution_id, status, duration_seconds, total_validations_executed, total_validations_failed, EXECUTION_TABLE)\n",
    "\n",
    "# 11. Punto de entrada\n",
    "if __name__ == \"__main__\":\n",
    "    dbutils.widgets.text(\"severity_filter\", \"\", \"Filtro de severidad (Opcional)\")\n",
    "    dbutils.widgets.text(\"validation_id_filter\", \"\", \"Filtro para 1 validación específica (Opcional)\")\n",
    "    dbutils.widgets.text(\"table_name\", \"\", \"Id de la tabla a validar\")\n",
    "    tables_param = dbutils.widgets.get(\"table_name\")\n",
    "    severity_param = dbutils.widgets.get(\"severity_filter\")\n",
    "    validation_id_param = dbutils.widgets.get(\"validation_id_filter\")\n",
    "    run_validations_for_tables(tables_param, severity_param, validation_id_param)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7361564235570810,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "02_dq_runner",
   "widgets": {
    "catalog_data": {
     "currentValue": "workspace",
     "nuid": "819e6d7d-cd7e-4d88-8552-acc4f9ac8760",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "workspace",
      "label": "Catálogo de UC donde residen las tablas",
      "name": "catalog_data",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "workspace",
      "label": "Catálogo de UC donde residen las tablas",
      "name": "catalog_data",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "catalog_name": {
     "currentValue": "workspace",
     "nuid": "25853fdc-7d06-4fa1-81cb-1f3fe8237969",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "workspace",
      "label": "Catálogo de UC donde residen las tablas",
      "name": "catalog_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "workspace",
      "label": "Catálogo de UC donde residen las tablas",
      "name": "catalog_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "schema_data": {
     "currentValue": "dummy_data",
     "nuid": "13ac30ca-5e18-4a49-bacb-225d9e8de956",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "dummy_data",
      "label": "Esquema de UC donde residen las tablas",
      "name": "schema_data",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "dummy_data",
      "label": "Esquema de UC donde residen las tablas",
      "name": "schema_data",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "schema_name": {
     "currentValue": "framework_dq",
     "nuid": "2e85d6b6-b8c5-4eb9-a6cf-1e430b548b8f",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "framework_dq",
      "label": "Esquema de UC donde residen las tablas",
      "name": "schema_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "framework_dq",
      "label": "Esquema de UC donde residen las tablas",
      "name": "schema_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "severity_filter": {
     "currentValue": "",
     "nuid": "b0b91389-645c-4519-801b-2ee73efc4b5b",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Filtro de severidad (Opcional)",
      "name": "severity_filter",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Filtro de severidad (Opcional)",
      "name": "severity_filter",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "table_name": {
     "currentValue": "clientes_run1",
     "nuid": "8e797fcf-5fa1-46c3-927d-9b8ebe30dc1b",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "clientes_run1",
      "label": "Id de la tabla a validar",
      "name": "table_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "clientes_run1",
      "label": "Id de la tabla a validar",
      "name": "table_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "validation_id_filter": {
     "currentValue": "",
     "nuid": "5a405b36-3fd8-4e45-a403-43449a288eaf",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Filtro para 1 validación específica (Opcional)",
      "name": "validation_id_filter",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Filtro para 1 validación específica (Opcional)",
      "name": "validation_id_filter",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
