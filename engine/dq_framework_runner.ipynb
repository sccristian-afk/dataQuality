{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26bfd00b-e3ad-441f-b44e-ae15fa86e7cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#%pip install databricks-labs-dqx==0.9.2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "841a4ba9-c909-4d41-a67f-2b8c007d7ca2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f74cca48-fa86-4da6-816a-ced2c7db003b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#%run /Workspace/Users/glamero17@gmail.com/test_dq/utils/custom_rules_library_py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "892ba1a7-ba2d-4bb1-97eb-f67cc848468a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#%run /Workspace/Users/glamero17@gmail.com/test_dq/utils/dq_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16d44982-8ced-4f0e-815a-5cb5f661a9f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# dq_framework_runner_final.py\n",
    "# ===============================\n",
    "\n",
    "import json, time, uuid\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, lag, when, sum as sum_\n",
    "\n",
    "# --- Import DQX dinámico ---\n",
    "import databricks.labs.dqx.check_funcs as dqx\n",
    "\n",
    "# --- Import funciones custom ---\n",
    "from utils.custom_rules_library_py import custom_validate_dni\n",
    "from utils.dq_utils import log_execution_start, log_execution_finish, log_validations_traceability, log_evidences_staging\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# --- Constantes de tablas ---\n",
    "CATALOG = \"workspace\"\n",
    "SCHEMA = \"dq_framework\"\n",
    "TABLE_CONFIG = f\"{CATALOG}.{SCHEMA}.dq_tables_config\"\n",
    "RULE_LIB_TABLE = f\"{CATALOG}.{SCHEMA}.dq_rules_library\"\n",
    "CATALOG_TABLE = f\"{CATALOG}.{SCHEMA}.dq_validations_catalog\"\n",
    "EXECUTION_TABLE = f\"{CATALOG}.{SCHEMA}.dq_execution_traceability\"\n",
    "VALIDATIONS_TABLE = f\"{CATALOG}.{SCHEMA}.dq_validations_traceability\"\n",
    "\n",
    "# Leer reglas BUILT-IN del catálogo\n",
    "df_rules_library = spark.table(RULE_LIB_TABLE) \\\n",
    "                .filter(col(\"implementation_type\") == \"BUILT-IN\") \\\n",
    "                .select(\"technical_rule_name\", \"rule_level\") \\\n",
    "                .collect()\n",
    "\n",
    "BUILT_IN_RULE_FUNCTIONS = {}\n",
    "ROW_LEVEL_FUNCS = set()\n",
    "DATASET_LEVEL_FUNCS = set()\n",
    "\n",
    "# Crear diccionario dinámico literal: función DQX, set de funciones nivel fila y set de funciones nivel dataset\n",
    "\n",
    "for row in df_rules_library:\n",
    "    func_name = row.technical_rule_name\n",
    "    if hasattr(dqx, func_name):\n",
    "        BUILT_IN_RULE_FUNCTIONS[func_name] = getattr(dqx, func_name)\n",
    "        if row.rule_level.upper() == \"ROW\":\n",
    "            ROW_LEVEL_FUNCS.add(func_name)\n",
    "        elif row.rule_level.upper() == \"DATASET\":\n",
    "            DATASET_LEVEL_FUNCS.add(func_name)\n",
    "    else:\n",
    "        print(f\"⚠️ Función DQX {func_name} no encontrada\")\n",
    "\n",
    "CUSTOM_RULE_FUNCTIONS = {\n",
    "    \"custom_validate_dni\": custom_validate_dni,\n",
    "    \"is_valid_nif_es\": is_valid_nif_es\n",
    "}\n",
    "\n",
    "RULES_REQUIRING_LITERAL_CONVERSION = [\n",
    "    \"is_equal_to\", \"is_not_equal_to\", \"is_not_less_than\", \"is_not_greater_than\"\n",
    "]\n",
    "\n",
    "# ===============================\n",
    "# Ejecutores auxiliares\n",
    "# ===============================\n",
    "\n",
    "def execute_builtin_rule(df, rule_name, rule_type, params, primary_key):\n",
    "    \"\"\"Ejecuta reglas BUILT-IN y devuelve df_failed y status\"\"\"\n",
    "    df_failed = None\n",
    "    status = \"PASSED\"\n",
    "\n",
    "    if rule_name in ROW_LEVEL_FUNCS:\n",
    "        if \"column\" in params:\n",
    "            params[\"column\"] = col(params[\"column\"])\n",
    "        condition_col = built_in_rule_functions[rule_name](**params)\n",
    "        df_failed = df.filter(condition_col.isNotNull())\n",
    "    elif rule_name in DATASET_LEVEL_FUNCS:\n",
    "        condition_col, apply_func = built_in_rule_functions[rule_name](**params)\n",
    "        df_failed = apply_func(df, spark, {}).filter(condition_col.isNotNull())\n",
    "    else:\n",
    "        status = \"ERROR\"\n",
    "        print(f\"BUILT-IN desconocida: {rule_name}\")\n",
    "    return df_failed, status\n",
    "\n",
    "def execute_custom_rule(df, rule_name, params):\n",
    "    \"\"\"Ejecuta reglas CUSTOM Python\"\"\"\n",
    "    df_failed = None\n",
    "    status = \"PASSED\"\n",
    "    func = CUSTOM_RULE_FUNCTIONS.get(rule_name)\n",
    "    if func:\n",
    "        df_failed = func(df, params.get(\"columns\"))\n",
    "    else:\n",
    "        status = \"ERROR\"\n",
    "        print(f\"CUSTOM desconocida: {rule_name}\")\n",
    "    return df_failed, status\n",
    "\n",
    "def execute_sql_rule(df, rule_name, params):\n",
    "    \"\"\"Ejecuta reglas CUSTOM_SQL_UDF\"\"\"\n",
    "    df_failed = None\n",
    "    status = \"PASSED\"\n",
    "    udf_name = rule_name\n",
    "    columns = params.get(\"columns\", [])\n",
    "    udf_func = CUSTOM_RULE_FUNCTIONS.get(rule_name)\n",
    "    if udf_func:\n",
    "        if udf_name and columns:\n",
    "            sql_expr = f\"{udf_name}({', '.join([ '`{}`'.format(c) for c in columns ])})\"\n",
    "            df_failed = df.filter(~expr(sql_expr))#.cache()\n",
    "        else:\n",
    "            status = \"ERROR\"\n",
    "            print(f\"CUSTOM_SQL_UDF mal configurada: {rule_name}\")\n",
    "    else:\n",
    "        status = \"ERROR\"\n",
    "        raise ValueError(f\"UDF '{udf_name}' no está en la whitelist y no se puede ejecutar\")\n",
    "    return df_failed, status\n",
    "    \n",
    "# ===============================\n",
    "# Runner por tabla\n",
    "# ===============================\n",
    "\n",
    "def run_validations_for_table(table_name, severity_param=None, validation_id_param=None):\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    execution_id = str(uuid.uuid4())\n",
    "    exec_timestamp = datetime.now()\n",
    "    exec_date = exec_timestamp.date()\n",
    "\n",
    "    # --- Leer configuración de tabla ---\n",
    "    config_row = spark.table(TABLE_CONFIG).filter(col(\"table_name\") == table_name).first()\n",
    "    if not config_row:\n",
    "        print(f\"No se encontró configuración para tabla '{table_name}'.\")\n",
    "        return\n",
    "\n",
    "    table_id = config_row.table_id\n",
    "    table_tech_name = config_row.table_name_tech\n",
    "    staging_table = config_row.staging_evidences_table\n",
    "    primary_key = config_row.primary_key\n",
    "\n",
    "    log_execution_start(execution_id, exec_timestamp, table_id, EXECUTION_TABLE)\n",
    "\n",
    "    # --- Leer validaciones ---\n",
    "    filter_query = f\"table_id='{table_id}' AND is_active=True\"\n",
    "    if severity_param: filter_query += f\" AND severity='{severity_param}'\"\n",
    "    if validation_id_param: filter_query += f\" AND validation_id='{validation_id_param}'\"\n",
    "\n",
    "    df_validations = spark.table(CATALOG_TABLE).filter(filter_query)\n",
    "    if df_validations.isEmpty():\n",
    "        print(f\"No hay validaciones activas para {table_name}.\")\n",
    "        log_execution_finish(execution_id, \"SUCCESS\", 0, 0, 0, EXECUTION_TABLE)\n",
    "        return\n",
    "\n",
    "    df_rules_lib = spark.table(RULE_LIB_TABLE).select(\"rule_id\", \"technical_rule_name\", col(\"implementation_type\").alias(\"rule_type\"))\n",
    "    rules_to_run = df_validations.join(df_rules_lib, \"rule_id\", \"left\").collect()\n",
    "\n",
    "    df = spark.table(table_tech_name)\n",
    "    total_records = df.count()\n",
    "    validations_executed = 0\n",
    "    validations_failed = 0\n",
    "\n",
    "    for v in rules_to_run:\n",
    "        validations_executed += 1\n",
    "        validation_id = v.validation_id\n",
    "        rule_id = v.rule_id\n",
    "        rule_type = v.rule_type\n",
    "        technical_rule_name = v.technical_rule_name\n",
    "        validation_params = json.loads(v.validation_definition or \"{}\")\n",
    "        failed_field = validation_params.get(\"column\") or (validation_params.get(\"columns\") or [primary_key])[0]\n",
    "\n",
    "        df_failed = None\n",
    "        rule_status = \"PASSED\"\n",
    "\n",
    "        try:\n",
    "            \n",
    "            # Conversión literal\n",
    "            if technical_rule_name in RULES_REQUIRING_LITERAL_CONVERSION:\n",
    "                for key in [\"value\", \"limit\"]:\n",
    "                    if key in validation_params and isinstance(validation_params[key], str):\n",
    "                        validation_params[key] = lit(validation_params[key])\n",
    "\n",
    "            \n",
    "            # --- Aplicar filtro de perímetro si existe ---\n",
    "            perimeter_sql = getattr(v, \"perimeter_definition\", None)\n",
    "            df_filtered = df  # fallback: todo el dataset\n",
    "            total_records_perimeter = df.count()\n",
    "\n",
    "            if perimeter_sql:\n",
    "                try:\n",
    "                    df_filtered = df.filter(perimeter_sql)\n",
    "                    total_records_perimeter = df_filtered.count()\n",
    "                    print(f\"Aplicando filtro de perímetro: '{perimeter_sql}' -> {total_records_perimeter} registros\")\n",
    "                except Exception as e:\n",
    "                    print(f\"⚠️ Filtro de perímetro inválido '{perimeter_sql}': {e}\")\n",
    "                    print(\"Ejecutando validación sobre el dataset completo.\")\n",
    "                    df_filtered = df\n",
    "                    total_records_perimeter = df.count()\n",
    "\n",
    "                    \n",
    "            # Ejecutar regla\n",
    "            if rule_type == \"BUILT-IN\":\n",
    "                df_failed, rule_status = execute_builtin_rule(df, technical_rule_name, rule_type, validation_params, primary_key)\n",
    "            elif rule_type == \"CUSTOM\":\n",
    "                df_failed, rule_status = execute_custom_rule(df, technical_rule_name, validation_params)\n",
    "            elif rule_type == \"CUSTOM_SQL_UDF\":\n",
    "                df_failed, rule_status = execute_sql_rule(df, technical_rule_name, validation_params)\n",
    "            else:\n",
    "                rule_status = \"ERROR\"\n",
    "                print(f\"Tipo de regla desconocido: {rule_type}\")\n",
    "\n",
    "            failed_count = 0\n",
    "            if df_failed is not None and not df_failed.isEmpty():\n",
    "                failed_count = df_failed.count()\n",
    "                validations_failed += 1\n",
    "                rule_status = \"FAILED\"\n",
    "                log_evidences_staging(df_failed, staging_table, execution_id, exec_date, validation_id, primary_key, failed_field, CATALOG, SCHEMA)\n",
    "\n",
    "            log_validations_traceability(execution_id, exec_date, validation_id, rule_id, rule_status, total_records_perimeter, failed_count, VALIDATIONS_TABLE)\n",
    "\n",
    "        except Exception as e:\n",
    "            validations_failed += 1\n",
    "            print(f\"Error crítico en regla {technical_rule_name}: {e}\")\n",
    "            log_validations_traceability(execution_id, exec_date, validation_id, rule_id, \"ERROR\", total_records_perimeter, -1, VALIDATIONS_TABLE)\n",
    "\n",
    "    # --- Fin de ejecución tabla ---\n",
    "    duration_seconds = round((datetime.now() - exec_timestamp).total_seconds(), 2)\n",
    "    status = \"SUCCESS\" if validations_failed == 0 else \"FAILED\"\n",
    "    log_execution_finish(execution_id, status, duration_seconds, validations_executed, validations_failed, EXECUTION_TABLE)\n",
    "    print(f\"Tabla {table_name} completada con estado: {status}\")\n",
    "    return execution_id, status\n",
    "\n",
    "# ===============================\n",
    "# Runner por job (múltiples tablas)\n",
    "# ===============================\n",
    "\n",
    "def run_validations_for_tables(tables_param=None, severity_param=None, validation_id_param=None):\n",
    "    \"\"\"\n",
    "    Ejecuta validaciones para todas las tablas indicadas o todas si tables_param=None\n",
    "    \"\"\"\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    tables_param_clean = tables_param.strip() if tables_param else None\n",
    "\n",
    "    if not tables_param_clean or tables_param_clean.lower() == \"all\":\n",
    "        df_tables = spark.table(TABLE_CONFIG).select(\"table_name\").collect()\n",
    "        tables_to_run = [r.table_name for r in df_tables]\n",
    "    else:\n",
    "        tables_to_run = [t.strip() for t in tables_param_clean.split(\",\") if t.strip()]\n",
    "\n",
    "    print(f\"Tablas a ejecutar: {tables_to_run}\")\n",
    "\n",
    "    for table_name in tables_to_run:\n",
    "        run_validations_for_table(table_name, severity_param, validation_id_param)\n",
    "\n",
    "# ===============================\n",
    "# Punto de entrada\n",
    "# ===============================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tables_param = dbutils.widgets.get(\"table_name\")  # puede ser vacío o \"ALL\"\n",
    "    severity_param = dbutils.widgets.get(\"severity_filter\")\n",
    "    validation_id_param = dbutils.widgets.get(\"validation_id_filter\")\n",
    "    run_validations_for_tables(tables_param, severity_param, validation_id_param)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "dq_framework_runner",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
