{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26bfd00b-e3ad-441f-b44e-ae15fa86e7cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install databricks-labs-dqx==0.9.2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "841a4ba9-c909-4d41-a67f-2b8c007d7ca2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f74cca48-fa86-4da6-816a-ced2c7db003b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run /Workspace/Users/glamero17@gmail.com/test_dq/utils/custom_rules_library_py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "892ba1a7-ba2d-4bb1-97eb-f67cc848468a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run /Workspace/Users/glamero17@gmail.com/test_dq/utils/dq_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16d44982-8ced-4f0e-815a-5cb5f661a9f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MAGIC %md\n",
    "# MAGIC # 游 Motor de Calidad de Datos (Orquestador)\n",
    "# MAGIC \n",
    "# MAGIC Este notebook ejecuta el framework de calidad de datos\n",
    "# MAGIC \n",
    "# MAGIC **Actualizaciones v10:**\n",
    "# MAGIC * **Arquitectura Normalizada:** Este script ahora hace un JOIN entre\n",
    "# MAGIC   `dq_validations_catalog` y `dq_rule_library` para obtener el\n",
    "# MAGIC   `technical_rule_name` y el `rule_type` en tiempo de ejecuci칩n.\n",
    "# MAGIC * Lee de la tabla `table_config` (nuevo nombre).\n",
    "# MAGIC * Sigue usando el motor de API 2 (`check_funcs.py`).\n",
    "# MAGIC * Mantiene la l칩gica de `def main()` y particionamiento por fecha.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1, 1. Imports y Widgets\n",
    "import yaml\n",
    "import uuid\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime, date\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, lit, current_timestamp\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# --- Funciones disponibles en check_funcs.py ---\n",
    "from databricks.labs.dqx.check_funcs import (\n",
    "    is_not_null, is_unique, regex_match, is_in_range, is_in_list, compare_datasets,\n",
    "    sql_expression, is_aggr_not_greater_than, foreign_key, is_aggr_not_less_than, is_valid_timestamp,\n",
    "    is_aggr_equal, is_aggr_not_equal, is_not_empty, is_not_null_and_not_empty, is_not_equal_to,\n",
    "    is_valid_date, is_not_in_future, sql_query, is_not_null_and_is_in_list, is_not_in_range,\n",
    "    is_not_less_than, is_not_greater_than, is_equal_to\n",
    ")\n",
    "\n",
    "#no me ha dejado importar:  has_valid_schema\n",
    "\n",
    "\n",
    "# --- Widgets ---\n",
    "#dbutils.widgets.text(\"table_id\", \"\", \"Id de la tabla a validar\")\n",
    "dbutils.widgets.text(\"severity_filter\", \"\", \"Filtro de severidad (Opcional)\")\n",
    "dbutils.widgets.text(\"validation_id_filter\", \"\", \"Filtro para 1 validaci칩n espec칤fica (Opcional)\")\n",
    "dbutils.widgets.text(\"table_name\", \"maestro_demo\", \"Id de la tabla a validar\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 2, 2. Carga de librer칤as y definici칩n de constantes/mapas\n",
    "\n",
    "dbutils.widgets.text(\"catalog_name\", \"workspace\", \"Cat치logo de UC donde residen las tablas\")\n",
    "dbutils.widgets.text(\"schema_name\", \"dq_framework\", \"Esquema de UC donde residen las tablas\")\n",
    "\n",
    "CATALOG = dbutils.widgets.get(\"catalog_name\")\n",
    "SCHEMA = dbutils.widgets.get(\"schema_name\")\n",
    "\n",
    "EXECUTION_TABLE = f\"{CATALOG}.{SCHEMA}.dq_execution_traceability\"\n",
    "VALIDATIONS_TABLE = f\"{CATALOG}.{SCHEMA}.dq_validations_traceability\"\n",
    "\n",
    "TABLE_CONFIG = f\"{CATALOG}.{SCHEMA}.dq_tables_config\"\n",
    "RULE_LIB_TABLE = f\"{CATALOG}.{SCHEMA}.dq_rules_library\"\n",
    "CATALOG_TABLE = f\"{CATALOG}.{SCHEMA}.dq_validations_catalog\"\n",
    "\n",
    "# --- Carga de librer칤as de funciones custom ---\n",
    "#  como hacer en PRO (en repor???????????????????\n",
    "#las funciones van a carpetas en repos (ej.\n",
    "#/Repos/usuario/proyecto/utils/dq_utils.py\n",
    "#/Repos/usuario/proyecto/utils/custom_rules_library_py.py)\n",
    "#Luego se importa como cualquier libreria. Pej.\n",
    "# from utils.dq_utils import alguna_funcion\n",
    "#from utils.custom_rules_library_py import otra_funcion\n",
    "\n",
    "# Mapea el string del nombre t칠cncio con un nombre de funci칩n disponible\n",
    "BUILT_IN_RULE_FUNCTIONS = {\n",
    "    \"is_not_null\": is_not_null, \"is_not_empty\": is_not_empty, \"is_not_null_and_not_empty\": is_not_null_and_not_empty,\n",
    "    \"is_in_list\": is_in_list, \"regex_match\": regex_match, \"is_in_range\": is_in_range, \"is_not_equal_to\": is_not_equal_to,\n",
    "    \"sql_expression\": sql_expression, \"is_valid_date\": is_valid_date, \"is_not_in_future\": is_not_in_future, \n",
    "    \"is_not_null_and_is_in_list\": is_not_null_and_is_in_list,\"is_not_less_than\": is_not_less_than, \"is_not_in_range\": is_not_in_range,\n",
    "    \"is_not_greater_than\": is_not_less_than, \"is_equal_to\": is_equal_to,\"is_valid_timestamp\": is_valid_timestamp,\n",
    "    \"is_unique\": is_unique, \"is_aggr_not_greater_than\": is_aggr_not_greater_than, \"is_aggr_not_less_than\": is_aggr_not_less_than, \n",
    "    \"is_aggr_equal\": is_aggr_equal, \"is_aggr_not_equal\": is_aggr_not_equal, \"foreign_key\": foreign_key, \n",
    "    \"sql_query\": sql_query, \"compare_datasets\": compare_datasets\n",
    "}\n",
    "   \n",
    "ROW_LEVEL_FUNCS = [\n",
    "    \"is_not_null\", \"is_not_empty\", \"is_not_null_and_not_empty\", \"is_in_list\", \n",
    "    \"regex_match\", \"is_in_range\", \"sql_expression\", \"is_valid_date\", \"is_not_in_future\",\n",
    "    \"is_valid_timestamp\", \"is_not_null_and_is_in_list\", \"is_not_in_range\",\n",
    "    \"is_not_less_than\", \"is_not_greater_than\", \"is_equal_to\", \"is_not_equal_to\"\n",
    "]\n",
    "DATASET_LEVEL_FUNCS = [\n",
    "    \"is_unique\", \"is_aggr_not_greater_than\", \"is_aggr_not_less_than\", \n",
    "    \"is_aggr_equal\", \"is_aggr_not_equal\", \"foreign_key\", \"sql_query\",\n",
    "    \"compare_datasets\"\n",
    "]\n",
    "\n",
    "CUSTOM_RULE_FUNCTIONS = {\n",
    "    \"custom_validate_dni\": custom_validate_dni \n",
    "}\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 3, 3. Funci칩n Principal (main)\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Flujo principal de orquestaci칩n.\n",
    "    \"\"\"    \n",
    "    # --- 1. Inicializar variables de trazabilidad ---\n",
    "    start_time = time.time()\n",
    "    execution_id = str(uuid.uuid4())\n",
    "    exec_timestamp = datetime.now()\n",
    "    exec_date = exec_timestamp.date()\n",
    "    \n",
    "    status = \"FAILED\"\n",
    "    total_records = 0\n",
    "    validations_executed_count = 0\n",
    "    validations_failed_count = 0\n",
    "    table_name = \"N/A\"\n",
    "    table_id_param = \"N/A\"\n",
    "    df = None\n",
    "    \n",
    "        # --- 2. Cargar par치metros del Job ---\n",
    "#--------------------------------------------- esta linea siguiente borrar porque deber칤a venir del JOB\n",
    "    \n",
    "    table_name_param = dbutils.widgets.get(\"table_name\")\n",
    "    severity_param = dbutils.widgets.get(\"severity_filter\")\n",
    "    validation_id_param = dbutils.widgets.get(\"validation_id_filter\")\n",
    "\n",
    "    if not table_name_param:\n",
    "        print(\"Error Cr칤tico: 'table_name' no puede estar vac칤o.\")\n",
    "    \n",
    "    # --- 3. Leer la configuraci칩n mMaestra ---\n",
    "    \n",
    "    config_row = (spark.table(TABLE_CONFIG)\n",
    "                    .filter(col(\"table_name\") == table_name_param)\n",
    "                    .first())\n",
    "    \n",
    "    if config_row is None:\n",
    "        print(f\"No se encontr칩 configuraci칩n en {TABLE_CONFIG} para la tabla '{table_name_param}'\")\n",
    "    \n",
    "    table_id_param = config_row.table_id\n",
    "    table_name = config_row.table_name\n",
    "    table_to_validate = config_row.table_name_tech\n",
    "    primary_key = config_row.primary_key\n",
    "    staging_evidences_table = config_row.staging_evidences_table\n",
    "    \n",
    "    # --- 4. Log de inicio ---\n",
    "    log_execution_start(execution_id, exec_timestamp, table_id_param, EXECUTION_TABLE)\n",
    "    \n",
    "    # --- 5. Leer las validaciones a ejecutar ---\n",
    "    filter_query = f\"table_id = '{table_id_param}' AND is_active = True\"\n",
    "    if severity_param:\n",
    "        filter_query += f\" AND severity = '{severity_param}'\"\n",
    "    if validation_id_param:\n",
    "        filter_query += f\" AND validation_id = '{validation_id_param}'\"\n",
    "    \n",
    "    # Leer las validaciones y cruzarlas con la librer칤a para obtener m치s informaci칩n\n",
    "    df_validations_to_run = (spark.table(CATALOG_TABLE)\n",
    "                                .filter(filter_query))\n",
    "    \n",
    "    df_library = (spark.table(RULE_LIB_TABLE)\n",
    "                    .select(\"rule_id\", \"technical_rule_name\", col(\"implementation_type\").alias(\"rule_type\")))\n",
    "\n",
    "    rules_list = (df_validations_to_run\n",
    "                    .join(df_library, \"rule_id\", \"left\")\n",
    "                    .select(\n",
    "                        df_validations_to_run.validation_id,\n",
    "                        df_validations_to_run.rule_id,\n",
    "                        df_validations_to_run.validation_definition,\n",
    "                        df_library.rule_type,\n",
    "                        df_library.technical_rule_name\n",
    "                    )\n",
    "                    .collect())\n",
    "    \n",
    "    if not rules_list:\n",
    "        print(f\"AVISO: No se encontraron validaciones activas para el filtro: {filter_query}\")\n",
    "        status = \"SUCCESS\"\n",
    "        duration = round(time.time() - start_time, 2)\n",
    "        return (execution_id, status, duration, validations_executed_count, validations_failed_count)\n",
    "        \n",
    "    print(f\"Se encontraron {len(rules_list)} validaciones para ejecutar.\")\n",
    "    \n",
    "\n",
    "    # --- 6. Carga de datos ---\n",
    "    #!!!!!!!!!!!!!!!!aqui no se permite la cache\n",
    "    #df = spark.table(table_to_validate).cache()\n",
    "    df = spark.table(table_to_validate)\n",
    "    total_records = df.count()\n",
    "    print(f\"Se han cargado {total_records} registros.\")\n",
    "\n",
    "    # --- 7. Motor de ejecuci칩n de validaciones ---\n",
    "    print(f\"--- Iniciando la ejecuci칩n de {len(rules_list)} validaciones ---\")\n",
    "\n",
    "    # Reglas que requieren un literal de comparaci칩n pero tambi칠n permiten una columna\n",
    "    RULES_REQUIRING_LITERAL_CONVERSION = [\n",
    "        \"is_equal_to\", \"is_not_equal_to\", \n",
    "        \"is_not_less_than\", \"is_not_greater_than\"\n",
    "    ]\n",
    "\n",
    "    for validation in rules_list:\n",
    "        \n",
    "        validation_id = validation.validation_id\n",
    "        rule_id = validation.rule_id\n",
    "        validation_definition_str = validation.validation_definition\n",
    "        rule_type = validation.rule_type\n",
    "        technical_rule_name = validation.technical_rule_name\n",
    "        \n",
    "        print(f\"\\n--- Ejecutando validaci칩n ID: {validation_id}, Regla: {technical_rule_name}) ---\")\n",
    "        validations_executed_count += 1\n",
    "        \n",
    "        df_failed = None\n",
    "        rule_status = \"PASSED\"\n",
    "        \n",
    "        try:\n",
    "            if not rule_type or not technical_rule_name:\n",
    "                raise Exception(f\"No se pudo encontrar la regla (rule_id: {rule_id}) en la dq_rule_library.\")\n",
    "                \n",
    "            params = json.loads(validation_definition_str)\n",
    "\n",
    "            failed_field = primary_key\n",
    "            if \"column\" in params:\n",
    "                failed_field = params[\"column\"]\n",
    "            elif \"columns\" in params and params[\"columns\"]:\n",
    "                failed_field = params[\"columns\"][0]\n",
    "\n",
    "            #cambio a literal del parametro value o limit\n",
    "            if technical_rule_name in RULES_REQUIRING_LITERAL_CONVERSION:\n",
    "                if \"value\" in params and isinstance(params[\"value\"], str):\n",
    "                    # Si el valor es un string, lo convertimos en un Literal de Spark\n",
    "                    params[\"value\"] = lit(params[\"value\"])\n",
    "                if \"limit\" in params and isinstance(params[\"limit\"], str):\n",
    "                    # Si el l칤mite es un string, lo convertimos en un Literal de Spark\n",
    "                    params[\"limit\"] = lit(params[\"limit\"])\n",
    "\n",
    "\n",
    "            # --- CASO 1: REGLA BUILT-IN ---\n",
    "            if rule_type == 'BUILT-IN':\n",
    "                if technical_rule_name not in BUILT_IN_RULE_FUNCTIONS:\n",
    "                    rule_status = \"ERROR\"\n",
    "                    print(f\"  > ERROR: Regla BUILT-IN desconocida: {technical_rule_name}\")\n",
    "                else:\n",
    "                    func_to_call = BUILT_IN_RULE_FUNCTIONS[technical_rule_name]\n",
    "                    if technical_rule_name in ROW_LEVEL_FUNCS:\n",
    "                        params_exec = params.copy()\n",
    "                        if 'column' in params_exec: params_exec['column'] = col(params_exec['column'])\n",
    "                        condition_col = func_to_call(**params)\n",
    "                        #!!!!!!!!!En serverless compute no es posible\n",
    "                        #df_failed = df.filter(condition_col.isNotNull()).cache()\n",
    "                        df_failed = df.filter(condition_col.isNotNull())\n",
    "                    elif technical_rule_name in DATASET_LEVEL_FUNCS:\n",
    "                        (condition_col, apply_func) = func_to_call(**params)\n",
    "                        df_with_cond = apply_func(df, spark, {}) \n",
    "                        #!!!!!!!!!En serverless compute no es posible\n",
    "                        #df_failed = df_with_cond.filter(condition_col.isNotNull()).cache()\n",
    "                        df_failed = df_with_cond.filter(condition_col.isNotNull())\n",
    "\n",
    "            # --- CASO 2: REGLA CUSTOM (Python) ---\n",
    "            elif rule_type == 'CUSTOM':\n",
    "                if technical_rule_name in CUSTOM_RULE_FUNCTIONS:\n",
    "                    function_to_call = CUSTOM_RULE_FUNCTIONS[technical_rule_name]\n",
    "                    #!!!!!!!!!En serverless compute no es posible\n",
    "                    #df_failed = function_to_call(df, params.get(\"columns\")).cache()\n",
    "                    df_failed = function_to_call(df, params.get(\"columns\"))\n",
    "                else:\n",
    "                    rule_status = \"ERROR\"\n",
    "                    print(f\"  > ERROR: Funci칩n CUSTOM desconocida: {technical_rule_name}\")\n",
    "            \n",
    "            # --- CASO 3: REGLA CUSTOM (SQL) ---\n",
    "            elif rule_type == 'CUSTOM_SQL_UDF':\n",
    "                udf_name = technical_rule_name\n",
    "                udf_columns = params.get(\"columns\", [])\n",
    "                if udf_name and udf_columns:\n",
    "                    sql_expression = f\"{udf_name}({', '.join(udf_columns)})\"\n",
    "                    #!!!!!!!!!En serverless compute no es posible\n",
    "                    #df_failed = df.filter(f\"NOT ({sql_expression})\").cache()\n",
    "                    df_failed = df.filter(f\"NOT ({sql_expression})\")\n",
    "                else:\n",
    "                    rule_status = \"ERROR\"\n",
    "                    print(f\"  > ERROR: CUSTOM_SQL_UDF mal configurada.\")\n",
    "            \n",
    "            # --- CASO 4: Tipo desconocido ---\n",
    "            else:\n",
    "                rule_status = \"ERROR\"\n",
    "                print(f\"  > ERROR: Tipo de regla '{rule_type}' desconocido.\")\n",
    "            \n",
    "            # --- 7a. Procesamiento de resultados (por regla) ---\n",
    "            failed_count = 0\n",
    "            if df_failed is not None and not df_failed.isEmpty():\n",
    "                rule_status = \"FAILED\"\n",
    "                validations_failed_count += 1\n",
    "                failed_count = df_failed.count()\n",
    "              \n",
    "                log_evidences_staging(\n",
    "                    df_failed=df_failed, \n",
    "                    staging_table_name=staging_evidences_table, \n",
    "                    execution_id=execution_id, \n",
    "                    exec_date=exec_date,\n",
    "                    validation_id=validation_id, \n",
    "                    primary_key_col=primary_key,\n",
    "                    failed_field=failed_field,\n",
    "                    catalog=CATALOG,\n",
    "                    schema=SCHEMA\n",
    "                )\n",
    "                #!!!!!!!!!En serverless compute no es posible\n",
    "                #df_failed.unpersist()\n",
    "            \n",
    "            # --- 7b. Log de trazabilidad de validaci칩n ---\n",
    "            log_validations_traceability(\n",
    "                execution_id=execution_id,\n",
    "                exec_date=exec_date,\n",
    "                validation_id=validation_id,\n",
    "                rule_id=rule_id,\n",
    "                status=rule_status,\n",
    "                perimeter=total_records,\n",
    "                failed_count=failed_count,\n",
    "                validations_table=VALIDATIONS_TABLE\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  > Erro cr칤tico ejecutando regla: {technical_rule_name} (Id: {validation_id}). Error: {e}\")\n",
    "            validations_failed_count += 1\n",
    "            log_validations_traceability(\n",
    "                execution_id=execution_id,\n",
    "                exec_date=exec_date,\n",
    "                validation_id=validation_id,\n",
    "                rule_id=rule_id,\n",
    "                status=\"ERROR\",\n",
    "                perimeter=total_records,\n",
    "                failed_count=-1,\n",
    "                validations_table=VALIDATIONS_TABLE\n",
    "            )\n",
    "            #!!!!!!!!!!!!!!!!!!! en sereveless no es posible\n",
    "            #if df_failed:\n",
    "                #df_failed.unpersist()\n",
    "    \n",
    "    # --- Fin del Bucle ---\n",
    "    print(\"\\n--- Ejecuci칩n de validaciones finalizada ---\")\n",
    "    status = \"SUCCESS\" if validations_failed_count == 0 else \"FAILED\"\n",
    "    \n",
    "        # --- 8. Finalizaci칩n y log de fin de ejecuci칩n ---\n",
    "    #if df is not None:\n",
    "        #!!!!!!!!!!!!!!!esto no se permite en serveless compute\n",
    "        #df.unpersist()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    duration = round(end_time - start_time, 2)\n",
    "    \n",
    "    log_execution_finish(\n",
    "        execution_id=execution_id,\n",
    "        status=status,\n",
    "        duration=duration,\n",
    "        validations_exec=validations_executed_count,\n",
    "        validations_fail=validations_failed_count,\n",
    "        trace_table=table_name\n",
    "    )\n",
    "    \n",
    "    print(f\"Framework finalizado para '{table_name}' en {duration}s. Estado final: {status}\")\n",
    "    return execution_id, status\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 4, 4. Punto de Entrada de Ejecuci칩n\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"antes main\")\n",
    "    result = main()\n",
    "    exec_id, status = result[0], result[1]\n",
    "    #butils.notebook.exit(f\"Ejecuci칩n {exec_id} completada. Estado: {status}\")\n",
    "        \n",
    "    #print(f\"Fallo fatal en la ejecuci칩n: {e}\")\n",
    "    #dbutils.notebook.exit(f\"Fallo fatal en la ejecuci칩n: {e}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "dq_framework_runner",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
