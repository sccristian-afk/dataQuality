{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86529df4-1b24-4827-805a-a75aeab89c25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# MAGIC %md\n",
    "# MAGIC # Calcular persistencia de incidencias\n",
    "# MAGIC \n",
    "# MAGIC \n",
    "# MAGIC **L��gica:**\n",
    "# MAGIC 1.  `main()` orquesta todo el proceso.\n",
    "# MAGIC 2.  Busca ejecuciones en `dq_validations_traceability` que necesiten c��lculo de persistencia.\n",
    "# MAGIC 3.  Encuentra los pares de ejecuci��n (N vs N-1) usando `dq_execution_traceability`.\n",
    "# MAGIC 4.  Carga las evidencias (`dq_evidences`), filtrando por las particiones de fecha (N y N-1).\n",
    "# MAGIC 5.  Calcula las m��tricas de fallos nuevos, persistentes y resueltos.\n",
    "# MAGIC 6.  Actualiza (`MERGE`) la tabla `dq_validations_traceability` con estas m��tricas.\n",
    "# MAGIC 7.  Limpia los nulos restantes (ej: primeras ejecuciones) y los establece en 0.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1, 1. Imports y constantes\n",
    "from pyspark.sql.functions import col, lag, desc, row_number, count, when, lit, coalesce\n",
    "from pyspark.sql.window import Window\n",
    "from delta.tables import DeltaTable\n",
    "import sys\n",
    "\n",
    "# --- Widgets ---\n",
    "#dbutils.widgets.text(\"table_id\", \"\", \"Id de la tabla a validar\")\n",
    "dbutils.widgets.text(\"table_name\", \"maestro_demo\", \"Id de la tabla a validar\")\n",
    "dbutils.widgets.text(\"catalog_name\", \"workspace\", \"Catálogo de UC donde residen las tablas\")\n",
    "dbutils.widgets.text(\"schema_names\", \"framework_dq\", \"Esquema de UC donde residen las tablas\")\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 2, 2. Carga de librerías y definición de constantes/mapas\n",
    "\n",
    "CATALOG = dbutils.widgets.get(\"catalog_name\")\n",
    "SCHEMA = dbutils.widgets.get(\"schema_names\")\n",
    "\n",
    "EXECUTION_TABLE = f\"{CATALOG}.{SCHEMA}.dq_execution_traceability\"\n",
    "VALIDATIONS_TABLE = f\"{CATALOG}.{SCHEMA}.dq_validations_traceability\"\n",
    "EVIDENCES_TABLE = f\"{CATALOG}.{SCHEMA}.dq_evidences\"\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Flujo optimizado extremo para cálculo de persistencia sin crear DataFrame intermedio\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # --- 1. Validaciones pendientes ---\n",
    "        df_validations_to_process = (\n",
    "            spark.table(VALIDATIONS_TABLE)\n",
    "            .filter((col(\"status\").isin(\"PASSED\", \"FAILED\")))\n",
    "            .select(\"execution_id\", \"validation_id\", \"rule_id\")\n",
    "            .distinct()\n",
    "        )\n",
    "\n",
    "        df_executions_to_process = df_validations_to_process.select(\"execution_id\").distinct()\n",
    "        if df_executions_to_process.isEmpty():\n",
    "            print(\"No hay nuevas ejecuciones para procesar.\")\n",
    "            return \"Éxito: No hay nuevas ejecuciones para procesar.\"\n",
    "        print(f\"Ejecuciones a procesar: {df_executions_to_process.count()}\")\n",
    "\n",
    "        # --- 2. Pares N vs N-1 ---\n",
    "        window_prev_exec = Window.partitionBy(\"table_id\").orderBy(\"execution_timestamp\")\n",
    "        df_execution_pairs = (\n",
    "            spark.table(EXECUTION_TABLE)\n",
    "            .filter(col(\"status\").isin(\"SUCCESS\", \"FAILED\"))\n",
    "            .withColumn(\"prev_execution_id\", lag(\"execution_id\").over(window_prev_exec))\n",
    "            .join(df_executions_to_process, \"execution_id\")\n",
    "            .filter(col(\"prev_execution_id\").isNotNull())\n",
    "            .select(\n",
    "                col(\"execution_id\").alias(\"current_execution_id\"),\n",
    "                \"prev_execution_id\",\n",
    "                \"table_id\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # --- 4. Preparar DeltaTable ---\n",
    "        delta_validations_table = DeltaTable.forName(spark, VALIDATIONS_TABLE)\n",
    "        delta_evidences_table = DeltaTable.forName(spark, EVIDENCES_TABLE)\n",
    "\n",
    "        if df_execution_pairs.isEmpty():\n",
    "            # Primera ejecución: todas las evidencias son nuevas\n",
    "            df_evidences = (\n",
    "                spark.table(EVIDENCES_TABLE)\n",
    "                .filter(col(\"execution_id\").isin([row.execution_id for row in df_executions_to_process.collect()]))\n",
    "                .withColumn(\"is_new_failure\", lit(1))\n",
    "            )\n",
    "\n",
    "            df_for_merge = (\n",
    "                df_evidences.groupBy(\"execution_id\", \"validation_id\")\n",
    "                .agg(\n",
    "                    count(\"*\").alias(\"new_failures\")\n",
    "                )\n",
    "                .withColumn(\"persistent_failures\", lit(0))\n",
    "                .withColumn(\"resolved_failures\", lit(0))\n",
    "            )\n",
    "            \n",
    "            df_for_merge = df_for_merge.repartition(\"execution_id\", \"validation_id\")\n",
    "            delta_validations_table.alias(\"target\").merge(\n",
    "                df_for_merge.alias(\"source\"),\n",
    "                (col(\"target.execution_id\") == col(\"source.execution_id\")) &\n",
    "                (col(\"target.validation_id\") == col(\"source.validation_id\"))\n",
    "            ).whenMatchedUpdate(\n",
    "                set={\n",
    "                    \"new_failures\": col(\"source.new_failures\"),\n",
    "                    \"persistent_failures\": col(\"source.persistent_failures\"),\n",
    "                    \"resolved_failures\": col(\"source.resolved_failures\")\n",
    "                }\n",
    "            ).execute()\n",
    "\n",
    "            # Merge para solo actualizar is_new_failure\n",
    "            delta_evidences_table.alias(\"target\").merge(\n",
    "                df_evidences.alias(\"source\"),\n",
    "                (\n",
    "                    (col(\"target.execution_id\") == col(\"source.execution_id\")) &\n",
    "                    (col(\"target.validation_id\") == col(\"source.validation_id\")) &\n",
    "                    (col(\"target.table_pk\") == col(\"source.table_pk\"))\n",
    "                )\n",
    "            ).whenMatchedUpdate(\n",
    "                set={\"is_new_failure\": col(\"source.is_new_failure\")}\n",
    "            ).execute()\n",
    "\n",
    "            print(\"Primera ejecución procesada: todas las evidencias marcadas como nuevas.\")\n",
    "            return \"Cálculo de persistencia finalizado: primera ejecución\"\n",
    "        else:\n",
    "            # --- 3. Evidencias relevantes ---\n",
    "            relevant_executions = df_execution_pairs.select(\"current_execution_id\").union(\n",
    "                df_execution_pairs.select(\"prev_execution_id\")\n",
    "            ).distinct()\n",
    "\n",
    "            df_evidences = (\n",
    "                spark.table(EVIDENCES_TABLE)\n",
    "                .join(relevant_executions,\n",
    "                      (col(\"execution_id\") == col(\"current_execution_id\")) |\n",
    "                      (col(\"execution_id\") == col(\"prev_execution_id\")),\n",
    "                      \"inner\"\n",
    "                )\n",
    "                .select(\"execution_id\", \"validation_id\", \"table_pk\")\n",
    "                .distinct()\n",
    "                #.cache()\n",
    "            )\n",
    "\n",
    "        # --- 5. MERGE directo con cálculo de métricas usando window ---\n",
    "        window_spec = Window.partitionBy(\"validation_id\", \"table_pk\").orderBy(\"execution_id\")\n",
    "        \n",
    "        # Añadimos is_new_failure a nivel de fila en las evidencias\n",
    "        df_evidences = df_evidences.withColumn(\n",
    "            \"prev_execution_id_lag\", lag(\"execution_id\").over(window_spec)\n",
    "        ).withColumn(\n",
    "            \"is_new_failure\",\n",
    "            when(col(\"prev_execution_id_lag\").isNull(), 1).otherwise(0)\n",
    "        )\n",
    "        \n",
    "        df_for_merge = df_evidences.withColumn(\n",
    "            \"prev_execution_id_lag\", lag(\"execution_id\").over(window_spec)\n",
    "        ).withColumn(\n",
    "            \"new_failures\", when(col(\"execution_id\") != col(\"prev_execution_id_lag\"), 1).otherwise(0)\n",
    "        ).withColumn(\n",
    "            \"persistent_failures\", when(col(\"execution_id\") == col(\"prev_execution_id_lag\"), 1).otherwise(0)\n",
    "        ).withColumn(\n",
    "            \"resolved_failures\", when(col(\"prev_execution_id_lag\").isNotNull() & (col(\"execution_id\") != col(\"prev_execution_id_lag\")), 1).otherwise(0)\n",
    "        ).groupBy(\"execution_id\", \"validation_id\").agg(\n",
    "            sum(\"new_failures\").alias(\"new_failures\"),\n",
    "            sum(\"persistent_failures\").alias(\"persistent_failures\"),\n",
    "            sum(\"resolved_failures\").alias(\"resolved_failures\")\n",
    "        ).fillna(0)\n",
    "\n",
    "        df_for_merge = df_for_merge.repartition(\"execution_id\", \"validation_id\")\n",
    "        delta_validations_table.alias(\"target\").merge(\n",
    "            df_for_merge.alias(\"source\"),\n",
    "            (col(\"target.execution_id\") == col(\"source.execution_id\")) &\n",
    "            (col(\"target.validation_id\") == col(\"source.validation_id\"))\n",
    "        ).whenMatchedUpdateAll(\n",
    "                set={\n",
    "                    \"new_failures\": col(\"source.new_failures\"),\n",
    "                    \"persistent_failures\": col(\"source.persistent_failures\"),\n",
    "                    \"resolved_failures\": col(\"source.resolved_failures\")\n",
    "                }\n",
    "        ).execute()\n",
    "        print(\"Métricas de persistencia actualizadas en un solo paso.\")\n",
    "\n",
    "        # Merge para solo actualizar is_new_failure\n",
    "        delta_evidences_table.alias(\"target\").merge(\n",
    "            df_evidences.alias(\"source\"),\n",
    "            (\n",
    "                (col(\"target.execution_id\") == col(\"source.execution_id\")) &\n",
    "                (col(\"target.validation_id\") == col(\"source.validation_id\")) &\n",
    "                (col(\"target.table_pk\") == col(\"source.table_pk\"))\n",
    "            )\n",
    "        ).whenMatchedUpdate(\n",
    "            set={\"is_new_failure\": col(\"source.is_new_failure\")}\n",
    "        ).execute()\n",
    "\n",
    "        # --- 6. Rellenar nulos restantes ---\n",
    "        delta_validations_table.update(\n",
    "            condition=(\n",
    "                col(\"status\").isin(\"PASSED\", \"FAILED\") &\n",
    "                (col(\"persistent_failures\").isNull() |\n",
    "                 col(\"new_failures\").isNull() |\n",
    "                 col(\"resolved_failures\").isNull())\n",
    "            ),\n",
    "            set={\"persistent_failures\": 0, \"new_failures\": 0, \"resolved_failures\": 0}\n",
    "        )\n",
    "        print(\"Nulos restantes rellenados.\")\n",
    "\n",
    "        # --- 7. Liberar memoria ---\n",
    "        if 'df_evidences' in locals() and df_evidences.is_cached:\n",
    "            df_evidences.unpersist()\n",
    "\n",
    "        return \"Cálculo de persistencia finalizado.\"\n",
    "\n",
    "    except Exception as e:\n",
    "        if 'df_evidences' in locals() and df_evidences.is_cached:\n",
    "            df_evidences.unpersist()\n",
    "        raise e\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Ejecutamos la función principal\n",
    "        rows_processed = main()\n",
    "    except Exception as e:\n",
    "        # Salida explícita en caso de fallo\n",
    "        dbutils.notebook.exit(f\"Fallo en la persistencia de evidencias: {e}\")\n",
    "    else:\n",
    "        # Salida explícita en caso de éxito\n",
    "        dbutils.notebook.exit(f\"Éxito: {rows_processed} evidencias procesadas.\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04_calculate_persistence",
   "widgets": {
    "catalog_name": {
     "currentValue": "workspace",
     "nuid": "7315e0c4-56fa-424d-81d5-e364d7f49da6",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "workspace",
      "label": "Catálogo de UC donde residen las tablas",
      "name": "catalog_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "workspace",
      "label": "Catálogo de UC donde residen las tablas",
      "name": "catalog_name",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "schema_name": {
     "currentValue": "dq_framework",
     "nuid": "2fa8eb40-bc90-4def-8645-9cfd9c2265f6",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "dq_framework",
      "label": "Esquema de UC donde residen las tablas",
      "name": "schema_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "dq_framework",
      "label": "Esquema de UC donde residen las tablas",
      "name": "schema_name",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "table_name": {
     "currentValue": "maestro_demo",
     "nuid": "f96f6636-47fe-4cd8-ad15-722c5bd9daff",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "maestro_demo",
      "label": "Id de la tabla a validar",
      "name": "table_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "maestro_demo",
      "label": "Id de la tabla a validar",
      "name": "table_name",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
