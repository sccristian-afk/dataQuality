{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86529df4-1b24-4827-805a-a75aeab89c25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "Calcular persistencia\n",
    "\n",
    "Lógica\n",
    "1. 'main()' orquesta el proceso.\n",
    "2. Recupera las ejecuciones SUCCESS o FAILED para el cálculo\n",
    "3. Recupera las validaciones de la última ejecución\n",
    "4. Recupera las validaciones de la ejecución anterior\n",
    "5. Realiza los cálculos para incidencias nuevas, persistentes o resueltas\n",
    "6. Actualiza las tablas de validaciones y evidencias\n",
    "'''\n",
    "\n",
    "# 1. Imports\n",
    "from pyspark.sql.functions import col, lag, desc, row_number, count, when, lit, coalesce\n",
    "from pyspark.sql.window import Window\n",
    "from delta.tables import DeltaTable\n",
    "import sys\n",
    "\n",
    "# 2. Widgets\n",
    "dbutils.widgets.text(\"table_name\", \"maestro_demo\", \"Id de la tabla a validar\")\n",
    "dbutils.widgets.text(\"catalog_name\", \"workspace\", \"Catálogo de UC donde residen las tablas\")\n",
    "dbutils.widgets.text(\"schema_name\", \"framework_dq\", \"Esquema de UC donde residen las tablas\")\n",
    "\n",
    "CATALOG = dbutils.widgets.get(\"catalog_name\")\n",
    "SCHEMA = dbutils.widgets.get(\"schema_name\")\n",
    "\n",
    "EXECUTION_TABLE = f\"{CATALOG}.{SCHEMA}.dq_execution_traceability\"\n",
    "VALIDATIONS_TABLE = f\"{CATALOG}.{SCHEMA}.dq_validations_traceability\"\n",
    "EVIDENCES_TABLE = f\"{CATALOG}.{SCHEMA}.dq_evidences\"\n",
    "\n",
    "# 3. Función principal\n",
    "def main():\n",
    "    try:\n",
    "        # 1. Recuperar ejecuciones a analizar\n",
    "        df_exec = (\n",
    "            spark.table(EXECUTION_TABLE)\n",
    "            .filter(col(\"status\").isin(\"SUCCESS\", \"FAILED\"))\n",
    "            .orderBy(col(\"execution_timestamp\").desc())\n",
    "        )\n",
    "\n",
    "        if df_exec.count() == 0:\n",
    "            return \"No existen ejecuciones\"\n",
    "        \n",
    "        # 2. Cálculo de la última ejecución y su antecesora\n",
    "        latest_execution_id = df_exec.first().execution_id\n",
    "\n",
    "        window_exec = Window.orderBy(col(\"execution_timestamp\"))\n",
    "        df_exec_with_prev = df_exec.withColumn(\n",
    "            \"prev_execution_id\", lag(\"execution_id\").over(window_exec)\n",
    "        )\n",
    "\n",
    "        prev_execution_id = (\n",
    "            df_exec_with_prev.filter(col(\"execution_id\") == latest_execution_id)\n",
    "            .select(\"prev_execution_id\")\n",
    "            .first()\n",
    "            .prev_execution_id\n",
    "        )\n",
    "\n",
    "        delta_validations = DeltaTable.forName(spark, VALIDATIONS_TABLE)\n",
    "        delta_evidences = DeltaTable.forName(spark, EVIDENCES_TABLE)\n",
    "\n",
    "        # 3. Lógica para primera ejecución: todas las evidencias se marcan como nuevas\n",
    "        if prev_execution_id is None:\n",
    "\n",
    "            df_current = (\n",
    "                spark.table(EVIDENCES_TABLE)\n",
    "                .filter(col(\"execution_id\") == latest_execution_id)\n",
    "                .withColumn(\"is_new_failure\", lit(1))\n",
    "            )\n",
    "\n",
    "            df_merge = (\n",
    "                df_current.groupBy(\"execution_id\", \"validation_id\")\n",
    "                .agg(count(\"*\").alias(\"new_failures\"))\n",
    "                .withColumn(\"persistent_failures\", lit(0))\n",
    "                .withColumn(\"resolved_failures\", lit(0))\n",
    "            )\n",
    "\n",
    "            # 3.1 Merge validaciones\n",
    "            delta_validations.alias(\"target\").merge(\n",
    "                df_merge.alias(\"source\"),\n",
    "                (col(\"target.execution_id\") == col(\"source.execution_id\"))\n",
    "                & (col(\"target.validation_id\") == col(\"source.validation_id\")),\n",
    "            ).whenMatchedUpdate(\n",
    "                set={\n",
    "                    \"new_failures\": col(\"source.new_failures\"),\n",
    "                    \"persistent_failures\": col(\"source.persistent_failures\"),\n",
    "                    \"resolved_failures\": col(\"source.resolved_failures\"),\n",
    "                }\n",
    "            ).execute()\n",
    "\n",
    "            # 3.2 Merge evidencias\n",
    "            delta_evidences.alias(\"target\").merge(\n",
    "                df_current.alias(\"source\"),\n",
    "                (col(\"target.execution_id\") == col(\"source.execution_id\"))\n",
    "                & (col(\"target.validation_id\") == col(\"source.validation_id\"))\n",
    "                & (col(\"target.table_pk\") == col(\"source.table_pk\")),\n",
    "            ).whenMatchedUpdate(\n",
    "                set={\"is_new_failure\": col(\"source.is_new_failure\")}\n",
    "            ).execute()\n",
    "\n",
    "            return \"Primera ejecución procesada correctamente\"\n",
    "\n",
    "        else:\n",
    "            # 4. Lógica para siguientes ejecuciones: cargar evidencias previas y actuales\n",
    "\n",
    "            # 4.1 Evidencias de la última ejecución (alias 'curr' para joins postreriores)\n",
    "            df_current = (\n",
    "                spark.table(EVIDENCES_TABLE)\n",
    "                .filter(col(\"execution_id\") == latest_execution_id)\n",
    "                .select(\n",
    "                    col(\"evidence_id\"),\n",
    "                    col(\"execution_id\"),\n",
    "                    col(\"validation_id\"),\n",
    "                    col(\"table_pk\")\n",
    "                )\n",
    "                .alias(\"curr\")\n",
    "            )\n",
    "\n",
    "            # 4.2 Evidencias de la ejecución anterior (alias 'prev' para joins postreriores)\n",
    "            df_prev = (\n",
    "                spark.table(EVIDENCES_TABLE)\n",
    "                .filter(col(\"execution_id\") == prev_execution_id)\n",
    "                .select(\n",
    "                    col(\"evidence_id\"),\n",
    "                    col(\"execution_id\"),\n",
    "                    col(\"validation_id\"),\n",
    "                    col(\"table_pk\")\n",
    "                )\n",
    "                .withColumnRenamed(\"evidence_id\", \"prev_evidence_id\")\n",
    "                .withColumnRenamed(\"execution_id\", \"prev_execution_id\")\n",
    "                .withColumnRenamed(\"validation_id\", \"prev_validation_id\")\n",
    "                .withColumnRenamed(\"table_pk\", \"prev_table_pk\")\n",
    "                .alias(\"prev\")\n",
    "            )\n",
    "\n",
    "            # 4.3 Calcular is_new_failure (actual - prev)\n",
    "\n",
    "            df_evidences = df_current.join(\n",
    "                df_prev,\n",
    "                (col(\"curr.validation_id\") == col(\"prev.prev_validation_id\"))\n",
    "                & (col(\"curr.table_pk\") == col(\"prev.prev_table_pk\")),\n",
    "                \"left_outer\",\n",
    "            ).withColumn(\n",
    "                \"is_new_failure\",\n",
    "                when(col(\"prev.prev_validation_id\").isNull(), lit(1)).otherwise(lit(0)),\n",
    "            ).select(\n",
    "                col(\"curr.execution_id\"),\n",
    "                col(\"curr.validation_id\"),\n",
    "                col(\"curr.table_pk\"),\n",
    "                col(\"is_new_failure\"),\n",
    "            )\n",
    "\n",
    "            # 4.4 Calcular resolved_failures (prev - current)\n",
    "\n",
    "            df_resolved = df_prev.join(\n",
    "                df_current,\n",
    "                (col(\"prev.prev_validation_id\") == col(\"curr.validation_id\"))\n",
    "                & (col(\"prev.prev_table_pk\") == col(\"curr.table_pk\")),\n",
    "                \"left_anti\",\n",
    "            ).groupBy(\n",
    "                col(\"prev_execution_id\"),\n",
    "                col(\"prev_validation_id\"),\n",
    "            ).agg(count(\"*\").alias(\"resolved_failures\"))\n",
    "\n",
    "            # 4.5 Calcular new y persistent\n",
    "\n",
    "            df_failures = (\n",
    "                df_evidences.groupBy(\"execution_id\", \"validation_id\")\n",
    "                .agg(\n",
    "                    count(\"is_new_failure\").alias(\"new_failures\"),\n",
    "                    (count(\"*\") - count(\"is_new_failure\")).alias(\"persistent_failures\"),\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # 4.6 Agrupa todos los cálculos\n",
    "            df_failures = df_failures.join(\n",
    "                df_resolved.withColumnRenamed(\"prev_execution_id\", \"execution_id\")\n",
    "                .withColumnRenamed(\"prev_validation_id\", \"validation_id\"),\n",
    "                [\"execution_id\", \"validation_id\"],\n",
    "                \"left\",\n",
    "            ).fillna({\"resolved_failures\": 0})\n",
    "\n",
    "            # 4.7 Merge final de validaciones\n",
    "            delta_validations.alias(\"target\").merge(\n",
    "                df_failures.alias(\"source\"),\n",
    "                (col(\"target.execution_id\") == col(\"source.execution_id\"))\n",
    "                & (col(\"target.validation_id\") == col(\"source.validation_id\")),\n",
    "            ).whenMatchedUpdate(\n",
    "                set={\n",
    "                    \"new_failures\": col(\"source.new_failures\"),\n",
    "                    \"persistent_failures\": col(\"source.persistent_failures\"),\n",
    "                    \"resolved_failures\": col(\"source.resolved_failures\"),\n",
    "                }\n",
    "            ).execute()\n",
    "\n",
    "            # 4.8 Merge final de evidencias\n",
    "            delta_evidences.alias(\"target\").merge(\n",
    "                df_evidences.alias(\"source\"),\n",
    "                (col(\"target.execution_id\") == col(\"source.execution_id\"))\n",
    "                & (col(\"target.validation_id\") == col(\"source.validation_id\"))\n",
    "                & (col(\"target.table_pk\") == col(\"source.table_pk\")),\n",
    "            ).whenMatchedUpdate(set={\"is_new_failure\": col(\"source.is_new_failure\")}).execute()\n",
    "\n",
    "            return \"Ejecución procesada correctamente.\"\n",
    "\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        result = main()\n",
    "    except Exception as e:\n",
    "        dbutils.notebook.exit(f\"Fallo en la persistencia de evidencias: {e}\")\n",
    "    else:\n",
    "        dbutils.notebook.exit(f\"Éxito: evidencias procesadas\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04_calculate_persistence",
   "widgets": {
    "catalog_name": {
     "currentValue": "workspace",
     "nuid": "7315e0c4-56fa-424d-81d5-e364d7f49da6",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "workspace",
      "label": "Catálogo de UC donde residen las tablas",
      "name": "catalog_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "workspace",
      "label": "Catálogo de UC donde residen las tablas",
      "name": "catalog_name",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "schema_name": {
     "currentValue": "framework_dq",
     "nuid": "2fa8eb40-bc90-4def-8645-9cfd9c2265f6",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "dq_framework",
      "label": "Esquema de UC donde residen las tablas",
      "name": "schema_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "dq_framework",
      "label": "Esquema de UC donde residen las tablas",
      "name": "schema_name",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
