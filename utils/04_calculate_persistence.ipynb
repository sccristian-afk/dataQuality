{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86529df4-1b24-4827-805a-a75aeab89c25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "Calcular persistencia\n",
    "\n",
    "Lógica\n",
    "1. 'main()' orquesta el proceso.\n",
    "2. Recupera las ejecuciones SUCCESS o FAILED para el cálculo\n",
    "3. Recupera las validaciones de la última ejecución\n",
    "4. Recupera las validaciones de la ejecución anterior\n",
    "5. Realiza los cálculos para incidencias nuevas, persistentes o resueltas\n",
    "6. Actualiza las tablas de validaciones y evidencias\n",
    "'''\n",
    "\n",
    "# 1. Imports\n",
    "from pyspark.sql.functions import col, lag, desc, row_number, count, when, lit, coalesce\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.types import IntegerType\n",
    "import sys\n",
    "\n",
    "# 2. Widgets\n",
    "def get_mandatory_widget(name, label=None):\n",
    "    if label is None:\n",
    "        label = name\n",
    "    try:\n",
    "        # Intentar crear el widget\n",
    "        dbutils.widgets.text(name, \"\", label)\n",
    "    except:\n",
    "        # Si ya existe, ignorar el error\n",
    "        pass\n",
    "    value = dbutils.widgets.get(name)\n",
    "    if not value.strip():\n",
    "        raise ValueError(f\"Parámetro obligatorio: {name}\")\n",
    "    return value\n",
    "\n",
    "CATALOG = get_mandatory_widget(\"catalog_name\",\"Catálogo de UC donde residen las tablas\")\n",
    "SCHEMA = get_mandatory_widget(\"schema_name\", \"Esquema de UC donde residen las tablas\")\n",
    "\n",
    "EXECUTION_TABLE = f\"{CATALOG}.{SCHEMA}.dq_execution_traceability\"\n",
    "VALIDATIONS_TABLE = f\"{CATALOG}.{SCHEMA}.dq_validations_traceability\"\n",
    "EVIDENCES_TABLE = f\"{CATALOG}.{SCHEMA}.dq_evidences\"\n",
    "\n",
    "# 3. Función principal\n",
    "def main():\n",
    "    \n",
    "    try:\n",
    "        # 1. Recupera ejecuciones a analizar\n",
    "        df_exec = (\n",
    "            spark.table(EXECUTION_TABLE)\n",
    "            .filter(col(\"status\").isin(\"SUCCESS\", \"FAILED\"))\n",
    "            .orderBy(col(\"execution_timestamp\").desc())\n",
    "        )\n",
    "\n",
    "        if df_exec.count() == 0:\n",
    "            return \"No existen ejecuciones\"\n",
    "        \n",
    "        # 2. Cálculo de la última ejecución y su antecesora\n",
    "        latest_execution_id = df_exec.first().execution_id\n",
    "\n",
    "        window_exec = Window.orderBy(col(\"execution_timestamp\"))\n",
    "        df_exec_with_prev = df_exec.withColumn(\n",
    "            \"prev_execution_id\", lag(\"execution_id\").over(window_exec)\n",
    "        )\n",
    "\n",
    "        prev_execution_id = (\n",
    "            df_exec_with_prev.filter(col(\"execution_id\") == latest_execution_id)\n",
    "            .select(\"prev_execution_id\")\n",
    "            .first()\n",
    "            .prev_execution_id\n",
    "        )\n",
    "\n",
    "        delta_validations = DeltaTable.forName(spark, VALIDATIONS_TABLE)\n",
    "        delta_evidences = DeltaTable.forName(spark, EVIDENCES_TABLE)\n",
    "\n",
    "        # 3. Lógica para primera ejecución: todas las evidencias se marcan como nuevas\n",
    "        if prev_execution_id is None:\n",
    "\n",
    "            df_current = (\n",
    "                spark.table(EVIDENCES_TABLE)\n",
    "                .filter(col(\"execution_id\") == latest_execution_id)\n",
    "                .withColumn(\"is_new_failure_num\", lit(1).cast(IntegerType()))\n",
    "                .withColumn(\"is_new_failure\", lit(True))\n",
    "            )\n",
    "\n",
    "            df_merge = (\n",
    "                spark.table(EVIDENCES_TABLE)\n",
    "                .groupBy(\"execution_id\", \"validation_id\")\n",
    "                .agg(F.count(\"*\").alias(\"new_failures\"))\n",
    "                .withColumn(\"persistent_failures\", lit(0).cast(IntegerType()))\n",
    "                .withColumn(\"resolved_failures\", lit(0).cast(IntegerType()))\n",
    "            )\n",
    "\n",
    "            # 3.1 Merge validaciones\n",
    "            delta_validations.alias(\"target\").merge(\n",
    "                df_merge.alias(\"source\"),\n",
    "                (col(\"target.execution_id\") == col(\"source.execution_id\"))\n",
    "                & (col(\"target.validation_id\") == col(\"source.validation_id\")),\n",
    "            ).whenMatchedUpdate(\n",
    "                set={\n",
    "                    \"new_failures\": col(\"source.new_failures\"),\n",
    "                    \"persistent_failures\": col(\"source.persistent_failures\"),\n",
    "                    \"resolved_failures\": col(\"source.resolved_failures\"),\n",
    "                }\n",
    "            ).whenNotMatchedInsert(\n",
    "                values={\n",
    "                    \"execution_id\": col(\"source.execution_id\"),\n",
    "                    \"validation_id\": col(\"source.validation_id\"),\n",
    "                    \"new_failures\": col(\"source.new_failures\"),\n",
    "                    \"persistent_failures\": col(\"source.persistent_failures\"),\n",
    "                    \"resolved_failures\": col(\"source.resolved_failures\"),\n",
    "                }\n",
    "            ).execute()\n",
    "\n",
    "            # 3.2 Merge evidencias\n",
    "            delta_evidences.alias(\"target\").merge(\n",
    "                df_current.alias(\"source\"),\n",
    "                (col(\"target.execution_id\") == col(\"source.execution_id\"))\n",
    "                & (col(\"target.validation_id\") == col(\"source.validation_id\"))\n",
    "                & (col(\"target.table_pk\") == col(\"source.table_pk\")),\n",
    "            ).whenMatchedUpdate(\n",
    "                set={\"is_new_failure\": col(\"source.is_new_failure\")}\n",
    "            ).execute()\n",
    "\n",
    "            return \"Primera ejecución procesada correctamente\"\n",
    "\n",
    "        else:\n",
    "            # 4. Lógica para siguientes ejecuciones: cargar evidencias previas y actuales\n",
    "\n",
    "            # 4.1 Evidencias de la última ejecución (alias 'curr' para joins posteriores)\n",
    "            df_current = (\n",
    "                spark.table(EVIDENCES_TABLE)\n",
    "                .filter(col(\"execution_id\") == latest_execution_id)\n",
    "                .select(\n",
    "                    col(\"evidence_id\"),\n",
    "                    col(\"execution_id\"),\n",
    "                    col(\"validation_id\"),\n",
    "                    col(\"table_pk\"),\n",
    "                    col(\"failed_value\")\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # 4.2 Evidencias de la ejecución anterior (alias 'prev' para joins posteriores)\n",
    "            df_prev = (\n",
    "                spark.table(EVIDENCES_TABLE)\n",
    "                .filter(col(\"execution_id\") == prev_execution_id)\n",
    "                .select(\n",
    "                    col(\"evidence_id\").alias(\"prev_evidence_id\"),\n",
    "                    col(\"execution_id\").alias(\"prev_execution_id\"),\n",
    "                    col(\"validation_id\").alias(\"prev_validation_id\"),\n",
    "                    col(\"table_pk\").alias(\"prev_table_pk\"),\n",
    "                    col(\"failed_value\").alias(\"prev_failed_value\")\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # 4.3 Calcula is_new_failure (actual - prev)\n",
    "            df_new_fails = df_current.join(\n",
    "                df_prev,\n",
    "                (col(\"validation_id\") == col(\"prev_validation_id\")) &\n",
    "                (col(\"table_pk\") == col(\"prev_table_pk\")) &\n",
    "                (col(\"failed_value\") == col(\"prev_failed_value\")),\n",
    "                \"left_anti\"\n",
    "            ).select(\n",
    "                \"execution_id\", \"validation_id\", \"table_pk\", \"failed_value\"\n",
    "            ).withColumn(\"is_new_failure_num\", lit(1).cast(IntegerType()))\n",
    "\n",
    "            # Conteo por ejecución y validación\n",
    "            df_new_fails_count = (\n",
    "                df_new_fails.groupBy(\"execution_id\", \"validation_id\")\n",
    "                .agg(F.sum(\"is_new_failure_num\").alias(\"new_failures\"))\n",
    "            )\n",
    "\n",
    "            # 4.4 Calcula resolved_failures (prev - current)\n",
    "\n",
    "            df_resolved_fails = df_prev.join(\n",
    "                df_current,\n",
    "                (col(\"prev_validation_id\") == col(\"validation_id\")) &\n",
    "                (col(\"prev_table_pk\") == col(\"table_pk\")) &\n",
    "                (col(\"prev_failed_value\") == col(\"failed_value\")),\n",
    "                \"left_anti\"\n",
    "            ).select(\n",
    "                \"prev_execution_id\", \"prev_validation_id\", \"prev_table_pk\", \"prev_failed_value\"\n",
    "            ).withColumn(\"is_resolved\", lit(1).cast(IntegerType()))\n",
    "\n",
    "            df_resolved_fails = df_resolved_fails.withColumnRenamed(\"prev_execution_id\", \"execution_id\")\\\n",
    "                                     .withColumnRenamed(\"prev_validation_id\", \"validation_id\")\\\n",
    "                                     .withColumnRenamed(\"prev_table_pk\", \"table_pk\")\\\n",
    "                                     .withColumnRenamed(\"prev_failed_value\", \"failed_value\")\n",
    "\n",
    "            df_resolved_count = (\n",
    "                df_resolved_fails.groupBy(\"execution_id\", \"validation_id\")\n",
    "                .agg(F.sum(\"is_resolved\").alias(\"resolved_failures\"))\n",
    "            )\n",
    "\n",
    "            # 4.5 Calcula persistent failures\n",
    "\n",
    "            df_persistent_fails = df_current.join(\n",
    "                df_prev,\n",
    "                (col(\"validation_id\") == col(\"prev_validation_id\")) &\n",
    "                (col(\"table_pk\") == col(\"prev_table_pk\")) &\n",
    "                (col(\"failed_value\") == col(\"prev_failed_value\")),\n",
    "                \"inner\"\n",
    "            ).select(\n",
    "                \"execution_id\", \"validation_id\", \"table_pk\", \"failed_value\"\n",
    "            ).withColumn(\"is_persistent\", lit(1).cast(IntegerType()))\n",
    "\n",
    "            df_persistent_count = (\n",
    "                df_persistent_fails.groupBy(\"execution_id\", \"validation_id\")\n",
    "                .agg(F.sum(\"is_persistent\").alias(\"persistent_failures\"))\n",
    "            )\n",
    "\n",
    "            # 4.6 Agrupa todos los cálculos\n",
    "            df_failures = df_new_fails_count.join(\n",
    "                df_persistent_count,\n",
    "                [\"execution_id\", \"validation_id\"],\n",
    "                \"outer\",\n",
    "            ).fillna({\"new_failures\": 0, \"persistent_failures\": 0}).join(\n",
    "                df_resolved_count,\n",
    "                [\"execution_id\", \"validation_id\"],\n",
    "                \"outer\",\n",
    "            ).fillna({\"resolved_failures\": 0})    \n",
    "\n",
    "            # 4.7 Merge final de validaciones\n",
    "            delta_validations.alias(\"target\").merge(\n",
    "                df_failures.alias(\"source\"),\n",
    "                (col(\"target.execution_id\") == col(\"source.execution_id\"))\n",
    "                & (col(\"target.validation_id\") == col(\"source.validation_id\")),\n",
    "            ).whenMatchedUpdate(\n",
    "                set={\n",
    "                    \"new_failures\": col(\"source.new_failures\"),\n",
    "                    \"persistent_failures\": col(\"source.persistent_failures\"),\n",
    "                    \"resolved_failures\": col(\"source.resolved_failures\"),\n",
    "                }\n",
    "            ).whenNotMatchedInsert(\n",
    "                values={\n",
    "                    \"execution_id\": col(\"source.execution_id\"),\n",
    "                    \"validation_id\": col(\"source.validation_id\"),\n",
    "                    \"new_failures\": col(\"source.new_failures\"),\n",
    "                    \"persistent_failures\": col(\"source.persistent_failures\"),\n",
    "                    \"resolved_failures\": col(\"source.resolved_failures\"),\n",
    "                }\n",
    "            ).execute()\n",
    "\n",
    "            # 4.8 Merge final de evidencias\n",
    "            delta_evidences.update(\n",
    "                condition=col(\"execution_id\") == latest_execution_id,\n",
    "                set={\"is_new_failure\": lit(False)}\n",
    "            )\n",
    "            \n",
    "            delta_evidences.alias(\"target\").merge(\n",
    "                df_new_fails.alias(\"source\"),\n",
    "                (col(\"target.execution_id\") == col(\"source.execution_id\"))\n",
    "                & (col(\"target.validation_id\") == col(\"source.validation_id\"))\n",
    "                & (col(\"target.table_pk\") == col(\"source.table_pk\"))\n",
    "                & (col(\"target.failed_value\") == col(\"source.failed_value\")),\n",
    "            ).whenMatchedUpdate(set={\"is_new_failure\": lit(True)}).execute()\n",
    "\n",
    "            return \"Ejecución procesada correctamente.\"\n",
    "\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        result = main()\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Fallo en la persistencia de evidencias: {e}\")\n",
    "    else:\n",
    "        dbutils.notebook.exit(f\"Éxito: evidencias procesadas\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04_calculate_persistence",
   "widgets": {
    "catalog_name": {
     "currentValue": "workspace",
     "nuid": "7315e0c4-56fa-424d-81d5-e364d7f49da6",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "workspace",
      "label": "Catálogo de UC donde residen las tablas",
      "name": "catalog_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "workspace",
      "label": "Catálogo de UC donde residen las tablas",
      "name": "catalog_name",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "schema_name": {
     "currentValue": "framework_dq",
     "nuid": "2fa8eb40-bc90-4def-8645-9cfd9c2265f6",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "dq_framework",
      "label": "Esquema de UC donde residen las tablas",
      "name": "schema_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "dq_framework",
      "label": "Esquema de UC donde residen las tablas",
      "name": "schema_name",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
