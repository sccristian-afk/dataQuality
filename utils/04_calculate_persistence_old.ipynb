{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f80eb5e-490f-4045-9a6a-33cdc486e3b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# MAGIC %md\n",
    "# MAGIC # Calcular persistencia de incidencias\n",
    "# MAGIC \n",
    "# MAGIC \n",
    "# MAGIC **L��gica:**\n",
    "# MAGIC 1.  `main()` orquesta todo el proceso.\n",
    "# MAGIC 2.  Busca ejecuciones en `dq_validations_traceability` que necesiten c��lculo de persistencia.\n",
    "# MAGIC 3.  Encuentra los pares de ejecuci��n (N vs N-1) usando `dq_execution_traceability`.\n",
    "# MAGIC 4.  Carga las evidencias (`dq_evidences`), filtrando por las particiones de fecha (N y N-1).\n",
    "# MAGIC 5.  Calcula las m��tricas de fallos nuevos, persistentes y resueltos.\n",
    "# MAGIC 6.  Actualiza (`MERGE`) la tabla `dq_validations_traceability` con estas m��tricas.\n",
    "# MAGIC 7.  Limpia los nulos restantes (ej: primeras ejecuciones) y los establece en 0.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1, 1. Imports y constantes\n",
    "from pyspark.sql.functions import col, lag, desc, row_number, count, when, lit, coalesce\n",
    "from pyspark.sql.window import Window\n",
    "from delta.tables import DeltaTable\n",
    "import sys\n",
    "\n",
    "# --- Widgets ---\n",
    "#dbutils.widgets.text(\"table_id\", \"\", \"Id de la tabla a validar\")\n",
    "dbutils.widgets.text(\"table_name\", \"maestro_demo\", \"Id de la tabla a validar\")\n",
    "dbutils.widgets.text(\"catalog_name\", \"workspace\", \"Catálogo de UC donde residen las tablas\")\n",
    "dbutils.widgets.text(\"schema_name\", \"dq_framework\", \"Esquema de UC donde residen las tablas\")\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 2, 2. Carga de librerías y definición de constantes/mapas\n",
    "\n",
    "CATALOG = dbutils.widgets.get(\"catalog_name\")\n",
    "SCHEMA = dbutils.widgets.get(\"schema_name\")\n",
    "\n",
    "EXECUTION_TABLE = f\"{CATALOG}.{SCHEMA}.dq_execution_traceability\"\n",
    "VALIDATIONS_TABLE = f\"{CATALOG}.{SCHEMA}.dq_validations_traceability\"\n",
    "EVIDENCES_TABLE = f\"{CATALOG}.{SCHEMA}.dq_evidences\"\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Flujo optimizado extremo para cálculo de persistencia sin crear DataFrame intermedio\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # --- 1. Validaciones pendientes ---\n",
    "        df_validations_to_process = (\n",
    "            spark.table(VALIDATIONS_TABLE)\n",
    "            .filter((col(\"status\").isin(\"PASSED\", \"FAILED\")) & col(\"persistent_failures\").isNull())\n",
    "            .select(\"execution_id\", \"validation_id\", \"rule_id\")\n",
    "            .distinct()\n",
    "        )\n",
    "\n",
    "        df_executions_to_process = df_validations_to_process.select(\"execution_id\").distinct()\n",
    "\n",
    "        if df_executions_to_process.isEmpty():\n",
    "            print(\"No se encontraron ejecuciones nuevas para procesar.\")\n",
    "            return \"Éxito: No hay nuevas ejecuciones para procesar.\"\n",
    "\n",
    "        print(f\"Ejecuciones a procesar: {df_executions_to_process.count()}\")\n",
    "\n",
    "        # --- 2. Encontrar pares de Ejecuci��n (N vs N-1) ---\n",
    "        window_prev_exec = Window.partitionBy(\"table_id\").orderBy(\"execution_timestamp\")\n",
    "        df_execution_pairs = (\n",
    "            spark.table(EXECUTION_TABLE)\n",
    "            .filter(col(\"status\") == \"SUCCESS\")\n",
    "            .withColumn(\"prev_execution_id\", lag(\"execution_id\").over(window_prev_exec))\n",
    "            .withColumn(\"prev_execution_date\", lag(\"execution_timestamp\").over(window_prev_exec))\n",
    "            .join(df_executions_to_process, \"execution_id\")\n",
    "            .filter(col(\"prev_execution_id\").isNotNull())\n",
    "            .select(\n",
    "                col(\"execution_id\").alias(\"current_execution_id\"),\n",
    "                col(\"execution_date\").alias(\"current_execution_date\"),\n",
    "                \"prev_execution_id\",\n",
    "                col(\"prev_execution_date\"),\n",
    "                \"table_id\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if df_execution_pairs.isEmpty():\n",
    "            print(\"Ejecuciones nuevas encontradas, pero no tienen una ejecuci��n anterior v��lida para comparar\")\n",
    "            df_evidences = spark.createDataFrame([], schema=spark.table(EVIDENCES_TABLE).schema)\n",
    "        else:\n",
    "            # --- 3. Evidencias relevantes ---\n",
    "            relevant_executions = df_execution_pairs.select(\"current_execution_id\").union(\n",
    "                df_execution_pairs.select(\"prev_execution_id\")\n",
    "            ).distinct()\n",
    "            df_evidences = (\n",
    "                spark.table(EVIDENCES_TABLE)\n",
    "                .join(relevant_executions,\n",
    "                      (col(\"execution_id\") == col(\"current_execution_id\")) |\n",
    "                      (col(\"execution_id\") == col(\"prev_execution_id\")),\n",
    "                      \"inner\"\n",
    "                )\n",
    "                .select(\"execution_id\", \"validation_id\", \"table_pk\")\n",
    "                .distinct()\n",
    "                #.cache()\n",
    "            )\n",
    "\n",
    "        # --- 4. Preparar DeltaTable ---\n",
    "        delta_validations_table = DeltaTable.forName(spark, VALIDATIONS_TABLE)\n",
    "\n",
    "        # --- 5. MERGE directo con cálculo de métricas usando window ---\n",
    "        if len(df_evidences.take(1)) > 0:\n",
    "            window_spec = Window.partitionBy(\"validation_id\", \"table_pk\").orderBy(\"execution_id\")\n",
    "            df_for_merge = df_evidences.withColumn(\n",
    "                \"prev_execution_id_lag\", lag(\"execution_id\").over(window_spec)\n",
    "            ).withColumn(\n",
    "                \"new_failures\", when(col(\"execution_id\") != col(\"prev_execution_id_lag\"), 1).otherwise(0)\n",
    "            ).withColumn(\n",
    "                \"persistent_failures\", when(col(\"execution_id\") == col(\"prev_execution_id_lag\"), 1).otherwise(0)\n",
    "            ).withColumn(\n",
    "                \"resolved_failures\", when(col(\"prev_execution_id_lag\").isNotNull() & (col(\"execution_id\") != col(\"prev_execution_id_lag\")), 1).otherwise(0)\n",
    "            ).groupBy(\"execution_id\", \"validation_id\").agg(\n",
    "                sum(\"new_failures\").alias(\"new_failures\"),\n",
    "                sum(\"persistent_failures\").alias(\"persistent_failures\"),\n",
    "                sum(\"resolved_failures\").alias(\"resolved_failures\")\n",
    "            ).fillna(0)\n",
    "\n",
    "            df_for_merge = df_for_merge.repartition(\"execution_id\", \"validation_id\")\n",
    "            delta_validations_table.alias(\"target\").merge(\n",
    "                df_for_merge.alias(\"source\"),\n",
    "                (col(\"target.execution_id\") == col(\"source.execution_id\")) &\n",
    "                (col(\"target.validation_id\") == col(\"source.validation_id\"))\n",
    "            ).whenMatchedUpdateAll().execute()\n",
    "            print(\"Métricas de persistencia actualizadas\")\n",
    "\n",
    "        # --- 6. Rellenar nulos restantes ---\n",
    "        delta_validations_table.update(\n",
    "            condition=(\n",
    "                col(\"status\").isin(\"PASSED\", \"FAILED\") &\n",
    "                (col(\"persistent_failures\").isNull() |\n",
    "                 col(\"new_failures\").isNull() |\n",
    "                 col(\"resolved_failures\").isNull())\n",
    "            ),\n",
    "            set={\"persistent_failures\": 0, \"new_failures\": 0, \"resolved_failures\": 0}\n",
    "        )\n",
    "        print(\"Nulos restantes rellenados.\")\n",
    "\n",
    "        # --- 7. Liberar memoria ---\n",
    "        if 'df_evidences' in locals() and df_evidences.is_cached:\n",
    "            df_evidences.unpersist()\n",
    "\n",
    "        return \"Cálculo de persistencia finalizado.\"\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fatal durante el cálculo de persistencia: {e}\")\n",
    "        if 'df_evidences' in locals() and df_evidences.is_cached:\n",
    "            df_evidences.unpersist()\n",
    "        raise e\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04_calculate_persistence_old",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
