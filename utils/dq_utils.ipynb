{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cddec712-d34c-411d-ad0d-5f3ffd85e79a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Lenguaje del Notebook: Python\n",
    "# VERSI√ìN 12: Alineado con el esquema de tablas final del usuario.\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC # üõ†Ô∏è Librer√≠a de Utilidades del Framework DQ (v12)\n",
    "# MAGIC \n",
    "# MAGIC Contiene funciones auxiliares de logging.\n",
    "# MAGIC Invocado v√≠a `%run` desde el notebook orquestador.\n",
    "# MAGIC \n",
    "# MAGIC **Actualizaciones v12:**\n",
    "# MAGIC * **Corregido:** Se han actualizado todas las funciones de logging para que\n",
    "# MAGIC   coincidan *exactamente* con el esquema de tablas final (el que me has pasado).\n",
    "# MAGIC * `log_execution_start`: Ahora usa `table_id` (en lugar de `master_id`), `duration`, `validations_executed`, `validations_failed`. Se elimina `quality_score`.\n",
    "# MAGIC * `log_execution_finish`: Se actualiza para coincidir con los nuevos nombres de columna.\n",
    "# MAGIC * `log_validations_traceability`: El esquema ya era correcto, no hay cambios.\n",
    "# MAGIC * `log_evidences_staging`: Ahora escribe `failed_value` (en lugar de `failed_record`) y omite `is_new_failure` (que se calcula en post-proceso).\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1, 1. Imports\n",
    "from pyspark.sql.functions import col, lit, current_timestamp\n",
    "from pyspark.sql.types import StringType, DateType\n",
    "from delta.tables import DeltaTable\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, TimestampType, DateType, DecimalType, IntegerType, LongType)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 2, 2. Funciones de Log de Trazabilidad de Ejecuci√≥n (¬°CORREGIDO!)\n",
    "def log_execution_start(execution_id: str, exec_timestamp: datetime , table_id: str, trace_table: str):\n",
    "    \"\"\"\n",
    "    Inserta un registro inicial en la tabla dq_execution_traceability marcando la ejecuci√≥n como 'RUNNING'\n",
    "    \n",
    "    Args:\n",
    "        execution_id (str): Identificador √∫nico de la ejecuci√≥n.\n",
    "        exec_timestamp (datetime): Fecha y hora de inicio de la ejecuci√≥n.\n",
    "        table_id (str): Identificador de la tabla que se valida.\n",
    "        trace_table (DataFrame): DataFrame de trazabilidad donde se guarda el log.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Logging inicio de ejecuci√≥n {execution_id} para table_id {table_id}...\")\n",
    "    try:\n",
    "        schema = StructType([\n",
    "            StructField(\"execution_id\", StringType(), True),\n",
    "            StructField(\"table_id\", StringType(), True),\n",
    "            StructField(\"execution_timestamp\", TimestampType(), True),\n",
    "            StructField(\"execution_date\", DateType(), True),\n",
    "            StructField(\"status\", StringType(), True),\n",
    "            StructField(\"duration_seconds\", DecimalType(10,2), True),\n",
    "            StructField(\"validations_executed\", IntegerType(), True),\n",
    "            StructField(\"validations_failed\", IntegerType(), True)\n",
    "        ])\n",
    "\n",
    "        # Derivamos la fecha (tipo Date) a partir del timestamp\n",
    "        exec_date = exec_timestamp.date()\n",
    "\n",
    "        start_log_df = spark.createDataFrame(\n",
    "            data=[(\n",
    "                execution_id, table_id, exec_timestamp, exec_date, \n",
    "                \"RUNNING\", None, None, None\n",
    "            )],\n",
    "            schema=schema\n",
    "        )\n",
    "        \n",
    "        (start_log_df.write\n",
    "         .format(\"delta\")\n",
    "         .mode(\"append\")\n",
    "         .saveAsTable(trace_table))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error fatal al iniciar el log de trazabilidad: {e}\")\n",
    "        raise\n",
    "\n",
    "def log_execution_finish(execution_id, status, duration, validations_exec, validations_fail, trace_table):\n",
    "    \"\"\"\n",
    "    Actualiza el registro de la ejecuci√≥n en dq_execution_traceability\n",
    "    con el estado final (SUCCESS/FAILED) y los KPIs resumen.\n",
    "    Usa el nuevo esquema v12.\n",
    "    \"\"\"\n",
    "    print(f\"Logging fin de ejecuci√≥n {execution_id}. Estado: {status}\")\n",
    "    try:\n",
    "        \n",
    "        schema = StructType([\n",
    "            StructField(\"execution_id\", StringType(), True),\n",
    "            StructField(\"status\", StringType(), True),\n",
    "            StructField(\"duration_seconds\", DecimalType(10,2), True),\n",
    "            StructField(\"validations_executed\", IntegerType(), True),\n",
    "            StructField(\"validations_failed\", IntegerType(), True)\n",
    "        ])\n",
    "\n",
    "        delta_trace_table = DeltaTable.forName(spark, trace_table)\n",
    "        \n",
    "        # DataFrame con los datos a actualizar\n",
    "        update_df = spark.createDataFrame(\n",
    "            data=[(execution_id, status, duration, validations_exec, validations_fail)],\n",
    "            schema=schema\n",
    "        )\n",
    "\n",
    "        # Actualizar la fila existente bas√°ndonos en el execution_id\n",
    "        (delta_trace_table.alias(\"target\")\n",
    "         .merge(update_df.alias(\"source\"), \"target.execution_id = source.execution_id\")\n",
    "         .whenMatchedUpdate(set={\n",
    "             \"status\": \"source.status\",\n",
    "             \"duration\": \"source.duration\",\n",
    "             \"validations_executed\": \"source.validations_executed\",\n",
    "             \"validations_failed\": \"source.validations_failed\"\n",
    "         })\n",
    "         .execute())\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error al *finalizar* el log de trazabilidad para {execution_id}: {e}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 3, 3. Funciones de Log de Resultados de Validaci√≥n (Sin Cambios)\n",
    "def log_validations_traceability(execution_id, exec_date, validation_id, rule_id, status, perimeter, failed_count, validations_table):\n",
    "    \"\"\"\n",
    "    Escribe el resultado (PASSED/FAILED/ERROR) y el conteo de una regla\n",
    "    en la tabla dq_validations_traceability.\n",
    "    (El esquema v12 coincide con el anterior).\n",
    "    \"\"\"\n",
    "    print(f\"  > Logging resultado para validaci√≥n {validation_id}: {status}, {failed_count} fallos.\")\n",
    "    try:\n",
    "        schema = StructType([\n",
    "            StructField(\"validation_trace_id\", StringType(), True),\n",
    "            StructField(\"execution_id\", StringType(), True),\n",
    "            StructField(\"validation_id\", StringType(), True),\n",
    "            StructField(\"rule_id\", StringType(), True),\n",
    "            StructField(\"execution_date\", DateType(), True),\n",
    "            StructField(\"status\", StringType(), True),\n",
    "            StructField(\"perimeter\", LongType(), True),\n",
    "            StructField(\"failed_records_count\", LongType(), True),\n",
    "            StructField(\"persistent_failures\", LongType(), True),\n",
    "            StructField(\"new_failures\", LongType(), True),\n",
    "            StructField(\"resolved_failures\", LongType(), True)\n",
    "        ])\n",
    "\n",
    "        trace_df = spark.createDataFrame(\n",
    "            data=[(\n",
    "                str(uuid.uuid4()), execution_id, validation_id, rule_id, \n",
    "                exec_date, status, perimeter, failed_count, 0, 0, 0\n",
    "            )],\n",
    "            schema=schema\n",
    "        )\n",
    "        \n",
    "        (trace_df.write\n",
    "         .format(\"delta\")\n",
    "         .mode(\"append\")\n",
    "         .saveAsTable(validations_table))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error al escribir en log de validaci√≥n para {validation_id}: {e}\")\n",
    "        print(f\"  > AVISO: No se pudo loguear el resultado para la validaci√≥n {validation_id}.\")\n",
    "\n",
    "\n",
    "def log_evidences_staging(df_failed, staging_table_name, execution_id, exec_date, validation_id, primary_key_col, failed_field, catalog, schema):\n",
    "    \"\"\"\n",
    "    Escribe los registros fallidos en una tabla de STAGING temporal.\n",
    "    Usa el nuevo esquema v12 ('failed_value').\n",
    "    \"\"\"\n",
    "    \n",
    "    if df_failed.isEmpty():\n",
    "        print(\"  > No hay registros fallidos que escribir en staging.\")\n",
    "        return\n",
    "\n",
    "    print(f\"  > Transformando y escribiendo {df_failed.count()} registros fallidos en STAGING: {staging_table_name}...\")\n",
    "    \n",
    "    try:\n",
    "\n",
    "        # 1. Selecciona las columnas que peuden causar problemas al colisionar el nombre en algunos casos:\n",
    "        #    ej: primary_key_col == failed_field\n",
    "        df_transformed = (df_failed.select(\n",
    "            col(primary_key_col).alias(\"table_pk\"),\n",
    "            col(failed_field).alias(\"failed_value\")\n",
    "        ))\n",
    "\n",
    "        # Transformar el DataFrame al esquema de 'dq_evidences'\n",
    "        df_to_log = (df_transformed\n",
    "                     .withColumn(\"execution_id\", lit(execution_id))\n",
    "                     .withColumn(\"validation_id\", lit(validation_id))\n",
    "                     .withColumn(\"execution_date\", lit(exec_date)) \n",
    "                     .select(\n",
    "                         lit(str(uuid.uuid4())).alias(\"evidence_id\"),\n",
    "                         col(\"execution_id\"),\n",
    "                         col(\"validation_id\"),\n",
    "                         col(\"execution_date\"),\n",
    "                         col(\"failed_value\").cast(\"string\"),\n",
    "                         col(\"table_pk\").cast(\"string\")\n",
    "                         # 'is_new_failure' se a√±ade en el script de unificaci√≥n/persistencia\n",
    "                     )\n",
    "                    )\n",
    "\n",
    "        full_table_name = f\"{catalog}.{schema}.{staging_table_name}\"\n",
    "\n",
    "        # Escribir en la tabla de staging SOBRESCRIBIENDO\n",
    "        (df_to_log.write\n",
    "         .format(\"delta\")\n",
    "         .mode(\"append\") \n",
    "         .option(\"mergeSchema\", \"true\") \n",
    "         .saveAsTable(full_table_name))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error al escribir en la tabla STAGING {staging_table_name}: {e}\")\n",
    "        print(f\"  > AVISO: No se pudieron guardar las evidencias para la validaci√≥n {validation_id}.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "dq_utils",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
