{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18b4721b-e5ee-4473-bb7a-b099c433b22a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "Unificar Evidencias (Tablas staging -> Final)\n",
    "\n",
    "Este notebook se ejecuta después de todas las tareas de validación\n",
    "Es idempotente (seguro de ejecutarse varias veces sin errores) y seguro ante fallos\n",
    "\n",
    "Lógica\n",
    "1. 'main()' orquesta el proceso.\n",
    "2. Lee 'table_config' para encontrar todas las tablas 'staging_evidences_table'\n",
    "3. Las une dinámicamente ('unionByName') para crear un DataFrame 'df_staging_completo'\n",
    "4. Anti-Duplicación: obtiene los execution_id's de staging y los cruza con\n",
    "   'dq_evidences' para encontrar los IDs que aún no han sido procesados\n",
    "5. Filtra 'df_staging_completo' para quedarse solo con los datos nuevos\n",
    "6. Realiza 'APPEND' de los datos nuevos a la tabla final 'dq_evidences'\n",
    "7. Limpieza: si el 'APPEND' es exitoso, borra los execution_id's procesados\n",
    "   de todas las tablas de staging.\n",
    "'''\n",
    "\n",
    "# 1. Imports y Widgets\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, broadcast\n",
    "from functools import reduce\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Widgets\n",
    "def get_mandatory_widget(name, label=None):\n",
    "    if label is None:\n",
    "        label = name\n",
    "    try:\n",
    "        # Intentar crear el widget\n",
    "        dbutils.widgets.text(name, \"\", label)\n",
    "    except:\n",
    "        # Si ya existe, ignorar el error\n",
    "        pass\n",
    "    value = dbutils.widgets.get(name)\n",
    "    if not value.strip():\n",
    "        raise ValueError(f\"Parámetro obligatorio: {name}\")\n",
    "    return value\n",
    "\n",
    "CATALOG = get_mandatory_widget(\"catalog_name\",\"Catálogo de UC donde residen las tablas\")\n",
    "SCHEMA = get_mandatory_widget(\"schema_name\", \"Esquema de UC donde residen las tablas\")\n",
    "\n",
    "TABLE_CONFIG = f\"{CATALOG}.{SCHEMA}.dq_tables_config\"\n",
    "EVIDENCES_TABLE = f\"{CATALOG}.{SCHEMA}.dq_evidences\"\n",
    "\n",
    "# 2. Función Principal (main)\n",
    "def main():\n",
    "    \n",
    "    try:\n",
    "        # 1. Leer la configuración maestra        \n",
    "        master_configs = (spark.table(TABLE_CONFIG)\n",
    "                          .select(\"table_name\", \"staging_evidences_table\")\n",
    "                          .collect())\n",
    "        \n",
    "        if not master_configs:\n",
    "            raise Exception(f\"No se encontró configuración de maestros en {TABLE_CONFIG}\")\n",
    "\n",
    "        # 2. Listar todos los DataFrames de staging con evidencias sin procesar\n",
    "        dfs_to_union = []\n",
    "        staging_table_paths = []\n",
    "        \n",
    "        df_processed_ids = spark.table(EVIDENCES_TABLE).select(\"execution_id\").distinct()\n",
    "\n",
    "        for config in master_configs:\n",
    "            staging_table = config.staging_evidences_table\n",
    "            staging_table_path = f\"{CATALOG}.{SCHEMA}.{staging_table}\"\n",
    "            staging_table_paths.append(staging_table_path) \n",
    "            \n",
    "            if spark.catalog.tableExists(staging_table_path):\n",
    "                df_staging = spark.table(staging_table_path)\n",
    "                \n",
    "                df_filtered = df_staging.join(\n",
    "                    broadcast(df_processed_ids),\n",
    "                    \"execution_id\",\n",
    "                    \"left_anti\"\n",
    "                )\n",
    "\n",
    "                dfs_to_union.append(df_filtered)\n",
    "                \n",
    "            else:\n",
    "                print(f\"  > AVISO: No se encontró la tabla {staging_table_path}\")\n",
    "\n",
    "        # 3. Unificar DataFrames\n",
    "        if not dfs_to_union:\n",
    "            print(\"No hay datos nuevos en ninguna tabla de staging.\")\n",
    "            return 0\n",
    "        \n",
    "        df_staging_completo: DataFrame = dfs_to_union[0]\n",
    "        for df in dfs_to_union[1:]:\n",
    "            df_staging_completo = df_staging_completo.unionByName(df, allowMissingColumns=True)\n",
    "        df_staging_completo = df_staging_completo.distinct()\n",
    "\n",
    "        total_rows = df_staging_completo.count()\n",
    "\n",
    "        # 4. Append a tabla final\n",
    "        if total_rows > 0:\n",
    "            df_staging_completo.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").saveAsTable(EVIDENCES_TABLE)\n",
    "        else:\n",
    "            print(\"No se encontraron registros nuevos para añadir a la tabla final.\")\n",
    "            return 0\n",
    "\n",
    "        # 5. Limpieza segura de staging\n",
    "        for table_path in staging_table_paths:\n",
    "            if spark.catalog.tableExists(table_path):\n",
    "                try:                  \n",
    "                    spark.sql(f\"DROP TABLE {table_path}\")\n",
    "                    print(f\"  > Tabla eliminada correctamente: {table_path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  > AVISO: Fallo al limpiar {table_path}: {e}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error fatal durante la unificación de evidencias: {e}\")\n",
    "        raise e\n",
    "    \n",
    "    finally:\n",
    "        print(\"Proceso completado\")\n",
    "    \n",
    "    return total_rows\n",
    "\n",
    "# 3. Punto de Entrada de Ejecución\n",
    "if __name__ == \"__main__\":\n",
    "    rows_processed = 0\n",
    "    try:\n",
    "        rows_processed = main()\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Fallo en la unificación de evidencias: {e}\")\n",
    "    else:\n",
    "        dbutils.notebook.exit(f\"Éxito: {rows_processed} evidencias nuevas unificadas y añadidas.\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03_unify_evidences",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
