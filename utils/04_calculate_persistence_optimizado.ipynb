{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "86529df4-1b24-4827-805a-a75aeab89c25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Flujo optimizado extremo para cálculo de persistencia sin crear DataFrame intermedio\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # --- 1. Validaciones pendientes ---\n",
    "        df_validations_to_process = (\n",
    "            spark.table(VALIDATIONS_TABLE)\n",
    "            .filter((col(\"status\").isin(\"PASSED\", \"FAILED\")) & col(\"persistent_failures\").isNull())\n",
    "            .select(\"execution_id\", \"validation_id\", \"rule_id\")\n",
    "            .distinct()\n",
    "        )\n",
    "\n",
    "        df_executions_to_process = df_validations_to_process.select(\"execution_id\").distinct()\n",
    "        if df_executions_to_process.isEmpty():\n",
    "            print(\"No hay nuevas ejecuciones para procesar.\")\n",
    "            return \"Éxito: No hay nuevas ejecuciones para procesar.\"\n",
    "        print(f\"Ejecuciones a procesar: {df_executions_to_process.count()}\")\n",
    "\n",
    "        # --- 2. Pares N vs N-1 ---\n",
    "        window_prev_exec = Window.partitionBy(\"table_id\").orderBy(\"execution_timestamp\")\n",
    "        df_execution_pairs = (\n",
    "            spark.table(EXECUTION_TABLE)\n",
    "            .filter(col(\"status\") == \"SUCCESS\")\n",
    "            .withColumn(\"prev_execution_id\", lag(\"execution_id\").over(window_prev_exec))\n",
    "            .join(df_executions_to_process, \"execution_id\")\n",
    "            .filter(col(\"prev_execution_id\").isNotNull())\n",
    "            .select(\n",
    "                col(\"execution_id\").alias(\"current_execution_id\"),\n",
    "                \"prev_execution_id\",\n",
    "                \"table_id\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if df_execution_pairs.isEmpty():\n",
    "            print(\"No hay pares N vs N-1 para procesar.\")\n",
    "            df_evidences = spark.createDataFrame([], schema=spark.table(EVIDENCES_TABLE).schema)\n",
    "        else:\n",
    "            # --- 3. Evidencias relevantes ---\n",
    "            relevant_executions = df_execution_pairs.select(\"current_execution_id\").union(\n",
    "                df_execution_pairs.select(\"prev_execution_id\")\n",
    "            ).distinct()\n",
    "            df_evidences = (\n",
    "                spark.table(EVIDENCES_TABLE)\n",
    "                .join(relevant_executions,\n",
    "                      (col(\"execution_id\") == col(\"current_execution_id\")) |\n",
    "                      (col(\"execution_id\") == col(\"prev_execution_id\")),\n",
    "                      \"inner\"\n",
    "                )\n",
    "                .select(\"execution_id\", \"validation_id\", \"table_pk\")\n",
    "                .distinct()\n",
    "            )\n",
    "\n",
    "        # --- 4. Preparar DeltaTable ---\n",
    "        delta_validations_table = DeltaTable.forName(spark, VALIDATIONS_TABLE)\n",
    "\n",
    "        # --- 5. MERGE directo con cálculo de métricas usando window ---\n",
    "        if len(df_evidences.take(1)) > 0:\n",
    "            window_spec = Window.partitionBy(\"validation_id\", \"table_pk\").orderBy(\"execution_id\")\n",
    "            df_for_merge = df_evidences.withColumn(\n",
    "                \"prev_execution_id_lag\", lag(\"execution_id\").over(window_spec)\n",
    "            ).withColumn(\n",
    "                \"new_failures\", when(col(\"execution_id\") != col(\"prev_execution_id_lag\"), 1).otherwise(0)\n",
    "            ).withColumn(\n",
    "                \"persistent_failures\", when(col(\"execution_id\") == col(\"prev_execution_id_lag\"), 1).otherwise(0)\n",
    "            ).withColumn(\n",
    "                \"resolved_failures\", when(col(\"prev_execution_id_lag\").isNotNull() & (col(\"execution_id\") != col(\"prev_execution_id_lag\")), 1).otherwise(0)\n",
    "            ).groupBy(\"execution_id\", \"validation_id\").agg(\n",
    "                sum(\"new_failures\").alias(\"new_failures\"),\n",
    "                sum(\"persistent_failures\").alias(\"persistent_failures\"),\n",
    "                sum(\"resolved_failures\").alias(\"resolved_failures\")\n",
    "            ).fillna(0)\n",
    "\n",
    "            df_for_merge = df_for_merge.repartition(\"execution_id\", \"validation_id\")\n",
    "            delta_validations_table.alias(\"target\").merge(\n",
    "                df_for_merge.alias(\"source\"),\n",
    "                (col(\"target.execution_id\") == col(\"source.execution_id\")) &\n",
    "                (col(\"target.validation_id\") == col(\"source.validation_id\"))\n",
    "            ).whenMatchedUpdateAll().execute()\n",
    "            print(\"Métricas de persistencia actualizadas en un solo paso.\")\n",
    "\n",
    "        # --- 6. Rellenar nulos restantes ---\n",
    "        delta_validations_table.update(\n",
    "            condition=(\n",
    "                col(\"status\").isin(\"PASSED\", \"FAILED\") &\n",
    "                (col(\"persistent_failures\").isNull() |\n",
    "                 col(\"new_failures\").isNull() |\n",
    "                 col(\"resolved_failures\").isNull())\n",
    "            ),\n",
    "            set={\"persistent_failures\": 0, \"new_failures\": 0, \"resolved_failures\": 0}\n",
    "        )\n",
    "        print(\"Nulos restantes rellenados.\")\n",
    "\n",
    "        # --- 7. Liberar memoria ---\n",
    "        if 'df_evidences' in locals() and df_evidences.is_cached:\n",
    "            df_evidences.unpersist()\n",
    "\n",
    "        return \"Cálculo de persistencia finalizado.\"\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fatal durante el cálculo de persistencia: {e}\")\n",
    "        if 'df_evidences' in locals() and df_evidences.is_cached:\n",
    "            df_evidences.unpersist()\n",
    "        raise e\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04_calculate_persistence_optimizado",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
