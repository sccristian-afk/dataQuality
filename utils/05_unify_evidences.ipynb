{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18b4721b-e5ee-4473-bb7a-b099c433b22a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Lenguaje del Notebook: Python\n",
    "# VERSI√ìN 14: Unificador de Evidencias (Idempotente y Transaccional)\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC # üì• 5. Unificar Evidencias (Staging -> Final) (v14)\n",
    "# MAGIC \n",
    "# MAGIC Este notebook se ejecuta DESPU√âS de todas las tareas paralelas de validaci√≥n.\n",
    "# MAGIC Es idempotente y seguro ante fallos.\n",
    "# MAGIC \n",
    "# MAGIC **L√≥gica (Transaccional):**\n",
    "# MAGIC 1.  `main()` orquesta el proceso.\n",
    "# MAGIC 2.  Lee `table_config` para encontrar todas las tablas `staging_evidences_table`.\n",
    "# MAGIC 3.  Las une din√°micamente (`unionByName`) para crear un DataFrame `df_staging_completo`.\n",
    "# MAGIC 4.  **Anti-Duplicaci√≥n:** Obtiene los `execution_id`s de staging y los cruza con\n",
    "# MAGIC     `dq_evidences` para encontrar los IDs que *a√∫n no* han sido procesados.\n",
    "# MAGIC 5.  Filtra `df_staging_completo` para quedarse solo con los datos nuevos.\n",
    "# MAGIC 6.  **`APPEND`** los datos nuevos a la tabla final `dq_evidences`.\n",
    "# MAGIC 7.  **Limpieza:** Si el `APPEND` es exitoso, `DELETE` los `execution_id`s procesados\n",
    "# MAGIC     de todas las tablas de staging.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1, 1. Imports y Constantes\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, broadcast\n",
    "from functools import reduce\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# --- Widgets ---\n",
    "#dbutils.widgets.text(\"table_id\", \"\", \"Id de la tabla a validar\")\n",
    "dbutils.widgets.text(\"table_name\", \"maestro_demo\", \"Id de la tabla a validar\")\n",
    "dbutils.widgets.text(\"catalog_name\", \"workspace\", \"Cat√°logo de UC donde residen las tablas\")\n",
    "dbutils.widgets.text(\"schema_name\", \"dq_framework\", \"Esquema de UC donde residen las tablas\")\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 2, 2. Carga de librer√≠as y definici√≥n de constantes/mapas\n",
    "\n",
    "CATALOG = dbutils.widgets.get(\"catalog_name\")\n",
    "SCHEMA = dbutils.widgets.get(\"schema_name\")\n",
    "\n",
    "TABLE_CONFIG = f\"{CATALOG}.{SCHEMA}.dq_tables_config\"\n",
    "EVIDENCES_TABLE = f\"{CATALOG}.{SCHEMA}.dq_evidences\"\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 2, 2. Funci√≥n Principal (main)\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Flujo principal de unificaci√≥n de evidencias (v14).\n",
    "    \"\"\"\n",
    "    print(\"--- Iniciando Script de Unificaci√≥n de Evidencias (v14) ---\")\n",
    "    \n",
    "    try:\n",
    "        # --- 1. Leer la Configuraci√≥n Maestra ---\n",
    "        print(f\"Leyendo la configuraci√≥n de maestros desde: {TABLE_CONFIG}\")\n",
    "        \n",
    "        master_configs = (spark.table(TABLE_CONFIG)\n",
    "                          .select(\"table_name\", \"staging_evidences_table\")\n",
    "                          .collect())\n",
    "        \n",
    "        if not master_configs:\n",
    "            raise Exception(f\"No se encontr√≥ configuraci√≥n de maestros en {TABLE_CONFIG}\")\n",
    "\n",
    "        # --- 2. Recolectar todos los DataFrames de Staging ---\n",
    "        dfs_to_union = []\n",
    "        staging_table_paths = [] # Guardar las rutas para la limpieza final\n",
    "        \n",
    "        print(f\"Recolectando datos de {len(master_configs)} tablas de staging...\")\n",
    "\n",
    "        \n",
    "        if spark.catalog.tableExists(full_table_name):\n",
    "            print(f\"‚úÖ La tabla {full_table_name} existe\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è La tabla {full_table_name} NO existe\")\n",
    "\n",
    "        for config in master_configs:\n",
    "            staging_table = config.staging_evidences_table\n",
    "            staging_table_path = f\"{catalog}.{schema}.{staging_table}\"\n",
    "            staging_table_paths.append(staging_table_path) # A√±adir a la lista para la limpieza\n",
    "            \n",
    "            if spark.catalog.tableExists(staging_table_path):\n",
    "                print(f\"  > Leyendo datos de: {staging_table_path}\")\n",
    "                df_staging = spark.table(staging_table_path)\n",
    "                \n",
    "                if not df_staging.isEmpty():\n",
    "                    dfs_to_union.append(df_staging)\n",
    "                else:\n",
    "                    print(f\"  > AVISO: La tabla {staging_table_path} existe pero est√° vac√≠a. Saltando.\")\n",
    "            else:\n",
    "                print(f\"  > AVISO: No se encontr√≥ la tabla {staging_table_path}. Saltando.\")\n",
    "\n",
    "        # --- 3. Unificar los DataFrames ---\n",
    "        if not dfs_to_union:\n",
    "            print(\"No se encontraron datos en ninguna tabla de staging. No hay nada que unificar.\")\n",
    "            return 0 # Finaliza con √©xito, 0 registros procesados\n",
    "\n",
    "        print(f\"Unificando {len(dfs_to_union)} DataFrames de staging usando 'unionByName'...\")\n",
    "        \n",
    "        #versi√≥n previa, compleja\n",
    "        '''\n",
    "        df_staging_completo = reduce(\n",
    "            lambda df1, df2: df1.unionByName(df2, allowMissingColumns=True), \n",
    "            dfs_to_union\n",
    "        ).distinct() # Deduplicar por si acaso el orquestador se re-ejecut√≥\n",
    "        '''\n",
    "\n",
    "        # Inicializamos el DataFrame final como vac√≠o\n",
    "        df_staging_completo: DataFrame = None\n",
    "\n",
    "        for df in dfs_to_union:\n",
    "            if df_staging_completo is None:\n",
    "                # La primera iteraci√≥n, asignamos el primer DataFrame\n",
    "                df_staging_completo = df\n",
    "            else:\n",
    "                # Union por nombre de columnas, rellenando columnas faltantes\n",
    "                df_staging_completo = df_staging_completo.unionByName(df, allowMissingColumns=True)\n",
    "\n",
    "        # Eliminamos duplicados\n",
    "        df_staging_completo = df_staging_completo.distinct()#.cache()\n",
    "\n",
    "        # --- 4. L√≥gica Anti-Duplicaci√≥n (Idempotencia) ---\n",
    "        print(\"Buscando ejecuciones ya procesadas para evitar duplicados...\")\n",
    "        \n",
    "        # Obtenemos todos los execution_id que han llegado a staging\n",
    "        exec_ids_in_staging = (df_staging_completo\n",
    "                               .select(\"execution_id\")\n",
    "                               .distinct()\n",
    "                              )\n",
    "        \n",
    "        # Obtenemos los execution_id que YA est√°n en la tabla final\n",
    "        df_processed_ids = (spark.table(EVIDENCES_TABLE)\n",
    "                            .select(\"execution_id\")\n",
    "                            .distinct()\n",
    "                           )\n",
    "        \n",
    "        # Filtramos 'exec_ids_in_staging' para quedarnos solo con los que\n",
    "        # NO est√°n en 'df_processed_ids' (LEFT_ANTI JOIN)\n",
    "        df_unprocessed_ids = exec_ids_in_staging.join(\n",
    "            broadcast(df_processed_ids), # Broadcast para optimizar el join\n",
    "            \"execution_id\",\n",
    "            \"left_anti\"\n",
    "        )#.cache()\n",
    "        \n",
    "        unprocessed_count = df_unprocessed_ids.count()\n",
    "        if unprocessed_count == 0:\n",
    "            print(\"No hay ejecuciones nuevas en staging. Los datos ya fueron procesados en un run anterior.\")\n",
    "            # (Procedemos a la limpieza por si acaso)\n",
    "        else:\n",
    "            print(f\"Se han encontrado {unprocessed_count} ejecuciones nuevas para procesar.\")\n",
    "        \n",
    "        # Filtramos el DataFrame de staging completo para quedarnos solo con los datos nuevos\n",
    "        df_datos_nuevos = df_staging_completo.join(\n",
    "            broadcast(df_unprocessed_ids),\n",
    "            \"execution_id\",\n",
    "            \"inner\"\n",
    "        )\n",
    "        \n",
    "        total_rows = df_datos_nuevos.count()\n",
    "        \n",
    "        # --- 5. Escribir en la Tabla Final de Evidencias ---\n",
    "        if total_rows > 0:\n",
    "            print(f\"A√±adiendo (APPEND) {total_rows} registros nuevos a la tabla final: {EVIDENCES_TABLE}...\")\n",
    "            \n",
    "            (df_datos_nuevos.write\n",
    "             .format(\"delta\")\n",
    "             .mode(\"append\")\n",
    "             .option(\"mergeSchema\", \"true\")\n",
    "             .saveAsTable(EVIDENCES_TABLE))\n",
    "            \n",
    "            print(f\"¬°Unificaci√≥n completada! Se a√±adieron {total_rows} filas a {EVIDENCES_TABLE}.\")\n",
    "        else:\n",
    "            print(\"No se encontraron registros nuevos para a√±adir a la tabla final.\")\n",
    "\n",
    "        # --- 6. Limpieza Segura de Staging ---\n",
    "        # Borramos SOLO los IDs que hemos procesado (o que ya estaban procesados)\n",
    "        # de las tablas de staging.\n",
    "        exec_ids_to_delete = exec_ids_in_staging.select(\"execution_id\").collect()\n",
    "        \n",
    "        if exec_ids_to_delete:\n",
    "            delete_condition = \"execution_id IN ({})\".format(\n",
    "                \",\".join([f\"'{row.execution_id}'\" for row in exec_ids_to_delete])\n",
    "            )\n",
    "            \n",
    "            print(f\"Limpiando {len(exec_ids_to_delete)} ejecuciones de las tablas de staging...\")\n",
    "            \n",
    "            for table_path in staging_table_paths:\n",
    "                if spark.catalog.tableExists(table_path):\n",
    "                    try:\n",
    "                        delta_staging_table = DeltaTable.forName(spark, table_path)\n",
    "                        delta_staging_table.delete(condition = delete_condition)\n",
    "                        print(f\"  > Limpieza exitosa de: {table_path}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"  > AVISO: Fallo al limpiar la tabla de staging {table_path}. Error: {e}\")\n",
    "                        # No lanzamos error, el pr√≥ximo run lo volver√° a intentar\n",
    "            \n",
    "            print(\"Limpieza de tablas de staging completada.\")\n",
    "\n",
    "        #df_unprocessed_ids.unpersist()\n",
    "        return total_rows\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fatal durante la unificaci√≥n de evidencias: {e}\")\n",
    "        if 'df_unprocessed_ids' in locals() and df_unprocessed_ids.is_cached:\n",
    "            df_unprocessed_ids.unpersist()\n",
    "        raise e\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 3, 3. Punto de Entrada de Ejecuci√≥n\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        rows_processed = main()\n",
    "        dbutils.notebook.exit(f\"√âxito: {rows_processed} evidencias nuevas unificadas y a√±adidas.\")\n",
    "    except Exception as e:\n",
    "        dbutils.notebook.exit(f\"Fallo en la unificaci√≥n de evidencias: {e}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "05_unify_evidences",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
