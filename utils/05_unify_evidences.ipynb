{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18b4721b-e5ee-4473-bb7a-b099c433b22a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Lenguaje del Notebook: Python\n",
    "# VERSIN 14: Unificador de Evidencias (Idempotente y Transaccional)\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC #  5. Unificar Evidencias (Staging -> Final) (v14)\n",
    "# MAGIC \n",
    "# MAGIC Este notebook se ejecuta DESPUS de todas las tareas paralelas de validaci贸n.\n",
    "# MAGIC Es idempotente y seguro ante fallos.\n",
    "# MAGIC \n",
    "# MAGIC **L贸gica (Transaccional):**\n",
    "# MAGIC 1.  `main()` orquesta el proceso.\n",
    "# MAGIC 2.  Lee `table_config` para encontrar todas las tablas `staging_evidences_table`.\n",
    "# MAGIC 3.  Las une din谩micamente (`unionByName`) para crear un DataFrame `df_staging_completo`.\n",
    "# MAGIC 4.  **Anti-Duplicaci贸n:** Obtiene los `execution_id`s de staging y los cruza con\n",
    "# MAGIC     `dq_evidences` para encontrar los IDs que *a煤n no* han sido procesados.\n",
    "# MAGIC 5.  Filtra `df_staging_completo` para quedarse solo con los datos nuevos.\n",
    "# MAGIC 6.  **`APPEND`** los datos nuevos a la tabla final `dq_evidences`.\n",
    "# MAGIC 7.  **Limpieza:** Si el `APPEND` es exitoso, `DELETE` los `execution_id`s procesados\n",
    "# MAGIC     de todas las tablas de staging.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1, 1. Imports y Constantes\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, broadcast\n",
    "from functools import reduce\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# --- Widgets ---\n",
    "dbutils.widgets.text(\"catalog_name\", \"workspace\", \"Cat谩logo de UC donde residen las tablas\")\n",
    "dbutils.widgets.text(\"schema_name\", \"framework_dq\", \"Esquema de UC donde residen las tablas\")\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 2, 2. Carga de librer铆as y definici贸n de constantes/mapas\n",
    "\n",
    "CATALOG = dbutils.widgets.get(\"catalog_name\")\n",
    "SCHEMA = dbutils.widgets.get(\"schema_name\")\n",
    "\n",
    "TABLE_CONFIG = f\"{CATALOG}.{SCHEMA}.dq_tables_config\"\n",
    "EVIDENCES_TABLE = f\"{CATALOG}.{SCHEMA}.dq_evidences\"\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 2, 2. Funci贸n Principal (main)\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Flujo principal de unificaci贸n de evidencias (v14).\n",
    "    \"\"\"\n",
    "    print(\"--- Iniciando Script de Unificaci贸n de Evidencias (v14) ---\")\n",
    "    \n",
    "    try:\n",
    "        # --- 1. Leer la Configuraci贸n Maestra ---\n",
    "        print(f\"Leyendo la configuraci贸n de maestros desde: {TABLE_CONFIG}\")\n",
    "        \n",
    "        master_configs = (spark.table(TABLE_CONFIG)\n",
    "                          .select(\"table_name\", \"staging_evidences_table\")\n",
    "                          .collect())\n",
    "        \n",
    "        if not master_configs:\n",
    "            raise Exception(f\"No se encontr贸 configuraci贸n de maestros en {TABLE_CONFIG}\")\n",
    "\n",
    "        # --- 2. Recolectar todos los DataFrames de Staging ---\n",
    "        dfs_to_union = []\n",
    "        staging_table_paths = [] # Guardar las rutas para la limpieza final\n",
    "        \n",
    "        print(f\"Recolectando datos de {len(master_configs)} tablas de staging...\")\n",
    "\n",
    "        # Obtenemos los execution_id que YA est谩n en la tabla final\n",
    "        df_processed_ids = spark.table(EVIDENCES_TABLE).select(\"execution_id\").distinct()\n",
    "\n",
    "        for config in master_configs:\n",
    "            staging_table = config.staging_evidences_table\n",
    "            staging_table_path = f\"{CATALOG}.{SCHEMA}.{staging_table}\"\n",
    "            staging_table_paths.append(staging_table_path) \n",
    "            \n",
    "            if spark.catalog.tableExists(staging_table_path):\n",
    "                print(f\"  > Leyendo datos de: {staging_table_path}\")\n",
    "                df_staging = spark.table(staging_table_path)\n",
    "                \n",
    "                if df_staging.head(1):\n",
    "                    df_filtered = df_staging.join(\n",
    "                        broadcast(df_processed_ids),\n",
    "                        \"execution_id\",\n",
    "                        \"left_anti\"\n",
    "                    )\n",
    "                    if df_filtered.head(1):\n",
    "                        dfs_to_union.append(df_filtered)\n",
    "                    else:\n",
    "                        print(f\"  > Todas las filas de {staging_table_path} ya procesadas. Saltando.\")\n",
    "                else:\n",
    "                    print(f\"  > AVISO: La tabla {staging_table_path} existe pero est谩 vac铆a. Saltando.\")\n",
    "            else:\n",
    "                print(f\"  > AVISO: No se encontr贸 la tabla {staging_table_path}. Saltando.\")\n",
    "\n",
    "        # --- 3. Unificar DataFrames ---\n",
    "        df_staging_completo: DataFrame = dfs_to_union[0]\n",
    "        for df in dfs_to_union[1:]:\n",
    "            df_staging_completo = df_staging_completo.unionByName(df, allowMissingColumns=True)\n",
    "        # Eliminamos duplicados\n",
    "        df_staging_completo = df_staging_completo.distinct()#.cache()\n",
    "\n",
    "\n",
    "        # --- 4. Execution_id realmente nuevos ---\n",
    "        # Obtenemos todos los execution_id que han llegado a staging\n",
    "        exec_ids_in_staging = df_staging_completo.select(\"execution_id\").distinct()\n",
    "        # Filtramos 'exec_ids_in_staging' para quedarnos solo con los que\n",
    "        # NO est谩n en 'df_processed_ids' (LEFT_ANTI JOIN)\n",
    "        df_unprocessed_ids = exec_ids_in_staging.join(\n",
    "            broadcast(df_processed_ids),\n",
    "            \"execution_id\",\n",
    "            \"left_anti\"\n",
    "        )#.cache()\n",
    "\n",
    "        unprocessed_count = df_unprocessed_ids.count()\n",
    "        if unprocessed_count == 0:\n",
    "            print(\"No hay ejecuciones nuevas despu茅s de filtrar ya procesadas.\")\n",
    "            return 0\n",
    "\n",
    "        # Filtrar solo datos nuevos\n",
    "        df_datos_nuevos = df_staging_completo.join(\n",
    "            broadcast(df_unprocessed_ids),\n",
    "            \"execution_id\",\n",
    "            \"inner\"\n",
    "        )\n",
    "        total_rows = df_datos_nuevos.count()\n",
    "\n",
    "        # --- 5. Append a tabla final ---\n",
    "        if total_rows > 0:\n",
    "            print(f\"A帽adiendo {total_rows} filas nuevas a {EVIDENCES_TABLE}\")\n",
    "            df_datos_nuevos.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").saveAsTable(EVIDENCES_TABLE)\n",
    "            print(f\"隆Unificaci贸n completada!\")\n",
    "        else:\n",
    "            print(\"No se encontraron registros nuevos para a帽adir a la tabla final.\")\n",
    "\n",
    "\n",
    "        # --- 6. Limpieza segura de staging ---\n",
    "        for table_path in staging_table_paths:\n",
    "            if spark.catalog.tableExists(table_path):\n",
    "                try:                  \n",
    "                    spark.sql(f\"DROP TABLE {table_path}\")\n",
    "                    print(f\"  > Tabla eliminada correctamente: {table_path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  > AVISO: Fallo al limpiar {table_path}: {e}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error fatal durante la unificaci贸n de evidencias: {e}\")\n",
    "        if 'df_unprocessed_ids' in locals() and df_unprocessed_ids.is_cached:\n",
    "            df_unprocessed_ids.unpersist()\n",
    "        raise e\n",
    "    \n",
    "    finally:\n",
    "        #df_unprocessed_ids.unpersist()\n",
    "        print(\"comentado\")\n",
    "    \n",
    "    return total_rows\n",
    "\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 3, 3. Punto de Entrada de Ejecuci贸n\n",
    "if __name__ == \"__main__\":\n",
    "    rows_processed = 0\n",
    "    try:\n",
    "        # Ejecutamos la funci贸n principal\n",
    "        rows_processed = main()\n",
    "    except Exception as e:\n",
    "        # Salida expl铆cita en caso de fallo\n",
    "        dbutils.notebook.exit(f\"Fallo en la unificaci贸n de evidencias: {e}\")\n",
    "    else:\n",
    "        # Salida expl铆cita en caso de 茅xito\n",
    "        dbutils.notebook.exit(f\"xito: {rows_processed} evidencias nuevas unificadas y a帽adidas.\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "05_unify_evidences",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
