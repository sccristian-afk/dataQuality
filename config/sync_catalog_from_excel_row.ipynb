{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d82b6a6f-edd6-4c17-b8f2-1cd68706ea62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# DBTITLE 0,0. Librerías necesarias\n",
    "import pandas as pd\n",
    "import json\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import col, lit, current_timestamp, udf, split\n",
    "from pyspark.sql.types import StringType, BooleanType, ArrayType\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# DBTITLE 1,1. Widgets y constantes\n",
    "dbutils.widgets.text(\"catalog_name\", \"workspace\", \"Catálogo de UC donde residen las tablas\")\n",
    "dbutils.widgets.text(\"schema_name\", \"dq_framework\", \"Esquema de UC donde residen las tablas\")\n",
    "dbutils.widgets.text(\"tables_sheet\", \"Tablas\", \"Hoja Excel: Tablas\")\n",
    "dbutils.widgets.text(\"rules_sheet\", \"Reglas\", \"Hoja Excel: Reglas\")\n",
    "dbutils.widgets.text(\"validations_sheet\", \"Validaciones\", \"Hoja Excel: Validaciones\")\n",
    "dbutils.widgets.text(\"excel_file_path\", \"/Volumes/workspace/dq_framework/template/configValidaciones.xlsx\", \"Ruta del Excel\")\n",
    "\n",
    "CATALOG = dbutils.widgets.get(\"catalog_name\")\n",
    "SCHEMA = dbutils.widgets.get(\"schema_name\")\n",
    "CONFIG_SHEET = dbutils.widgets.get(\"tables_sheet\")\n",
    "LIBRARY_SHEET = dbutils.widgets.get(\"rules_sheet\")\n",
    "VALIDATIONS_SHEET = dbutils.widgets.get(\"validations_sheet\")\n",
    "EXCEL_PATH = dbutils.widgets.get(\"excel_file_path\")\n",
    "\n",
    "TABLE_CONFIG = f\"{CATALOG}.{SCHEMA}.dq_tables_config\"\n",
    "RULE_LIB_TABLE = f\"{CATALOG}.{SCHEMA}.dq_rules_library\"\n",
    "CATALOG_TABLE = f\"{CATALOG}.{SCHEMA}.dq_validations_catalog\"\n",
    "\n",
    "# Columnas esperadas por hoja\n",
    "CONFIG_EXPECTED_COLS = ['Id_tabla', 'Nombre_tabla', 'Nombre_técnico', 'Clave_primaria', 'Tabla_staging_evidencias']\n",
    "LIBRARY_EXPECTED_COLS = ['Id_regla', 'Nombre_técnico', 'Nombre_funcional', 'Descripción', \n",
    "                         'Dimensión_reglas', 'Tipo_implementación', 'Etiquetas', 'Clase', \n",
    "                         'Propietario', 'Fecha_actualizacion', 'Actualizado_por']\n",
    "VALIDATIONS_EXPECTED_COLS = ['Id_validación','Id_regla','Id_tabla','Nombre_técnico','Dominio_tabla',\n",
    "                             'Definición_perimetro','Param_columnas','Param_valor','Param_valor_min',\n",
    "                             'Param_valor_max','Param_conjunto','Param_tipo_dato','Param_expresion',\n",
    "                             'Param_query_SQL','Param_merge_columnas','Severidad','Owner','Etiquetas',\n",
    "                             'Actualizado_por','Fecha_actualización','Validación_activa']\n",
    "\n",
    "# DBTITLE 2,2. Leer Excel y validar columnas\n",
    "def load_excel_sheet(excel_path, sheet_name, expected_cols):\n",
    "    errors_list = []\n",
    "    try:\n",
    "        df_pd = pd.read_excel(excel_path, sheet_name=sheet_name, dtype=str).fillna('')\n",
    "        missing_cols = [c for c in expected_cols if c not in df_pd.columns]\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"Faltan columnas obligatorias: {missing_cols}\")\n",
    "        df_pd = df_pd.dropna(how='all')\n",
    "        if df_pd.empty:\n",
    "            print(f\"Hoja '{sheet_name}' vacía. No se sincroniza.\")\n",
    "            return None, errors_list\n",
    "        return spark.createDataFrame(df_pd), errors_list\n",
    "    except Exception as e:\n",
    "        errors_list.append({\"sheet\": sheet_name, \"fila\": None, \"error\": str(e)})\n",
    "        return None, errors_list\n",
    "\n",
    "# DBTITLE 3,3. Construir JSON de validaciones\n",
    "def build_json_definition(technical_name, rule_type, param_columnas_str, param_valor, param_min, param_max, param_conjunto_str,\n",
    "                          param_sql, param_tipo, param_query, param_merge):\n",
    "    params_dict = {}\n",
    "    try:\n",
    "        if rule_type == 'BUILT-IN':\n",
    "            if technical_name in [\"is_not_null\",\"is_not_empty\",\"is_not_null_and_not_empty\",\"is_not_in_future\"]:\n",
    "                cols = [c.strip() for c in param_columnas_str.split(',') if c]\n",
    "                if cols: params_dict[\"column\"] = cols[0]\n",
    "            elif technical_name == \"is_unique\":\n",
    "                cols = [c.strip() for c in param_columnas_str.split(',') if c]\n",
    "                if cols: params_dict[\"columns\"] = cols\n",
    "            elif technical_name in [\"is_in_list\",\"is_not_null_and_is_in_list\"]:\n",
    "                col_name = param_columnas_str.strip() if param_columnas_str else None\n",
    "                if col_name: params_dict[\"column\"] = col_name\n",
    "                allowed_values = [v.strip() for v in param_conjunto_str.split(',') if v]\n",
    "                if allowed_values: params_dict[\"allowed\"] = allowed_values\n",
    "            elif technical_name == \"regex_match\":\n",
    "                col_name = param_columnas_str.strip() if param_columnas_str else None\n",
    "                if col_name: params_dict[\"column\"] = col_name\n",
    "                if param_valor: params_dict[\"regex\"] = param_valor\n",
    "            elif technical_name in [\"is_equal_to\",\"is_not_equal_to\"]:\n",
    "                col_name = param_columnas_str.strip() if param_columnas_str else None\n",
    "                if col_name: params_dict[\"column\"] = col_name\n",
    "                if param_valor: params_dict[\"value\"] = param_valor\n",
    "            elif technical_name == \"is_valid_date\":\n",
    "                col_name = param_columnas_str.strip() if param_columnas_str else None\n",
    "                if col_name: params_dict[\"column\"] = col_name\n",
    "                if param_valor: params_dict[\"date_format\"] = param_valor\n",
    "            elif technical_name == \"is_valid_timestamp\":\n",
    "                col_name = param_columnas_str.strip() if param_columnas_str else None\n",
    "                if col_name: params_dict[\"column\"] = col_name\n",
    "                if param_valor: params_dict[\"timestamp_format\"] = param_valor\n",
    "            elif technical_name in [\"is_in_range\",\"is_not_in_range\"]:\n",
    "                col_name = param_columnas_str.strip() if param_columnas_str else None\n",
    "                if col_name: params_dict[\"column\"] = col_name\n",
    "                if param_min: params_dict[\"min_limit\"] = float(param_min)\n",
    "                if param_max: params_dict[\"max_limit\"] = float(param_max)\n",
    "            elif technical_name in [\"is_not_less_than\"]:\n",
    "                col_name = param_columnas_str.strip() if param_columnas_str else None\n",
    "                if col_name: params_dict[\"column\"] = col_name\n",
    "                if param_min: params_dict[\"limit\"] = float(param_min)\n",
    "            elif technical_name in [\"is_not_greater_than\"]:\n",
    "                col_name = param_columnas_str.strip() if param_columnas_str else None\n",
    "                if col_name: params_dict[\"column\"] = col_name\n",
    "                if param_max: params_dict[\"limit\"] = float(param_max)\n",
    "            elif technical_name == \"sql_expression\" and param_sql:\n",
    "                params_dict[\"expression\"] = param_sql\n",
    "            elif technical_name == \"sql_query\" and param_query:\n",
    "                params_dict[\"query\"] = param_query\n",
    "                merge_cols = [c.strip() for c in param_merge.split(',') if c]\n",
    "                if merge_cols: params_dict[\"merge_columns\"] = merge_cols\n",
    "        elif rule_type in ['CUSTOM']:\n",
    "            cols = [c.strip() for c in param_columnas_str.split(',') if c]\n",
    "            if cols: params_dict[\"columns\"] = cols\n",
    "        return json.dumps(params_dict)\n",
    "    except Exception as e:\n",
    "        return json.dumps({\"error\": str(e)})\n",
    "\n",
    "build_json_udf = udf(build_json_definition, StringType())\n",
    "\n",
    "# DBTITLE 4,4. Función genérica para sincronizar Delta por hoja\n",
    "def sync_delta(df_spark, target_table_name, merge_keys, column_mapping, sheet_name):\n",
    "    if df_spark is None:\n",
    "        return 0, []\n",
    "    errors_list = []\n",
    "    try:\n",
    "        delta_table = DeltaTable.forName(spark, target_table_name)\n",
    "        delta_table.alias(\"target\").merge(\n",
    "            df_spark.alias(\"source\"),\n",
    "            \" AND \".join([f\"target.{k} = source.{k}\" for k in merge_keys])\n",
    "        ).whenMatchedUpdate(set={k:v for k,v in column_mapping.items()}) \\\n",
    "         .whenNotMatchedInsert(values={k:v for k,v in column_mapping.items()}) \\\n",
    "         .execute()\n",
    "        return df_spark.count(), errors_list\n",
    "    except Exception as e:\n",
    "        errors_list.append({\"sheet\": sheet_name, \"fila\": None, \"error\": str(e)})\n",
    "        return 0, errors_list\n",
    "\n",
    "# DBTITLE 5,5. Sincronización específica por hoja\n",
    "def sync_tables(df_excel):\n",
    "    df_valid = df_excel.select(\n",
    "        col(\"Id_tabla\").alias(\"table_id\"),\n",
    "        col(\"Nombre_tabla\").alias(\"table_name\"),\n",
    "        col(\"Nombre_técnico\").alias(\"table_name_tech\"),\n",
    "        col(\"Clave_primaria\").alias(\"primary_key\"),\n",
    "        col(\"Tabla_staging_evidencias\").alias(\"staging_evidences_table\")\n",
    "    ).filter(col(\"Id_tabla\") != \"\")\n",
    "    mapping = {c:c for c in df_valid.columns}\n",
    "    return sync_delta(df_valid, TABLE_CONFIG, [\"table_id\"], mapping, \"Tablas\")\n",
    "\n",
    "def sync_rules(df_excel):\n",
    "    df_valid = df_excel.select(\n",
    "        col(\"Id_regla\").alias(\"rule_id\"),\n",
    "        col(\"Nombre_técnico\").alias(\"technical_rule_name\"),\n",
    "        col(\"Nombre_funcional\").alias(\"functional_name\"),\n",
    "        col(\"Descripción\").alias(\"description\"),\n",
    "        col(\"Dimensión_reglas\").alias(\"dimension_dq\"),\n",
    "        col(\"Tipo_implementación\").alias(\"implementation_type\"),\n",
    "        current_timestamp().alias(\"updated_at\"),\n",
    "        col(\"Actualizado_por\").alias(\"updated_by\"),\n",
    "        split(col(\"Etiquetas\"), \",\\s*\").alias(\"tags\"),\n",
    "        col(\"Propietario\").alias(\"owner\")\n",
    "    ).filter(col(\"Id_regla\") != \"\")\n",
    "    mapping = {c:c for c in df_valid.columns}\n",
    "    return sync_delta(df_valid, RULE_LIB_TABLE, [\"rule_id\"], mapping, \"Reglas\")\n",
    "\n",
    "def sync_validations(df_excel):\n",
    "    df_valid = df_excel.select(\n",
    "        col(\"Id_validación\").alias(\"validation_id\"),\n",
    "        col(\"Id_regla\").alias(\"rule_id\"),\n",
    "        col(\"Id_tabla\").alias(\"table_id\"),\n",
    "        col(\"Definición_perimetro\").alias(\"perimeter_definition\"),\n",
    "        col(\"Validación_activa\").cast(BooleanType()).alias(\"is_active\"),\n",
    "        col(\"Severidad\").alias(\"severity\"),\n",
    "        build_json_udf(\n",
    "            col(\"Nombre_técnico\"), col(\"Tipo_regla\"),\n",
    "            col(\"Param_columnas\"), col(\"Param_valor\"),\n",
    "            col(\"Param_valor_min\"), col(\"Param_valor_max\"),\n",
    "            col(\"Param_conjunto\"), col(\"Param_expresion\"),\n",
    "            col(\"Param_tipo_dato\"), col(\"Param_query_SQL\"),\n",
    "            col(\"Param_merge_columnas\")\n",
    "        ).alias(\"validation_definition\"),\n",
    "        col(\"Dominio_tabla\").alias(\"domain\"),\n",
    "        split(col(\"Param_columnas\"), \",\\s*\").alias(\"Param_columns\"),\n",
    "        col(\"Param_valor\").alias(\"Param_value\"),\n",
    "        col(\"Param_valor_min\").alias(\"Param_value_min\"),\n",
    "        col(\"Param_valor_max\").alias(\"Param_value_max\"),\n",
    "        col(\"Param_conjunto\").alias(\"Param_range\"),\n",
    "        col(\"Param_expresion\").alias(\"Param_expression\"),\n",
    "        col(\"Param_tipo_dato\").alias(\"Param_data_type\"),\n",
    "        col(\"Param_query_SQL\").alias(\"Param_query_SQL\"),\n",
    "        col(\"Param_merge_columnas\").alias(\"Param_merge_columns\"),\n",
    "        col(\"Owner\").alias(\"owner\"),\n",
    "        current_timestamp().alias(\"updated_at\"),\n",
    "        col(\"Actualizado_por\").alias(\"updated_by\"),\n",
    "        split(col(\"Etiquetas\"), \",\\s*\").alias(\"tags\")\n",
    "    ).filter(col(\"Id_validación\") != \"\")\n",
    "    mapping = {c:c for c in df_valid.columns}\n",
    "    return sync_delta(df_valid, CATALOG_TABLE, [\"validation_id\"], mapping, \"Validaciones\")\n",
    "\n",
    "# DBTITLE 6,6. Main Orquestador\n",
    "def main():\n",
    "    total_errors = []\n",
    "    try:\n",
    "        # 1. Tablas\n",
    "        df_tablas, errors_tablas = load_excel_sheet(EXCEL_PATH, CONFIG_SHEET, CONFIG_EXPECTED_COLS)\n",
    "        total_errors.extend(errors_tablas)\n",
    "        synced_tablas, _ = sync_tables(df_tablas)\n",
    "        \n",
    "        # 2. Reglas\n",
    "        df_rules, errors_rules = load_excel_sheet(EXCEL_PATH, LIBRARY_SHEET, LIBRARY_EXPECTED_COLS)\n",
    "        total_errors.extend(errors_rules)\n",
    "        synced_rules, _ = sync_rules(df_rules)\n",
    "        \n",
    "        # 3. Validaciones\n",
    "        df_valids, errors_valids = load_excel_sheet(EXCEL_PATH, VALIDATIONS_SHEET, VALIDATIONS_EXPECTED_COLS)\n",
    "        total_errors.extend(errors_valids)\n",
    "        synced_valids, _ = sync_validations(df_valids)\n",
    "        \n",
    "        print(f\"Sincronización completa: {synced_tablas} tablas, {synced_rules} reglas, {synced_valids} validaciones.\")\n",
    "        if total_errors:\n",
    "            print(f\"Se detectaron {len(total_errors)} errores durante la sincronización:\")\n",
    "            for e in total_errors: print(e)\n",
    "        return f\"Éxito: Tablas {synced_tablas}, Reglas {synced_rules}, Validaciones {synced_valids}, Errores {len(total_errors)}\"\n",
    "    except Exception as e:\n",
    "        print(f\"Fallo crítico: {e}\")\n",
    "        raise\n",
    "\n",
    "# DBTITLE 7,7. Punto de entrada\n",
    "if __name__ == \"__main__\":\n",
    "    result_msg = main()\n",
    "    dbutils.notebook.exit(result_msg)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "sync_catalog_from_excel_row",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
