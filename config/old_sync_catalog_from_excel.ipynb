{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba6dba24-b6c9-487d-803d-b325e30692f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install pandas openpyxl "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7826e6e-d311-4f1e-84a0-b15a2aca6468",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d82b6a6f-edd6-4c17-b8f2-1cd68706ea62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MAGIC %md\n",
    "# MAGIC #  Sincronizador de Reglas (Excel -> Cat谩logo de Validaciones, metadatos de tablas y cat谩logo de reglas)\n",
    "# MAGIC \n",
    "# MAGIC Este notebook es la herramienta para poblar 'dq_tables_config', 'dq_rules_library' y 'dq_validations_catalog'\n",
    "# MAGIC leyendo la plantilla de Excel validationsConfig\n",
    "# MAGIC \n",
    "# MAGIC **Funciones:**\n",
    "# MAGIC 1.  'load_and_validate_excel': lee y valida el fichero Excel\n",
    "# MAGIC 2.  'build_json_definition': construye el JSON 'rule_definition' a partir de las columnas de par谩metros\n",
    "# MAGIC 2.  'sync_table_config': sincroniza los cambios en 'dq_tables_config' con la funci贸n 'MERGE'\n",
    "# MAGIC 2.  'sync_rule_library': sincroniza los cambios en 'dq_rules_library' con la funci贸n 'MERGE'\n",
    "# MAGIC 2.  'sync_validations_catalog': sincroniza los cambios en 'dq_validations_catalog' con la funci贸n 'MERGE'\n",
    "# MAGIC 2.  'main': orquesta las llamadas a las dem谩s funciones\n",
    "\n",
    "# COMMAND ----------\n",
    "# Libreria para poder leer el .xlsx por medio de Pandas. \n",
    "# !!!!!!!!!!!!!!!comentario interno (borrar cuando se solucione): O se instala en el cluster o se a帽ade a otra linea de c贸digo (si falla de esta forma)!!!!!!!!!!!!!!!\n",
    "# MAGIC %pip install pandas openpyxl\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1, 1. Imports, constantes y widgets\n",
    "import pandas as pd\n",
    "import json\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import col, lit, current_timestamp, udf, struct, split\n",
    "from pyspark.sql.types import StringType, MapType, StructType, StructField, BooleanType, ArrayType\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# --- Constantes de tablas ---\n",
    "dbutils.widgets.text(\"catalog_name\", \"workspace\", \"Cat谩logo de UC donde residen las tablas\")\n",
    "dbutils.widgets.text(\"schema_name\", \"dq_framework\", \"Esquema de UC donde residen las tablas\")\n",
    "dbutils.widgets.text(\"tables_sheet\", \"Tablas\", \"Cat谩logo de UC donde residen las tablas\")\n",
    "dbutils.widgets.text(\"rules_sheet\", \"Reglas\", \"Esquema de UC donde residen las tablas\")\n",
    "dbutils.widgets.text(\"validations_sheet\", \"Validaciones\", \"Cat谩logo de UC donde residen las tablas\")\n",
    "dbutils.widgets.text(\"excel_file_path\", \"/Volumes/workspace/dq_framework/template/configValidaciones.xlsx\", \"Ruta al fichero Excel con la informaci贸n\")\n",
    "\n",
    "CATALOG = dbutils.widgets.get(\"catalog_name\")\n",
    "SCHEMA = dbutils.widgets.get(\"schema_name\")\n",
    "\n",
    "TABLE_CONFIG = f\"{CATALOG}.{SCHEMA}.dq_tables_config\"\n",
    "RULE_LIB_TABLE = f\"{CATALOG}.{SCHEMA}.dq_rules_library\"\n",
    "CATALOG_TABLE = f\"{CATALOG}.{SCHEMA}.dq_validations_catalog\"\n",
    "\n",
    "CONFIG_SHEET = dbutils.widgets.get(\"tables_sheet\")\n",
    "LIBRARY_SHEET = dbutils.widgets.get(\"rules_sheet\")\n",
    "VALIDATIONS_SHEET = dbutils.widgets.get(\"validations_sheet\")\n",
    "EXCEL_PATH = dbutils.widgets.get(\"excel_file_path\")\n",
    "\n",
    "# --- Columnas del Excel ---\n",
    "\n",
    "CONFIG_EXPECTED_COLS = [\n",
    "    'Id_tabla', 'Nombre_tabla', 'Nombre_t茅cnico', \n",
    "    'Clave_primaria', 'Tabla_staging_evidencias'\n",
    "]\n",
    "\n",
    "LIBRARY_EXPECTED_COLS = [\n",
    "    'Id_regla', 'Nombre_t茅cnico', 'Nombre_funcional','Descripci贸n', 'Nivel_regla',\n",
    "    'Dimensi贸n_reglas', 'Tipo_implementaci贸n', 'Etiquetas', 'Clase', \n",
    "    'Propietario', 'Fecha_actualizacion', 'Actualizado_por'\n",
    "]\n",
    "\n",
    "VALIDATIONS_EXPECTED_COLS = [\n",
    "    'Id_validaci贸n','Id_regla', 'Id_tabla', 'Nombre_t茅cnico', 'Dominio_tabla', 'Definici贸n_perimetro',\n",
    "    'Param_columnas', 'Param_valor','Param_valor_min','Param_valor_max', 'Param_conjunto',\n",
    "    'Param_tipo_dato','Param_expresion', 'Param_query_SQL', 'Param_merge_columnas', \n",
    "    'Severidad','Owner','Etiquetas', 'Actualizado_por', 'Fecha_actualizaci贸n', 'Validaci贸n_activa'\n",
    "]\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 2, 2. Leer y validar Excel\n",
    "def load_and_validate_excel(excel_path, sheet_name, expected_cols):\n",
    "    \"\"\"\n",
    "    Lee el fichero Excel usando Pandas y lo convierte a un DataFrame de Spark\n",
    "    Valida que las columnas esperadas existan.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Uso de Pandas para leer, forzando todo a string para evitar errores de tipo\n",
    "        pandas_df = pd.read_excel(excel_path, sheet_name=sheet_name, dtype=str).fillna('')\n",
    "\n",
    "        # Validar que las columnas necesarias existen      \n",
    "        missing_cols = [col for col in expected_cols if col not in pandas_df.columns]\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"Faltan columnas obligatorias en el Excel: {missing_cols}\")\n",
    "        \n",
    "        pandas_df = pandas_df.dropna(how='all')\n",
    "        if pandas_df.empty:\n",
    "            print(f\"Hoja '{sheet_name}' est谩 vac铆a. Saltando sincronizaci贸n.\")\n",
    "            return None\n",
    "        \n",
    "        # Convertir el DataFrame de Pandas a un DataFrame de Spark\n",
    "        return spark.createDataFrame(pandas_df)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error Cr铆tico: No se encontr贸 el fichero Excel en {excel_path}\")\n",
    "        raise\n",
    "    except ValueError as ve:\n",
    "        print(f\"Error de Validaci贸n: {ve}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        errors_list.append({\"sheet\": sheet_name, \"fila\": None, \"error\": str(e)})\n",
    "        return None, errors_list\n",
    "    \n",
    "# COMMAND ----------\n",
    "            \n",
    "# COMMAND ----------\n",
    "# DBTITLE 3, 3. Funci贸n Python para construir el JSON (la l贸gica de la regla)\n",
    "def build_json_definition(technical_name, rule_type, param_columnas_str, param_valor, param_min, param_max, param_conjunto_str, param_sql, param_tipo, param_query, param_merge):\n",
    "    \"\"\"\n",
    "    Funci贸n: con las columnas de par谩metros del Excel\n",
    "    construye el string JSON 'rule_definition' que espera el orquestador.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- Diccionario base ---\n",
    "    params_dict = {}\n",
    "    \n",
    "    try:\n",
    "        # --- L贸gica para reglas BUILT-IN ---\n",
    "        if rule_type == 'BUILT-IN':\n",
    "            \n",
    "            # Reglas que solo usan 'Param_Columnas'\n",
    "            if technical_name in [\"is_not_null\", \"is_not_empty\", \"is_not_null_and_not_empty\", \"is_not_in_future\"]:\n",
    "                columns = [col.strip() for col in param_columnas_str.split(',') if col and col.strip()]\n",
    "                if columns: params_dict[\"column\"] = columns[0]\n",
    "            \n",
    "            elif technical_name == \"is_unique\":\n",
    "                columns = [col.strip() for col in param_columnas_str.split(',') if col and col.strip()]\n",
    "                if columns: params_dict[\"columns\"] = columns # Espera 'columns' (plural)\n",
    "\n",
    "            # Regla 'is_in_list' y 'is_not_null_and_is_in_list'\n",
    "            elif technical_name in [\"is_in_list\", \"is_not_null_and_is_in_list\"]:\n",
    "                col_name = param_columnas_str.strip() if param_columnas_str else None\n",
    "                if col_name: params_dict[\"column\"] = col_name\n",
    "                allowed_values = [val.strip() for val in param_conjunto_str.split(',') if val and val.strip()]\n",
    "                if allowed_values: params_dict[\"allowed\"] = allowed_values\n",
    "            \n",
    "            # Regla 'regex_match'\n",
    "            elif technical_name == \"regex_match\":\n",
    "                col_name = param_columnas_str.strip() if param_columnas_str else None\n",
    "                if col_name: params_dict[\"column\"] = col_name\n",
    "                if param_valor: params_dict[\"regex\"] = param_valor\n",
    "             \n",
    "            # Regla 'is_equal_to' y 'is_not_equal_to'\n",
    "            elif technical_name in [\"is_equal_to\", \"is_not_equal_to\"]:\n",
    "                col_name = param_columnas_str.strip() if param_columnas_str else None\n",
    "                if col_name: params_dict[\"column\"] = col_name\n",
    "                if param_valor: params_dict[\"value\"] = param_valor\n",
    "\n",
    "            # Regla 'is_valid_date'\n",
    "            elif technical_name == \"is_valid_date\":\n",
    "                col_name = param_columnas_str.strip() if param_columnas_str else None\n",
    "                if col_name: params_dict[\"column\"] = col_name\n",
    "                if param_valor: params_dict[\"date_format\"] = param_valor\n",
    "\n",
    "            # Regla 'is_valid_timestamp'\n",
    "            elif technical_name == \"is_valid_timestamp\":\n",
    "                col_name = param_columnas_str.strip() if param_columnas_str else None\n",
    "                if col_name: params_dict[\"column\"] = col_name\n",
    "                if param_valor: params_dict[\"timestamp_format\"] = param_valor\n",
    "                \n",
    "            # Regla 'is_in_range' y 'is_not_in_range'\n",
    "            elif technical_name in [\"is_in_range\", \"is_not_in_range\"]:\n",
    "                col_name = param_columnas_str.strip() if param_columnas_str else None\n",
    "                if col_name: params_dict[\"column\"] = col_name\n",
    "                try: \n",
    "                    if param_min: params_dict[\"min_limit\"] = float(param_min)\n",
    "                except (ValueError, TypeError): params_dict[\"min_limit\"] = param_min\n",
    "                try: \n",
    "                    if param_max: params_dict[\"max_limit\"] = float(param_max)\n",
    "                except (ValueError, TypeError): params_dict[\"max_limit\"] = param_max\n",
    "            \n",
    "            # Regla 'is_not_less_than'\n",
    "            elif technical_name == \"is_not_less_than\":\n",
    "                col_name = param_columnas_str.strip() if param_columnas_str else None\n",
    "                if col_name: params_dict[\"column\"] = col_name\n",
    "                try: \n",
    "                    if param_min: params_dict[\"limit\"] = float(param_min)\n",
    "                except (ValueError, TypeError): params_dict[\"limit\"] = param_min\n",
    "            \n",
    "            # Regla 'is_not_greater_than'\n",
    "            elif technical_name == \"is_not_greater_than\":\n",
    "                col_name = param_columnas_str.strip() if param_columnas_str else None\n",
    "                if col_name: params_dict[\"column\"] = col_name\n",
    "                try: \n",
    "                    if param_max: params_dict[\"limit\"] = float(param_max)\n",
    "                except (ValueError, TypeError): params_dict[\"limit\"] = param_max\n",
    "\n",
    "            # Regla 'sql_expression'\n",
    "            elif technical_name == \"sql_expression\":\n",
    "                if param_sql: params_dict[\"expression\"] = param_sql\n",
    "            \n",
    "             # Regla 'sql_query'\n",
    "            elif technical_name == \"sql_query\":\n",
    "                if param_query: params_dict[\"query\"] = param_query\n",
    "                merge_cols = [col.strip() for col in param_merge.split(',') if col and col.strip()]\n",
    "                if merge_cols: params_dict[\"merge_columns\"] = merge_cols\n",
    "\n",
    "        # --- L贸gica para Reglas CUSTOM ---\n",
    "        elif rule_type in ['CUSTOM']:\n",
    "             columns = [col.strip() for col in param_columnas_str.split(',') if col and col.strip()]\n",
    "             if columns: params_dict[\"columns\"] = columns\n",
    "\n",
    "        # Convertir el diccionario de par谩metros final a un string JSON\n",
    "        return json.dumps(params_dict)\n",
    "\n",
    "    except Exception as e:\n",
    "        return json.dumps({\"error\": f\"Error al construir JSON de params: {e}\"})\n",
    "    \n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 5, 5. Sincronizar dq_rule_library\n",
    "def sync_rule_library(df_excel, target_table_name):\n",
    "    \"\"\"\n",
    "    Toma el DataFrame de la hoja 'Reglas' y lo sincroniza\n",
    "    con la tabla Delta 'dq_rules_library'.\n",
    "    \"\"\"\n",
    "    if df_excel is None:\n",
    "        return 0, 0\n",
    "        \n",
    "    try:\n",
    "        df_source = df_excel.select(\n",
    "            col(\"Id_regla\").alias(\"rule_id\"),\n",
    "            col(\"Nombre_t茅cnico\").alias(\"technical_rule_name\"),\n",
    "            col(\"Nombre_funcional\").alias(\"functional_name\"),\n",
    "            col(\"Descripci贸n\").alias(\"description\"),\n",
    "            col(\"Nivel_regla\").alias(\"rule_level\"),\n",
    "            col(\"Dimensi贸n_reglas\").alias(\"dimension_dq\"),\n",
    "            col(\"Tipo_implementaci贸n\").alias(\"implementation_type\"),\n",
    "            current_timestamp().alias(\"updated_at\"),\n",
    "            col(\"Actualizado_por\").alias(\"updated_by\"),\n",
    "            split(col(\"Etiquetas\"), \",\\s*\").alias(\"tags\"),\n",
    "            col(\"Propietario\").alias(\"owner\"),\n",
    "        ).filter(col(\"rule_id\") != \"\")\n",
    "\n",
    "        delta_table = DeltaTable.forName(spark, target_table_name)\n",
    "\n",
    "        (delta_table.alias(\"target\")\n",
    "         .merge(\n",
    "             df_source.alias(\"source\"),\n",
    "             condition = \"target.rule_id = source.rule_id\"\n",
    "         )\n",
    "         .whenMatchedUpdate(set = {\n",
    "             \"technical_rule_name\": \"source.technical_rule_name\",\n",
    "             \"functional_name\": \"source.functional_name\",\n",
    "             \"description\": \"source.description\",\n",
    "             \"rule_level\": \"source.rule_level\",\n",
    "             \"dimension_dq\": \"source.dimension_dq\",\n",
    "             \"implementation_type\": \"source.implementation_type\",\n",
    "             \"updated_at\": \"source.updated_at\",\n",
    "             \"updated_by\": \"source.updated_by\",\n",
    "             \"tags\": \"source.tags\",\n",
    "             \"owner\": \"source.owner\"\n",
    "         })\n",
    "         .whenNotMatchedInsert(values = {\n",
    "             \"rule_id\": \"source.rule_id\",\n",
    "             \"technical_rule_name\": \"source.technical_rule_name\",\n",
    "             \"functional_name\": \"source.functional_name\",\n",
    "             \"description\": \"source.description\",\n",
    "             \"rule_level\": \"source.rule_level\",\n",
    "             \"dimension_dq\": \"source.dimension_dq\",\n",
    "             \"implementation_type\": \"source.implementation_type\",\n",
    "             \"updated_at\": \"source.updated_at\",\n",
    "             \"updated_by\": \"source.updated_by\",\n",
    "             \"tags\": \"source.tags\",\n",
    "             \"owner\": \"source.owner\"\n",
    "         })\n",
    "         .execute()\n",
    "        )\n",
    "        count = df_source.count()\n",
    "        print(f\"xito: {count} registros sincronizados en {target_table_name}.\")\n",
    "        return count\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR al sincronizar {target_table_name}: {e}\")\n",
    "        raise\n",
    "\n",
    "build_json_udf = udf(build_json_definition, StringType())\n",
    "\n",
    "# DBTITLE 4,4. Funci贸n gen茅rica para sincronizar Delta por hoja\n",
    "def sync_delta(df_spark, target_table_name, merge_keys, column_mapping, sheet_name):\n",
    "    if df_spark is None:\n",
    "        return 0, []\n",
    "    errors_list = []\n",
    "    try:\n",
    "        delta_table = DeltaTable.forName(spark, target_table_name)\n",
    "        delta_table.alias(\"target\").merge(\n",
    "            df_spark.alias(\"source\"),\n",
    "            \" AND \".join([f\"target.{k} = source.{k}\" for k in merge_keys])\n",
    "        ).whenMatchedUpdate(set={k:v for k,v in column_mapping.items()}) \\\n",
    "         .whenNotMatchedInsert(values={k:v for k,v in column_mapping.items()}) \\\n",
    "         .execute()\n",
    "        return df_spark.count(), errors_list\n",
    "    except Exception as e:\n",
    "        errors_list.append({\"sheet\": sheet_name, \"fila\": None, \"error\": str(e)})\n",
    "        return 0, errors_list\n",
    "\n",
    "# DBTITLE 5,5. Sincronizaci贸n espec铆fica por hoja\n",
    "def sync_tables(df_excel):\n",
    "\n",
    "    if df_excel is None:\n",
    "        return 0, 0\n",
    "    \n",
    "    df_valid = df_excel.select(\n",
    "        col(\"Id_tabla\").alias(\"table_id\"),\n",
    "        col(\"Nombre_tabla\").alias(\"table_name\"),\n",
    "        col(\"Nombre_t茅cnico\").alias(\"table_name_tech\"),\n",
    "        col(\"Clave_primaria\").alias(\"primary_key\"),\n",
    "        col(\"Tabla_staging_evidencias\").alias(\"staging_evidences_table\")\n",
    "    ).filter(col(\"Id_tabla\") != \"\")\n",
    "\n",
    "    mapping = {c:c for c in df_valid.columns}\n",
    "    return sync_delta(df_valid, TABLE_CONFIG, [\"table_id\"], mapping, \"Tablas\")   \n",
    "\n",
    "def sync_rules(df_excel):\n",
    "\n",
    "    if df_excel is None:\n",
    "        return 0, 0\n",
    "    \n",
    "    df_valid = df_excel.select(\n",
    "        col(\"Id_regla\").alias(\"rule_id\"),\n",
    "        col(\"Nombre_t茅cnico\").alias(\"technical_rule_name\"),\n",
    "        col(\"Nombre_funcional\").alias(\"functional_name\"),\n",
    "        col(\"Descripci贸n\").alias(\"description\"),\n",
    "        col(\"Nivel_regla\").alias(\"rule_level\"),\n",
    "        col(\"Dimensi贸n_reglas\").alias(\"dimension_dq\"),\n",
    "        col(\"Tipo_implementaci贸n\").alias(\"implementation_type\"),\n",
    "        current_timestamp().alias(\"updated_at\"),\n",
    "        col(\"Actualizado_por\").alias(\"updated_by\"),\n",
    "        split(col(\"Etiquetas\"), \",\\s*\").alias(\"tags\"),\n",
    "        col(\"Propietario\").alias(\"owner\")\n",
    "    ).filter(col(\"Id_regla\") != \"\")\n",
    "    mapping = {c:c for c in df_valid.columns}\n",
    "    return sync_delta(df_valid, RULE_LIB_TABLE, [\"rule_id\"], mapping, \"Reglas\")\n",
    "\n",
    "def sync_validations(df_excel):\n",
    "\n",
    "    if df_excel is None:\n",
    "        return 0, 0\n",
    "    \n",
    "    df_valid = df_excel.select(\n",
    "        col(\"Id_validaci贸n\").alias(\"validation_id\"),\n",
    "        col(\"Id_regla\").alias(\"rule_id\"),\n",
    "        col(\"Id_tabla\").alias(\"table_id\"),\n",
    "        col(\"Definici贸n_perimetro\").alias(\"perimeter_definition\"),\n",
    "        col(\"Validaci贸n_activa\").cast(BooleanType()).alias(\"is_active\"),\n",
    "        col(\"Severidad\").alias(\"severity\"),\n",
    "        build_json_udf(\n",
    "            col(\"Nombre_t茅cnico\"), col(\"Tipo_regla\"),\n",
    "            col(\"Param_columnas\"), col(\"Param_valor\"),\n",
    "            col(\"Param_valor_min\"), col(\"Param_valor_max\"),\n",
    "            col(\"Param_conjunto\"), col(\"Param_expresion\"),\n",
    "            col(\"Param_tipo_dato\"), col(\"Param_query_SQL\"),\n",
    "            col(\"Param_merge_columnas\")\n",
    "        ).alias(\"validation_definition\"),\n",
    "        col(\"Dominio_tabla\").alias(\"domain\"),\n",
    "        split(col(\"Param_columnas\"), \",\\s*\").alias(\"Param_columns\"),\n",
    "        col(\"Param_valor\").alias(\"Param_value\"),\n",
    "        col(\"Param_valor_min\").alias(\"Param_value_min\"),\n",
    "        col(\"Param_valor_max\").alias(\"Param_value_max\"),\n",
    "        col(\"Param_conjunto\").alias(\"Param_range\"),\n",
    "        col(\"Param_expresion\").alias(\"Param_expression\"),\n",
    "        col(\"Param_tipo_dato\").alias(\"Param_data_type\"),\n",
    "        col(\"Param_query_SQL\").alias(\"Param_query_SQL\"),\n",
    "        col(\"Param_merge_columnas\").alias(\"Param_merge_columns\"),\n",
    "        col(\"Owner\").alias(\"owner\"),\n",
    "        current_timestamp().alias(\"updated_at\"),\n",
    "        col(\"Actualizado_por\").alias(\"updated_by\"),\n",
    "        split(col(\"Etiquetas\"), \",\\s*\").alias(\"tags\")\n",
    "    ).filter(col(\"Id_validaci贸n\") != \"\")\n",
    "    mapping = {c:c for c in df_valid.columns}\n",
    "    return sync_delta(df_valid, CATALOG_TABLE, [\"validation_id\"], mapping, \"Validaciones\")\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 7, 7. Funci贸n Principal (main) - Orquestaci贸n del script\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Flujo principal del script de sincronizaci贸n.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # --- 1. Sincronizar dq_tables_config ---\n",
    "        df_tablas, errors_tablas = load_and_validate_excel(EXCEL_PATH, CONFIG_SHEET, CONFIG_EXPECTED_COLS)\n",
    "        total_errors.extend(errors_tablas)\n",
    "        synced_tablas, _ = sync_tables(df_tablas)\n",
    "        \n",
    "        # 2. Sincronizar Rules_library\n",
    "        df_rules, errors_rules = load_and_validate_excel(EXCEL_PATH, LIBRARY_SHEET, LIBRARY_EXPECTED_COLS)\n",
    "        total_errors.extend(errors_rules)\n",
    "        synced_rules, _ = sync_rules(df_rules)\n",
    "        \n",
    "        # 3. Sincronizar Validations_catalog\n",
    "        df_valids, errors_valids = load_and_validate_excel(EXCEL_PATH, VALIDATIONS_SHEET, VALIDATIONS_EXPECTED_COLS)\n",
    "        total_errors.extend(errors_valids)\n",
    "        synced_valids, _ = sync_validations(df_valids)\n",
    "        \n",
    "        print(f\"Sincronizaci贸n completa\")\n",
    "        if total_errors:\n",
    "            print(f\"Se detectaron {len(total_errors)} errores durante la sincronizaci贸n:\")\n",
    "            #for e in total_errors: print(e)\n",
    "        return f\"xito: Tablas {synced_tablas}, Reglas {synced_rules}, Validaciones {synced_valids}, Errores {len(total_errors)}\"\n",
    "    except Exception as e:\n",
    "        print(f\"Fallo cr铆tico: {e}\")\n",
    "        raise\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 7, 7. Punto de Entrada de Ejecuci贸n\n",
    "if __name__ == \"__main__\":\n",
    "    result_msg = main()\n",
    "    dbutils.notebook.exit(result_msg)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "old_sync_catalog_from_excel",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
