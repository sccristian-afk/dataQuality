{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba6dba24-b6c9-487d-803d-b325e30692f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install pandas openpyxl "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7826e6e-d311-4f1e-84a0-b15a2aca6468",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d82b6a6f-edd6-4c17-b8f2-1cd68706ea62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MAGIC %md\n",
    "# MAGIC #  Sincronizador de Reglas (Excel -> Cat谩logo de Validaciones, metadatos de tablas y cat谩logo de reglas)\n",
    "# MAGIC \n",
    "# MAGIC Este notebook es la herramienta para poblar 'dq_tables_config', 'dq_rules_library' y 'dq_validations_catalog'\n",
    "# MAGIC leyendo la plantilla de Excel validationsConfig\n",
    "# MAGIC \n",
    "# MAGIC **Funciones:**\n",
    "# MAGIC 1.  'load_and_validate_excel': lee y valida el fichero Excel\n",
    "# MAGIC 2.  'build_json_definition': construye el JSON 'rule_definition' a partir de las columnas de par谩metros\n",
    "# MAGIC 2.  'sync_table_config': sincroniza los cambios en 'dq_tables_config' con la funci贸n 'MERGE'\n",
    "# MAGIC 2.  'sync_rule_library': sincroniza los cambios en 'dq_rules_library' con la funci贸n 'MERGE'\n",
    "# MAGIC 2.  'sync_validations_catalog': sincroniza los cambios en 'dq_validations_catalog' con la funci贸n 'MERGE'\n",
    "# MAGIC 2.  'main': orquesta las llamadas a las dem谩s funciones\n",
    "\n",
    "# COMMAND ----------\n",
    "# Libreria para poder leer el .xlsx por medio de Pandas. \n",
    "# !!!!!!!!!!!!!!!comentario interno (borrar cuando se solucione): O se instala en el cluster o se a帽ade a otra linea de c贸digo (si falla de esta forma)!!!!!!!!!!!!!!!\n",
    "# MAGIC %pip install pandas openpyxl\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1, 1. Imports, constantes y widgets\n",
    "import pandas as pd\n",
    "import json\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import col, lit, current_timestamp, udf, struct, split\n",
    "from pyspark.sql.types import StringType, MapType, StructType, StructField, BooleanType, ArrayType\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# --- Constantes de tablas ---\n",
    "dbutils.widgets.text(\"catalog_name\", \"workspace\", \"Cat谩logo de UC donde residen las tablas\")\n",
    "dbutils.widgets.text(\"schema_name\", \"dq_framework\", \"Esquema de UC donde residen las tablas\")\n",
    "\n",
    "CATALOG = dbutils.widgets.get(\"catalog_name\")\n",
    "SCHEMA = dbutils.widgets.get(\"schema_name\")\n",
    "\n",
    "TABLE_CONFIG = f\"{CATALOG}.{SCHEMA}.dq_tables_config\"\n",
    "RULE_LIB_TABLE = f\"{CATALOG}.{SCHEMA}.dq_rules_library\"\n",
    "CATALOG_TABLE = f\"{CATALOG}.{SCHEMA}.dq_validations_catalog\"\n",
    "\n",
    "# --- Constantes globales ---\n",
    "CURRENT_USER = dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get()\n",
    "\n",
    "# --- Constantes de hojas y columnas del Excel ---\n",
    "dbutils.widgets.text(\"tables_sheet\", \"Tablas\", \"Cat谩logo de UC donde residen las tablas\")\n",
    "dbutils.widgets.text(\"rules_sheet\", \"Reglas\", \"Esquema de UC donde residen las tablas\")\n",
    "dbutils.widgets.text(\"validations_sheet\", \"Validaciones\", \"Cat谩logo de UC donde residen las tablas\")\n",
    "\n",
    "CONFIG_SHEET = dbutils.widgets.get(\"tables_sheet\")\n",
    "CONFIG_EXPECTED_COLS = [\n",
    "    'Id_tabla', 'Nombre_tabla', 'Nombre_t茅cnico', \n",
    "    'Clave_primaria', 'Tabla_staging_evidencias'\n",
    "]\n",
    "\n",
    "LIBRARY_SHEET = dbutils.widgets.get(\"rules_sheet\")\n",
    "LIBRARY_EXPECTED_COLS = [\n",
    "    'Id_regla', 'Nombre_t茅cnico', 'Nombre_funcional', 'Descripci贸n', \n",
    "    'Dimensi贸n_reglas', 'Tipo_implementaci贸n', 'Etiquetas', 'Clase', \n",
    "    'Propietario', 'Fecha_actualizacion', 'Actualizado_por'\n",
    "]\n",
    "\n",
    "VALIDATIONS_SHEET = dbutils.widgets.get(\"validations_sheet\")\n",
    "VALIDATIONS_EXPECTED_COLS = [\n",
    "    'Id_validaci贸n','Id_regla', 'Id_tabla', 'Nombre_t茅cnico', 'Dominio_tabla', 'Definici贸n_perimetro',\n",
    "    'Param_columnas', 'Param_valor','Param_valor_min','Param_valor_max', 'Param_conjunto',\n",
    "    'Param_tipo_dato','Param_expresion', 'Param_query_SQL', 'Param_merge_columnas', \n",
    "    'Severidad','Owner','Etiquetas', 'Actualizado_por', 'Fecha_actualizaci贸n', 'Validaci贸n_activa'\n",
    "]\n",
    "\n",
    "# --- Widgets ---\n",
    "dbutils.widgets.text(\"excel_file_path\", \"/Volumes/workspace/dq_framework/template/configValidaciones.xlsx\", \"Ruta al fichero Excel con la informaci贸n\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 2, 2. Leer y validar Excel\n",
    "def load_and_validate_excel(excel_path, sheet_name, expected_cols):\n",
    "    \"\"\"\n",
    "    Lee el fichero Excel usando Pandas y lo convierte a un DataFrame de Spark\n",
    "    Valida que las columnas esperadas existan.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Uso de Pandas para leer, forzando todo a string para evitar errores de tipo\n",
    "        pandas_df = pd.read_excel(excel_path, sheet_name=sheet_name, dtype=str).fillna('')\n",
    "\n",
    "        # Validar que las columnas necesarias existen      \n",
    "        missing_cols = [col for col in expected_cols if col not in pandas_df.columns]\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"Faltan columnas obligatorias en el Excel: {missing_cols}\")\n",
    "        \n",
    "        pandas_df = pandas_df.dropna(how='all')\n",
    "        if pandas_df.empty:\n",
    "            print(f\"Hoja '{sheet_name}' est谩 vac铆a. Saltando sincronizaci贸n.\")\n",
    "            return None\n",
    "        \n",
    "        # Convertir el DataFrame de Pandas a un DataFrame de Spark\n",
    "        return spark.createDataFrame(pandas_df)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error Cr铆tico: No se encontr贸 el fichero Excel en {excel_path}\")\n",
    "        raise\n",
    "    except ValueError as ve:\n",
    "        print(f\"Error de Validaci贸n: {ve}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"Error Cr铆tico al leer el Excel: {e}\")\n",
    "        raise\n",
    "    \n",
    "# COMMAND ----------\n",
    "            \n",
    "# COMMAND ----------\n",
    "# DBTITLE 3, 3. Funci贸n Python para construir el JSON (la l贸gica de la regla)\n",
    "def build_json_definition(technical_name, rule_type, param_columnas_str, param_valor, param_min, param_max, param_conjunto_str, param_sql, param_tipo, param_query, param_merge):\n",
    "    \"\"\"\n",
    "    Funci贸n: con las columnas de par谩metros del Excel\n",
    "    construye el string JSON 'rule_definition' que espera el orquestador.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- Diccionario base ---\n",
    "    params_dict = {}\n",
    "    \n",
    "    try:\n",
    "        # --- L贸gica para reglas BUILT-IN ---\n",
    "        if rule_type == 'BUILT-IN':\n",
    "            \n",
    "            # Reglas que solo usan 'Param_Columnas'\n",
    "            if technical_name in [\"is_not_null\", \"is_not_empty\", \"is_not_null_and_not_empty\", \"is_not_in_future\"]:\n",
    "                columns = [col.strip() for col in param_columnas_str.split(',') if col and col.strip()]\n",
    "                if columns: params_dict[\"column\"] = columns[0]\n",
    "            \n",
    "            elif technical_name == \"is_unique\":\n",
    "                columns = [col.strip() for col in param_columnas_str.split(',') if col and col.strip()]\n",
    "                if columns: params_dict[\"columns\"] = columns # Espera 'columns' (plural)\n",
    "\n",
    "            # Regla 'is_in_list' y 'is_not_null_and_is_in_list'\n",
    "            elif technical_name in [\"is_in_list\", \"is_not_null_and_is_in_list\"]:\n",
    "                col_name = param_columnas_str.strip() if param_columnas_str else None\n",
    "                if col_name: params_dict[\"column\"] = col_name\n",
    "                allowed_values = [val.strip() for val in param_conjunto_str.split(',') if val and val.strip()]\n",
    "                if allowed_values: params_dict[\"allowed\"] = allowed_values\n",
    "            \n",
    "            # Regla 'regex_match'\n",
    "            elif technical_name == \"regex_match\":\n",
    "                col_name = param_columnas_str.strip() if param_columnas_str else None\n",
    "                if col_name: params_dict[\"column\"] = col_name\n",
    "                if param_valor: params_dict[\"regex\"] = param_valor\n",
    "             \n",
    "            # Regla 'is_equal_to' y 'is_not_equal_to'\n",
    "            elif technical_name in [\"is_equal_to\", \"is_not_equal_to\"]:\n",
    "                col_name = param_columnas_str.strip() if param_columnas_str else None\n",
    "                if col_name: params_dict[\"column\"] = col_name\n",
    "                if param_valor: params_dict[\"value\"] = param_valor\n",
    "\n",
    "            # Regla 'is_valid_date'\n",
    "            elif technical_name == \"is_valid_date\":\n",
    "                col_name = param_columnas_str.strip() if param_columnas_str else None\n",
    "                if col_name: params_dict[\"column\"] = col_name\n",
    "                if param_valor: params_dict[\"date_format\"] = param_valor\n",
    "\n",
    "            # Regla 'is_valid_timestamp'\n",
    "            elif technical_name == \"is_valid_timestamp\":\n",
    "                col_name = param_columnas_str.strip() if param_columnas_str else None\n",
    "                if col_name: params_dict[\"column\"] = col_name\n",
    "                if param_valor: params_dict[\"timestamp_format\"] = param_valor\n",
    "                \n",
    "            # Regla 'is_in_range' y 'is_not_in_range'\n",
    "            elif technical_name in [\"is_in_range\", \"is_not_in_range\"]:\n",
    "                col_name = param_columnas_str.strip() if param_columnas_str else None\n",
    "                if col_name: params_dict[\"column\"] = col_name\n",
    "                try: \n",
    "                    if param_min: params_dict[\"min_limit\"] = float(param_min)\n",
    "                except (ValueError, TypeError): params_dict[\"min_limit\"] = param_min\n",
    "                try: \n",
    "                    if param_max: params_dict[\"max_limit\"] = float(param_max)\n",
    "                except (ValueError, TypeError): params_dict[\"max_limit\"] = param_max\n",
    "            \n",
    "            # Regla 'is_not_less_than'\n",
    "            elif technical_name == \"is_not_less_than\":\n",
    "                col_name = param_columnas_str.strip() if param_columnas_str else None\n",
    "                if col_name: params_dict[\"column\"] = col_name\n",
    "                try: \n",
    "                    if param_min: params_dict[\"limit\"] = float(param_min)\n",
    "                except (ValueError, TypeError): params_dict[\"limit\"] = param_min\n",
    "            \n",
    "            # Regla 'is_not_greater_than'\n",
    "            elif technical_name == \"is_not_greater_than\":\n",
    "                col_name = param_columnas_str.strip() if param_columnas_str else None\n",
    "                if col_name: params_dict[\"column\"] = col_name\n",
    "                try: \n",
    "                    if param_max: params_dict[\"limit\"] = float(param_max)\n",
    "                except (ValueError, TypeError): params_dict[\"limit\"] = param_max\n",
    "\n",
    "            # Regla 'sql_expression'\n",
    "            elif technical_name == \"sql_expression\":\n",
    "                if param_sql: params_dict[\"expression\"] = param_sql\n",
    "            \n",
    "             # Regla 'sql_query'\n",
    "            elif technical_name == \"sql_query\":\n",
    "                if param_query: params_dict[\"query\"] = param_query\n",
    "                merge_cols = [col.strip() for col in param_merge.split(',') if col and col.strip()]\n",
    "                if merge_cols: params_dict[\"merge_columns\"] = merge_cols\n",
    "\n",
    "        # --- L贸gica para Reglas CUSTOM ---\n",
    "        elif rule_type in ['CUSTOM']:\n",
    "             columns = [col.strip() for col in param_columnas_str.split(',') if col and col.strip()]\n",
    "             if columns: params_dict[\"columns\"] = columns\n",
    "\n",
    "        # Convertir el diccionario de par谩metros final a un string JSON\n",
    "        return json.dumps(params_dict)\n",
    "\n",
    "    except Exception as e:\n",
    "        return json.dumps({\"error\": f\"Error al construir JSON de params: {e}\"})\n",
    "    \n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 4, 4. Sincronizar table_config\n",
    "def sync_table_config(df_excel, target_table_name):\n",
    "    \"\"\"\n",
    "    Toma el DataFrame de la hoja 'Tablas' y lo sincroniza\n",
    "    con la tabla 'dq_tables_config'.\n",
    "    \"\"\"\n",
    "    if df_excel is None:\n",
    "        return 0, 0\n",
    "\n",
    "    try:\n",
    "        df_source = df_excel.select(\n",
    "            col(\"Id_tabla\").alias(\"table_id\"),\n",
    "            col(\"Nombre_tabla\").alias(\"table_name\"),\n",
    "            col(\"Nombre_t茅cnico\").alias(\"table_name_tech\"),\n",
    "            col(\"Clave_Primaria\").alias(\"primary_key\"),\n",
    "            col(\"Tabla_staging_evidencias\").alias(\"staging_evidences_table\")\n",
    "        ).filter(col(\"Id_tabla\") != \"\")\n",
    "\n",
    "        delta_table = DeltaTable.forName(spark, target_table_name)\n",
    "\n",
    "        (delta_table.alias(\"target\")\n",
    "         .merge(\n",
    "             df_source.alias(\"source\"),\n",
    "             condition = \"target.table_id = source.table_id\"\n",
    "         )\n",
    "         .whenMatchedUpdateAll()\n",
    "         .whenNotMatchedInsertAll()\n",
    "         .execute()\n",
    "        )\n",
    "        count = df_source.count()\n",
    "        print(f\"xito: {count} registros sincronizados en {target_table_name}.\")\n",
    "        return count\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR al sincronizar {target_table_name}: {e}\")\n",
    "        raise\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 5, 5. Sincronizar dq_rule_library\n",
    "def sync_rule_library(df_excel, target_table_name):\n",
    "    \"\"\"\n",
    "    Toma el DataFrame de la hoja 'Reglas' y lo sincroniza\n",
    "    con la tabla Delta 'dq_rules_library'.\n",
    "    \"\"\"\n",
    "    if df_excel is None:\n",
    "        return 0, 0\n",
    "        \n",
    "    try:\n",
    "        df_source = df_excel.select(\n",
    "            col(\"Id_regla\").alias(\"rule_id\"),\n",
    "            col(\"Nombre_t茅cnico\").alias(\"technical_rule_name\"),\n",
    "            col(\"Nombre_funcional\").alias(\"functional_name\"),\n",
    "            col(\"Descripci贸n\").alias(\"description\"),\n",
    "            col(\"Dimensi贸n_reglas\").alias(\"dimension_dq\"),\n",
    "            col(\"Tipo_implementaci贸n\").alias(\"implementation_type\"),\n",
    "            current_timestamp().alias(\"updated_at\"),\n",
    "            col(\"Actualizado_por\").alias(\"updated_by\"),\n",
    "            split(col(\"Etiquetas\"), \",\\s*\").alias(\"tags\"),\n",
    "            col(\"Propietario\").alias(\"owner\"),\n",
    "        ).filter(col(\"rule_id\") != \"\")\n",
    "\n",
    "        delta_table = DeltaTable.forName(spark, target_table_name)\n",
    "\n",
    "        (delta_table.alias(\"target\")\n",
    "         .merge(\n",
    "             df_source.alias(\"source\"),\n",
    "             condition = \"target.rule_id = source.rule_id\"\n",
    "         )\n",
    "         .whenMatchedUpdate(set = {\n",
    "             \"technical_rule_name\": \"source.technical_rule_name\",\n",
    "             \"functional_name\": \"source.functional_name\",\n",
    "             \"description\": \"source.description\",\n",
    "             \"dimension_dq\": \"source.dimension_dq\",\n",
    "             \"implementation_type\": \"source.implementation_type\",\n",
    "             \"updated_at\": \"source.updated_at\",\n",
    "             \"updated_by\": \"source.updated_by\",\n",
    "             \"tags\": \"source.tags\",\n",
    "             \"owner\": \"source.owner\"\n",
    "         })\n",
    "         .whenNotMatchedInsert(values = {\n",
    "             \"rule_id\": \"source.rule_id\",\n",
    "             \"technical_rule_name\": \"source.technical_rule_name\",\n",
    "             \"functional_name\": \"source.functional_name\",\n",
    "             \"description\": \"source.description\",\n",
    "             \"dimension_dq\": \"source.dimension_dq\",\n",
    "             \"implementation_type\": \"source.implementation_type\",\n",
    "             \"updated_at\": \"source.updated_at\",\n",
    "             \"updated_by\": \"source.updated_by\",\n",
    "             \"tags\": \"source.tags\",\n",
    "             \"owner\": \"source.owner\"\n",
    "         })\n",
    "         .execute()\n",
    "        )\n",
    "        count = df_source.count()\n",
    "        print(f\"xito: {count} registros sincronizados en {target_table_name}.\")\n",
    "        return count\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR al sincronizar {target_table_name}: {e}\")\n",
    "        raise\n",
    "\n",
    "# DBTITLE 6, 6. Sincronizar dq_validations_catalog\n",
    "\n",
    "# Registrar la UDF de transformaci贸n a JSON\n",
    "build_json_udf = udf(build_json_definition, StringType())\n",
    "\n",
    "def sync_validations_catalog(df_excel, target_table_name):\n",
    "    \"\"\"\n",
    "    Toma el DataFrame de la hoja 'Validaciones'\n",
    "    y lo sincroniza con la tabla Delta 'dq_validations_catalog'.\n",
    "    \"\"\"\n",
    "\n",
    "    if df_excel is None:\n",
    "        return 0, 0\n",
    "        \n",
    "    try:\n",
    "        df_final_source = (df_excel.select(\n",
    "            col(\"Id_validaci贸n\").alias(\"validation_id\"),\n",
    "            col(\"Id_regla\").alias(\"rule_id\"), \n",
    "            col(\"Id_tabla\").alias(\"table_id\"),\n",
    "            col(\"Definici贸n_perimetro\").alias(\"perimeter_definition\"),            \n",
    "            col(\"Validaci贸n_activa\").cast(BooleanType()).alias(\"is_active\"),\n",
    "            col(\"Severidad\").alias(\"severity\"),\n",
    "            build_json_udf(\n",
    "                col(\"Nombre_t茅cnico\"), col(\"Tipo_regla\"),\n",
    "                col(\"Param_columnas\"), col(\"Param_valor\"),\n",
    "                col(\"Param_valor_min\"), col(\"Param_valor_max\"),\n",
    "                col(\"Param_conjunto\"), col(\"Param_expresion\"),\n",
    "                col(\"Param_tipo_dato\"), col(\"Param_query_SQL\"),\n",
    "                col(\"Param_merge_columnas\")\n",
    "            ).alias(\"validation_definition\"),\n",
    "            col(\"Dominio_tabla\").alias(\"domain\"),\n",
    "            split(col(\"Param_columnas\"), \",\\s*\").alias(\"Param_columns\"),\n",
    "            col(\"Param_valor\").alias(\"Param_value\"),\n",
    "            col(\"Param_valor_min\").alias(\"Param_value_min\"), \n",
    "            col(\"Param_valor_max\").alias(\"Param_value_max\"),\n",
    "            col(\"Param_conjunto\").alias(\"Param_range\"), \n",
    "            col(\"Param_expresion\").alias(\"Param_expression\"),\n",
    "            col(\"Param_tipo_dato\").alias(\"Param_data_type\"), \n",
    "            col(\"Param_query_SQL\").alias(\"Param_query_SQL\"),\n",
    "            col(\"Param_merge_columnas\").alias(\"Param_merge_columns\"), \n",
    "            col(\"Owner\").alias(\"owner\"),\n",
    "            current_timestamp().alias(\"updated_at\"),\n",
    "            col(\"Actualizado_por\").alias(\"updated_by\"),\n",
    "            split(col(\"Etiquetas\"), \",\\s*\").alias(\"tags\")\n",
    "        ))\n",
    "        \n",
    "        delta_table = DeltaTable.forName(spark, target_table_name)\n",
    "        \n",
    "        (delta_table.alias(\"target\")\n",
    "         .merge(\n",
    "             df_final_source.alias(\"source\"), \n",
    "             \"target.validation_id = source.validation_id\"\n",
    "        )\n",
    "         .whenMatchedUpdate(set = {\n",
    "            \"rule_id\": \"source.rule_id\", \n",
    "            \"table_id\": \"source.table_id\",\n",
    "            \"perimeter_definition\": \"source.perimeter_definition\",\n",
    "            \"is_active\": \"source.is_active\",\n",
    "            \"severity\": \"source.severity\", \n",
    "            \"validation_definition\": \"source.validation_definition\",\n",
    "            \"domain\": \"source.domain\",\n",
    "            \"Param_columns\": \"source.Param_columns\", \n",
    "            \"Param_value\": \"source.Param_value\",\n",
    "            \"Param_value_min\": \"source.Param_value_min\", \n",
    "            \"Param_value_max\": \"source.Param_value_max\",\n",
    "            \"Param_range\": \"source.Param_range\", \n",
    "            \"Param_expression\": \"source.Param_expression\",\n",
    "            \"Param_data_type\": \"source.Param_data_type\", \n",
    "            \"Param_query_SQL\": \"source.Param_query_SQL\",\n",
    "            \"Param_merge_columns\": \"source.Param_merge_columns\",\n",
    "            \"owner\": \"source.owner\", \n",
    "            \"tags\": \"source.tags\",\n",
    "            \"updated_at\": \"source.updated_at\", \n",
    "            \"updated_by\": \"source.updated_by\"\n",
    "         })\n",
    "         .whenNotMatchedInsert(values = {\n",
    "            \"validation_id\": \"source.validation_id\",\n",
    "            \"rule_id\": \"source.rule_id\", \n",
    "            \"table_id\": \"source.table_id\",\n",
    "            \"perimeter_definition\": \"source.perimeter_definition\",\n",
    "            \"is_active\": \"source.is_active\",\n",
    "            \"severity\": \"source.severity\", \n",
    "            \"validation_definition\": \"source.validation_definition\",\n",
    "            \"domain\": \"source.domain\",\n",
    "            \"Param_columns\": \"source.Param_columns\", \n",
    "            \"Param_value\": \"source.Param_value\",\n",
    "            \"Param_value_min\": \"source.Param_value_min\", \n",
    "            \"Param_value_max\": \"source.Param_value_max\",\n",
    "            \"Param_range\": \"source.Param_range\", \n",
    "            \"Param_expression\": \"source.Param_expression\",\n",
    "            \"Param_data_type\": \"source.Param_data_type\", \n",
    "            \"Param_query_SQL\": \"source.Param_query_SQL\",\n",
    "            \"Param_merge_columns\": \"source.Param_merge_columns\",\n",
    "            \"owner\": \"source.owner\", \n",
    "            \"tags\": \"source.tags\",\n",
    "            \"updated_at\": \"source.updated_at\", \n",
    "            \"updated_by\": \"source.updated_by\"    \n",
    "         })\n",
    "         .execute()\n",
    "        )\n",
    "\n",
    "        count = df_final_source.count()\n",
    "        print(f\"xito: {count} registros sincronizados en {target_table_name}.\")\n",
    "        return count, 0\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR al sincronizar {target_table_name}: {e}\")\n",
    "        raise\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 7, 7. Funci贸n Principal (main) - Orquestaci贸n del script\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Flujo principal del script de sincronizaci贸n.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        excel_path = dbutils.widgets.get(\"excel_file_path\")\n",
    "        \n",
    "        # --- 1. Sincronizar dq_tables_config ---\n",
    "        df_config = load_and_validate_excel(excel_path, CONFIG_SHEET, CONFIG_EXPECTED_COLS)\n",
    "        sync_table_config(df_config, TABLE_CONFIG)\n",
    "        \n",
    "        # --- 2. Sincronizar Rules_library ---\n",
    "        df_library = load_and_validate_excel(excel_path, LIBRARY_SHEET, LIBRARY_EXPECTED_COLS)\n",
    "        sync_rule_library(df_library, RULE_LIB_TABLE)\n",
    "\n",
    "        # --- 3. Sincronizar Validations_catalog ---\n",
    "        df_validations = load_and_validate_excel(excel_path, VALIDATIONS_SHEET, VALIDATIONS_EXPECTED_COLS)\n",
    "        synced_count, error_count = sync_validations_catalog(df_validations, CATALOG_TABLE)\n",
    "        \n",
    "        msg = f\"Sincronizaci贸n completada\"\n",
    "        if error_count > 0:\n",
    "            msg += f\" AVISO: {error_count} filas de validaci贸n fueron ignoradas por errores.\"\n",
    "        \n",
    "        return msg\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fatal durante la sincronizaci贸n: {e}\")\n",
    "        raise e\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 7, 7. Punto de Entrada de Ejecuci贸n\n",
    "if __name__ == \"__main__\":\n",
    "    status = \"xito\"\n",
    "    completion_message = \"\"\n",
    "    \n",
    "    try:\n",
    "        completion_message = main()  \n",
    "    except Exception as e:\n",
    "        status = \"Fallo\"\n",
    "        completion_message = f\"Fallo en la sincronizaci贸n: {e}\"\n",
    "        print(completion_message)\n",
    "    dbutils.notebook.exit(f\"{status}: {completion_message}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "sync_catalog_from_excel",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
